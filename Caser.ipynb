{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sknnn2xs-AHi",
        "outputId": "a4f15d3b-23ff-42f1-8d49-ded97e42bf52"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "System version: 3.8.9 (tags/v3.8.9:a743f81, Apr  6 2021, 14:02:34) [MSC v.1928 64 bit (AMD64)]\n",
            "Tensorflow version: 2.8.0\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "import logging\n",
        "import papermill as pm\n",
        "import scrapbook as sb\n",
        "from tempfile import TemporaryDirectory\n",
        "import numpy as np\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.get_logger().setLevel('ERROR') # only show error messages\n",
        "\n",
        "from recommenders.utils.timer import Timer\n",
        "from recommenders.utils.constants import SEED\n",
        "from recommenders.models.deeprec.deeprec_utils import (\n",
        "    prepare_hparams\n",
        ")\n",
        "\n",
        "from resources.data_preprocessing import data_preprocessing\n",
        "# from recommenders.datasets.amazon_reviews import download_and_extract, data_preprocessing\n",
        "from recommenders.datasets.download_utils import maybe_download\n",
        "\n",
        "\n",
        "# from recommenders.models.deeprec.models.sequential.sli_rec import SLI_RECModel as SeqModel\n",
        "####  to use the other model, use one of the following lines:\n",
        "#from recommenders.models.deeprec.models.sequential.asvd import A2SVDModel as SeqModel\n",
        "from recommenders.models.deeprec.models.sequential.caser import CaserModel as SeqModel\n",
        "# from recommenders.models.deeprec.models.sequential.gru4rec import GRU4RecModel as SeqModel\n",
        "# from recommenders.models.deeprec.models.sequential.sum import SUMModel as SeqModel\n",
        "\n",
        "#from recommenders.models.deeprec.models.sequential.nextitnet import NextItNetModel\n",
        "\n",
        "from recommenders.models.deeprec.io.sequential_iterator import SequentialIterator\n",
        "#from recommenders.models.deeprec.io.nextitnet_iterator import NextItNetIterator\n",
        "\n",
        "print(\"System version: {}\".format(sys.version))\n",
        "print(\"Tensorflow version: {}\".format(tf.__version__))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "sxtfUZfP-ZWR"
      },
      "outputs": [],
      "source": [
        "##  ATTENTION: change to the corresponding config file, e.g., caser.yaml for CaserModel, sum.yaml for SUMModel\n",
        "# yaml_file = '../../recommenders/models/deeprec/config/sli_rec.yaml'  \n",
        "yaml_file = './caser.yaml'  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "_NnzhN4h_rI5"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 10\n",
        "BATCH_SIZE = 400\n",
        "RANDOM_SEED = SEED  # Set None for non-deterministic result\n",
        "\n",
        "data_path = os.path.join(\"resources/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "0RAMNcI3AClc"
      },
      "outputs": [],
      "source": [
        "# for test\n",
        "train_file = os.path.join(data_path, r'train_data')\n",
        "valid_file = os.path.join(data_path, r'valid_data')\n",
        "test_file = os.path.join(data_path, r'test_data')\n",
        "user_vocab = os.path.join(data_path, r'user_vocab.pkl')\n",
        "item_vocab = os.path.join(data_path, r'item_vocab.pkl')\n",
        "cate_vocab = os.path.join(data_path, r'cate_vocab.pkl')\n",
        "output_file = os.path.join(data_path, r'output_caser.txt')\n",
        "\n",
        "# reviews_name = 'json'\n",
        "# meta_name = 'json'\n",
        "# reviews_file = os.path.join(data_path, reviews_name)\n",
        "# meta_file = os.path.join(data_path, meta_name)\n",
        "train_num_ngs = 4 # number of negative instances with a positive instance for training\n",
        "valid_num_ngs = 4 # number of negative instances with a positive instance for validation\n",
        "test_num_ngs = 9 # number of negative instances with a positive instance for testing\n",
        "sample_rate = 0.01 # sample a small item set for training and testing here for fast example\n",
        "\n",
        "input_files = [data_path, train_file, valid_file, test_file, user_vocab, item_vocab, cate_vocab]\n",
        "\n",
        "if not os.path.exists(train_file):\n",
        "    # download_and_extract(reviews_name, reviews_file)\n",
        "    # download_and_extract(meta_name, meta_file)\n",
        "    data_preprocessing(*input_files, sample_rate=sample_rate, valid_num_ngs=valid_num_ngs, test_num_ngs=test_num_ngs)\n",
        "    #### uncomment this for the NextItNet model, because it does not need to unfold the user history\n",
        "    # data_preprocessing(*input_files, sample_rate=sample_rate, valid_num_ngs=valid_num_ngs, test_num_ngs=test_num_ngs, is_history_expanding=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "rwTMO3WEWaCu"
      },
      "outputs": [],
      "source": [
        "### NOTE:  \n",
        "### remember to use `_create_vocab(train_file, user_vocab, item_vocab, cate_vocab)` to generate the user_vocab, item_vocab and cate_vocab files, if you are using your own dataset rather than using our demo Amazon dataset.\n",
        "hparams = prepare_hparams(yaml_file, \n",
        "                          embed_l2=0., \n",
        "                          layer_l2=0., \n",
        "                          learning_rate=0.001,  # set to 0.01 if batch normalization is disable\n",
        "                          epochs=EPOCHS,\n",
        "                          batch_size=BATCH_SIZE,\n",
        "                          show_step=20,\n",
        "                          MODEL_DIR=os.path.join(data_path, \"model\", \"caser/\"),\n",
        "                          SUMMARIES_DIR=os.path.join(data_path, \"summary\", \"caser/\"),\n",
        "                          user_vocab=user_vocab,\n",
        "                          item_vocab=item_vocab,\n",
        "                          cate_vocab=cate_vocab,\n",
        "                          need_sample=True,\n",
        "                          train_num_ngs=train_num_ngs, # provides the number of negative instances for each positive instance for loss computation.\n",
        "            )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "4Rvq7gFluVPq"
      },
      "outputs": [],
      "source": [
        "input_creator = SequentialIterator\n",
        "#### uncomment this for the NextItNet model, because it needs a special data iterator for training\n",
        "#input_creator = NextItNetIterator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VFNfgcKNuaVv",
        "outputId": "2fa083f0-f73d-452a-efef-12151b40c7c5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\조예은\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\recommenders\\models\\deeprec\\models\\sequential\\caser.py:102: UserWarning: `tf.layers.conv1d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv1D` instead.\n",
            "  return tf.compat.v1.layers.conv1d(\n",
            "C:\\Users\\조예은\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\legacy_tf_layers\\convolutional.py:294: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
            "  return layer.apply(inputs)\n",
            "C:\\Users\\조예은\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\recommenders\\models\\deeprec\\models\\sequential\\caser.py:66: UserWarning: `tf.layers.flatten` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Flatten` instead.\n",
            "  out_v = tf.compat.v1.layers.flatten(out_v)\n",
            "C:\\Users\\조예은\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\legacy_tf_layers\\core.py:541: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
            "  return layer.apply(inputs)\n",
            "C:\\Users\\조예은\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\recommenders\\models\\deeprec\\models\\base_model.py:701: UserWarning: `tf.layers.batch_normalization` is deprecated and will be removed in a future version. Please use `tf.keras.layers.BatchNormalization` instead. In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.BatchNormalization` documentation).\n",
            "  curr_hidden_nn_layer = tf.compat.v1.layers.batch_normalization(\n",
            "C:\\Users\\조예은\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\legacy_tf_layers\\normalization.py:463: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
            "  return layer.apply(inputs, training=training)\n"
          ]
        }
      ],
      "source": [
        "model = SeqModel(hparams, input_creator, seed=RANDOM_SEED)\n",
        "\n",
        "## sometimes we don't want to train a model from scratch\n",
        "## then we can load a pre-trained model like this: \n",
        "#model.load_model(r'your_model_path')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8AQxPlk-uheU",
        "outputId": "55a5328a-44bd-4213-b10c-96f26313586a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'auc': 0.5032, 'logloss': 0.6932, 'mean_mrr': 0.2804, 'ndcg@2': 0.1496, 'ndcg@4': 0.2359, 'ndcg@6': 0.3131, 'group_auc': 0.5056}\n"
          ]
        }
      ],
      "source": [
        "# test_num_ngs is the number of negative lines after each positive line in your test_file\n",
        "print(model.run_eval(test_file, num_ngs=test_num_ngs)) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QFKQu9rGu0jq",
        "outputId": "c93b6ebf-50cc-43ea-e620-418e4537ad31"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 20 , total_loss: 1.4993, data_loss: 1.4993\n",
            "step 40 , total_loss: 1.4213, data_loss: 1.4213\n",
            "step 60 , total_loss: 1.3456, data_loss: 1.3456\n",
            "step 80 , total_loss: 1.3380, data_loss: 1.3380\n",
            "step 100 , total_loss: 1.3079, data_loss: 1.3079\n",
            "step 120 , total_loss: 1.2797, data_loss: 1.2797\n",
            "step 140 , total_loss: 1.3192, data_loss: 1.3192\n",
            "step 160 , total_loss: 1.2867, data_loss: 1.2867\n",
            "step 180 , total_loss: 1.2731, data_loss: 1.2731\n",
            "step 200 , total_loss: 1.2542, data_loss: 1.2542\n",
            "step 220 , total_loss: 1.2495, data_loss: 1.2495\n",
            "step 240 , total_loss: 1.1705, data_loss: 1.1705\n",
            "step 260 , total_loss: 1.2580, data_loss: 1.2580\n",
            "step 280 , total_loss: 1.1555, data_loss: 1.1555\n",
            "step 300 , total_loss: 1.1686, data_loss: 1.1686\n",
            "step 320 , total_loss: 1.2010, data_loss: 1.2010\n",
            "step 340 , total_loss: 1.2430, data_loss: 1.2430\n",
            "step 360 , total_loss: 1.1813, data_loss: 1.1813\n",
            "step 380 , total_loss: 1.2442, data_loss: 1.2442\n",
            "step 400 , total_loss: 1.2015, data_loss: 1.2015\n",
            "step 420 , total_loss: 1.1899, data_loss: 1.1899\n",
            "step 440 , total_loss: 1.1980, data_loss: 1.1980\n",
            "step 460 , total_loss: 1.1736, data_loss: 1.1736\n",
            "step 480 , total_loss: 1.1235, data_loss: 1.1235\n",
            "step 500 , total_loss: 1.1909, data_loss: 1.1909\n",
            "step 520 , total_loss: 1.1870, data_loss: 1.1870\n",
            "step 540 , total_loss: 1.2353, data_loss: 1.2353\n",
            "step 560 , total_loss: 1.1403, data_loss: 1.1403\n",
            "step 580 , total_loss: 1.1096, data_loss: 1.1096\n",
            "step 600 , total_loss: 1.1552, data_loss: 1.1552\n",
            "step 620 , total_loss: 1.1327, data_loss: 1.1327\n",
            "step 640 , total_loss: 1.1162, data_loss: 1.1162\n",
            "eval valid at epoch 1: auc:0.7495,logloss:0.6706,mean_mrr:0.6787,ndcg@2:0.6362,ndcg@4:0.7506,ndcg@6:0.7598,group_auc:0.7693\n",
            "step 20 , total_loss: 1.1142, data_loss: 1.1142\n",
            "step 40 , total_loss: 1.1464, data_loss: 1.1464\n",
            "step 60 , total_loss: 1.1737, data_loss: 1.1737\n",
            "step 80 , total_loss: 1.1715, data_loss: 1.1715\n",
            "step 100 , total_loss: 1.1896, data_loss: 1.1896\n",
            "step 120 , total_loss: 1.1508, data_loss: 1.1508\n",
            "step 140 , total_loss: 1.1691, data_loss: 1.1691\n",
            "step 160 , total_loss: 1.1961, data_loss: 1.1961\n",
            "step 180 , total_loss: 1.1760, data_loss: 1.1760\n",
            "step 200 , total_loss: 1.1324, data_loss: 1.1324\n",
            "step 220 , total_loss: 1.1679, data_loss: 1.1679\n",
            "step 240 , total_loss: 1.0323, data_loss: 1.0323\n",
            "step 260 , total_loss: 1.0991, data_loss: 1.0991\n",
            "step 280 , total_loss: 1.0664, data_loss: 1.0664\n",
            "step 300 , total_loss: 1.0887, data_loss: 1.0887\n",
            "step 320 , total_loss: 1.0411, data_loss: 1.0411\n",
            "step 340 , total_loss: 1.1044, data_loss: 1.1044\n",
            "step 360 , total_loss: 1.0701, data_loss: 1.0701\n",
            "step 380 , total_loss: 1.1246, data_loss: 1.1246\n",
            "step 400 , total_loss: 1.1197, data_loss: 1.1197\n",
            "step 420 , total_loss: 1.0607, data_loss: 1.0607\n",
            "step 440 , total_loss: 1.1026, data_loss: 1.1026\n",
            "step 460 , total_loss: 1.0570, data_loss: 1.0570\n",
            "step 480 , total_loss: 1.0318, data_loss: 1.0318\n",
            "step 500 , total_loss: 1.0651, data_loss: 1.0651\n",
            "step 520 , total_loss: 1.0774, data_loss: 1.0774\n",
            "step 540 , total_loss: 1.1454, data_loss: 1.1454\n",
            "step 560 , total_loss: 1.0936, data_loss: 1.0936\n",
            "step 580 , total_loss: 1.1310, data_loss: 1.1310\n",
            "step 600 , total_loss: 1.0811, data_loss: 1.0811\n",
            "step 620 , total_loss: 1.0790, data_loss: 1.0790\n",
            "step 640 , total_loss: 1.0892, data_loss: 1.0892\n",
            "eval valid at epoch 2: auc:0.7377,logloss:0.4539,mean_mrr:0.6752,ndcg@2:0.6328,ndcg@4:0.7493,ndcg@6:0.7573,group_auc:0.7681\n",
            "step 20 , total_loss: 1.1425, data_loss: 1.1425\n",
            "step 40 , total_loss: 1.0622, data_loss: 1.0622\n",
            "step 60 , total_loss: 1.1074, data_loss: 1.1074\n",
            "step 80 , total_loss: 1.1247, data_loss: 1.1247\n",
            "step 100 , total_loss: 1.0138, data_loss: 1.0138\n",
            "step 120 , total_loss: 1.1263, data_loss: 1.1263\n",
            "step 140 , total_loss: 0.9979, data_loss: 0.9979\n",
            "step 160 , total_loss: 1.0894, data_loss: 1.0894\n",
            "step 180 , total_loss: 1.0958, data_loss: 1.0958\n",
            "step 200 , total_loss: 1.1349, data_loss: 1.1349\n",
            "step 220 , total_loss: 1.0366, data_loss: 1.0366\n",
            "step 240 , total_loss: 1.0867, data_loss: 1.0867\n",
            "step 260 , total_loss: 1.0669, data_loss: 1.0669\n",
            "step 280 , total_loss: 1.0183, data_loss: 1.0183\n",
            "step 300 , total_loss: 1.1174, data_loss: 1.1174\n",
            "step 320 , total_loss: 1.0315, data_loss: 1.0315\n",
            "step 340 , total_loss: 1.0299, data_loss: 1.0299\n",
            "step 360 , total_loss: 1.0633, data_loss: 1.0633\n",
            "step 380 , total_loss: 1.0729, data_loss: 1.0729\n",
            "step 400 , total_loss: 1.0242, data_loss: 1.0242\n",
            "step 420 , total_loss: 1.1354, data_loss: 1.1354\n",
            "step 440 , total_loss: 1.0654, data_loss: 1.0654\n",
            "step 460 , total_loss: 0.9640, data_loss: 0.9640\n",
            "step 480 , total_loss: 1.0282, data_loss: 1.0282\n",
            "step 500 , total_loss: 0.9870, data_loss: 0.9870\n",
            "step 520 , total_loss: 1.0034, data_loss: 1.0034\n",
            "step 540 , total_loss: 0.9584, data_loss: 0.9584\n",
            "step 560 , total_loss: 1.0871, data_loss: 1.0871\n",
            "step 580 , total_loss: 0.9997, data_loss: 0.9997\n",
            "step 600 , total_loss: 1.0112, data_loss: 1.0112\n",
            "step 620 , total_loss: 1.0610, data_loss: 1.0610\n",
            "step 640 , total_loss: 1.0737, data_loss: 1.0737\n",
            "eval valid at epoch 3: auc:0.7765,logloss:0.4503,mean_mrr:0.6965,ndcg@2:0.6611,ndcg@4:0.7669,ndcg@6:0.7734,group_auc:0.7878\n",
            "step 20 , total_loss: 0.9599, data_loss: 0.9599\n",
            "step 40 , total_loss: 1.0831, data_loss: 1.0831\n",
            "step 60 , total_loss: 1.0543, data_loss: 1.0543\n",
            "step 80 , total_loss: 1.0012, data_loss: 1.0012\n",
            "step 100 , total_loss: 0.9580, data_loss: 0.9580\n",
            "step 120 , total_loss: 0.9894, data_loss: 0.9894\n",
            "step 140 , total_loss: 1.0438, data_loss: 1.0438\n",
            "step 160 , total_loss: 1.0080, data_loss: 1.0080\n",
            "step 180 , total_loss: 1.0710, data_loss: 1.0710\n",
            "step 200 , total_loss: 1.0197, data_loss: 1.0197\n",
            "step 220 , total_loss: 0.8960, data_loss: 0.8960\n",
            "step 240 , total_loss: 0.9673, data_loss: 0.9673\n",
            "step 260 , total_loss: 1.0870, data_loss: 1.0870\n",
            "step 280 , total_loss: 1.0771, data_loss: 1.0771\n",
            "step 300 , total_loss: 1.0377, data_loss: 1.0377\n",
            "step 320 , total_loss: 1.0164, data_loss: 1.0164\n",
            "step 340 , total_loss: 0.9146, data_loss: 0.9146\n",
            "step 360 , total_loss: 1.0121, data_loss: 1.0121\n",
            "step 380 , total_loss: 1.0295, data_loss: 1.0295\n",
            "step 400 , total_loss: 1.0017, data_loss: 1.0017\n",
            "step 420 , total_loss: 1.0446, data_loss: 1.0446\n",
            "step 440 , total_loss: 0.9683, data_loss: 0.9683\n",
            "step 460 , total_loss: 1.0930, data_loss: 1.0930\n",
            "step 480 , total_loss: 1.0143, data_loss: 1.0143\n",
            "step 500 , total_loss: 1.0732, data_loss: 1.0732\n",
            "step 520 , total_loss: 0.9600, data_loss: 0.9600\n",
            "step 540 , total_loss: 1.0658, data_loss: 1.0658\n",
            "step 560 , total_loss: 1.0104, data_loss: 1.0104\n",
            "step 580 , total_loss: 0.9904, data_loss: 0.9904\n",
            "step 600 , total_loss: 1.0171, data_loss: 1.0171\n",
            "step 620 , total_loss: 0.9793, data_loss: 0.9793\n",
            "step 640 , total_loss: 1.1162, data_loss: 1.1162\n",
            "eval valid at epoch 4: auc:0.7715,logloss:0.4501,mean_mrr:0.7017,ndcg@2:0.6696,ndcg@4:0.7709,ndcg@6:0.7773,group_auc:0.7921\n",
            "step 20 , total_loss: 0.9975, data_loss: 0.9975\n",
            "step 40 , total_loss: 1.0099, data_loss: 1.0099\n",
            "step 60 , total_loss: 0.9323, data_loss: 0.9323\n",
            "step 80 , total_loss: 0.9759, data_loss: 0.9759\n",
            "step 100 , total_loss: 0.9385, data_loss: 0.9385\n",
            "step 120 , total_loss: 0.9668, data_loss: 0.9668\n",
            "step 140 , total_loss: 0.9331, data_loss: 0.9331\n",
            "step 160 , total_loss: 1.0739, data_loss: 1.0739\n",
            "step 180 , total_loss: 1.0095, data_loss: 1.0095\n",
            "step 200 , total_loss: 1.0564, data_loss: 1.0564\n",
            "step 220 , total_loss: 1.0850, data_loss: 1.0850\n",
            "step 240 , total_loss: 0.9730, data_loss: 0.9730\n",
            "step 260 , total_loss: 0.9593, data_loss: 0.9593\n",
            "step 280 , total_loss: 1.0226, data_loss: 1.0226\n",
            "step 300 , total_loss: 0.9786, data_loss: 0.9786\n",
            "step 320 , total_loss: 0.9863, data_loss: 0.9863\n",
            "step 340 , total_loss: 0.9663, data_loss: 0.9663\n",
            "step 360 , total_loss: 0.9322, data_loss: 0.9322\n",
            "step 380 , total_loss: 0.9989, data_loss: 0.9989\n",
            "step 400 , total_loss: 1.0013, data_loss: 1.0013\n",
            "step 420 , total_loss: 1.0498, data_loss: 1.0498\n",
            "step 440 , total_loss: 0.9930, data_loss: 0.9930\n",
            "step 460 , total_loss: 0.9046, data_loss: 0.9046\n",
            "step 480 , total_loss: 0.9674, data_loss: 0.9674\n",
            "step 500 , total_loss: 1.0160, data_loss: 1.0160\n",
            "step 520 , total_loss: 0.9793, data_loss: 0.9793\n",
            "step 540 , total_loss: 0.9515, data_loss: 0.9515\n",
            "step 560 , total_loss: 0.9985, data_loss: 0.9985\n",
            "step 580 , total_loss: 0.9551, data_loss: 0.9551\n",
            "step 600 , total_loss: 1.0049, data_loss: 1.0049\n",
            "step 620 , total_loss: 0.9795, data_loss: 0.9795\n",
            "step 640 , total_loss: 1.0411, data_loss: 1.0411\n",
            "eval valid at epoch 5: auc:0.7994,logloss:0.4615,mean_mrr:0.7225,ndcg@2:0.694,ndcg@4:0.7878,ndcg@6:0.7928,group_auc:0.8083\n",
            "step 20 , total_loss: 1.0010, data_loss: 1.0010\n",
            "step 40 , total_loss: 1.0075, data_loss: 1.0075\n",
            "step 60 , total_loss: 0.9366, data_loss: 0.9366\n",
            "step 80 , total_loss: 0.9461, data_loss: 0.9461\n",
            "step 100 , total_loss: 1.0073, data_loss: 1.0073\n",
            "step 120 , total_loss: 1.0080, data_loss: 1.0080\n",
            "step 140 , total_loss: 0.9407, data_loss: 0.9407\n",
            "step 160 , total_loss: 1.0038, data_loss: 1.0038\n",
            "step 180 , total_loss: 0.9605, data_loss: 0.9605\n",
            "step 200 , total_loss: 0.9926, data_loss: 0.9926\n",
            "step 220 , total_loss: 1.0118, data_loss: 1.0118\n",
            "step 240 , total_loss: 1.0111, data_loss: 1.0111\n",
            "step 260 , total_loss: 0.9711, data_loss: 0.9711\n",
            "step 280 , total_loss: 0.9867, data_loss: 0.9867\n",
            "step 300 , total_loss: 1.0326, data_loss: 1.0326\n",
            "step 320 , total_loss: 0.9367, data_loss: 0.9367\n",
            "step 340 , total_loss: 0.9378, data_loss: 0.9378\n",
            "step 360 , total_loss: 1.0117, data_loss: 1.0117\n",
            "step 380 , total_loss: 1.0104, data_loss: 1.0104\n",
            "step 400 , total_loss: 1.0187, data_loss: 1.0187\n",
            "step 420 , total_loss: 0.9822, data_loss: 0.9822\n",
            "step 440 , total_loss: 0.9770, data_loss: 0.9770\n",
            "step 460 , total_loss: 0.9523, data_loss: 0.9523\n",
            "step 480 , total_loss: 1.0694, data_loss: 1.0694\n",
            "step 500 , total_loss: 0.9323, data_loss: 0.9323\n",
            "step 520 , total_loss: 0.9998, data_loss: 0.9998\n",
            "step 540 , total_loss: 1.0462, data_loss: 1.0462\n",
            "step 560 , total_loss: 0.9900, data_loss: 0.9900\n",
            "step 580 , total_loss: 1.0053, data_loss: 1.0053\n",
            "step 600 , total_loss: 1.0375, data_loss: 1.0375\n",
            "step 620 , total_loss: 0.9058, data_loss: 0.9058\n",
            "step 640 , total_loss: 0.9718, data_loss: 0.9718\n",
            "eval valid at epoch 6: auc:0.8165,logloss:0.5647,mean_mrr:0.7373,ndcg@2:0.7162,ndcg@4:0.7994,ndcg@6:0.8041,group_auc:0.8221\n",
            "step 20 , total_loss: 1.0506, data_loss: 1.0506\n",
            "step 40 , total_loss: 0.8969, data_loss: 0.8969\n",
            "step 60 , total_loss: 0.9516, data_loss: 0.9516\n",
            "step 80 , total_loss: 0.9693, data_loss: 0.9693\n",
            "step 100 , total_loss: 0.9144, data_loss: 0.9144\n",
            "step 120 , total_loss: 0.9829, data_loss: 0.9829\n",
            "step 140 , total_loss: 0.9384, data_loss: 0.9384\n",
            "step 160 , total_loss: 0.9555, data_loss: 0.9555\n",
            "step 180 , total_loss: 0.9564, data_loss: 0.9564\n",
            "step 200 , total_loss: 1.0265, data_loss: 1.0265\n",
            "step 220 , total_loss: 0.9189, data_loss: 0.9189\n",
            "step 240 , total_loss: 0.9741, data_loss: 0.9741\n",
            "step 260 , total_loss: 0.9236, data_loss: 0.9236\n",
            "step 280 , total_loss: 0.9302, data_loss: 0.9302\n",
            "step 300 , total_loss: 0.9454, data_loss: 0.9454\n",
            "step 320 , total_loss: 0.9806, data_loss: 0.9806\n",
            "step 340 , total_loss: 0.9706, data_loss: 0.9706\n",
            "step 360 , total_loss: 1.0086, data_loss: 1.0086\n",
            "step 380 , total_loss: 0.9318, data_loss: 0.9318\n",
            "step 400 , total_loss: 0.9534, data_loss: 0.9534\n",
            "step 420 , total_loss: 0.9470, data_loss: 0.9470\n",
            "step 440 , total_loss: 1.0713, data_loss: 1.0713\n",
            "step 460 , total_loss: 0.9945, data_loss: 0.9945\n",
            "step 480 , total_loss: 0.9583, data_loss: 0.9583\n",
            "step 500 , total_loss: 0.9432, data_loss: 0.9432\n",
            "step 520 , total_loss: 0.9148, data_loss: 0.9148\n",
            "step 540 , total_loss: 0.9773, data_loss: 0.9773\n",
            "step 560 , total_loss: 1.0048, data_loss: 1.0048\n",
            "step 580 , total_loss: 0.9273, data_loss: 0.9273\n",
            "step 600 , total_loss: 1.0392, data_loss: 1.0392\n",
            "step 620 , total_loss: 0.9043, data_loss: 0.9043\n",
            "step 640 , total_loss: 0.9487, data_loss: 0.9487\n",
            "eval valid at epoch 7: auc:0.8193,logloss:0.5266,mean_mrr:0.7408,ndcg@2:0.7186,ndcg@4:0.8025,ndcg@6:0.8067,group_auc:0.8253\n",
            "step 20 , total_loss: 0.9213, data_loss: 0.9213\n",
            "step 40 , total_loss: 0.9418, data_loss: 0.9418\n",
            "step 60 , total_loss: 0.9848, data_loss: 0.9848\n",
            "step 80 , total_loss: 0.8612, data_loss: 0.8612\n",
            "step 100 , total_loss: 0.9676, data_loss: 0.9676\n",
            "step 120 , total_loss: 0.9611, data_loss: 0.9611\n",
            "step 140 , total_loss: 0.9971, data_loss: 0.9971\n",
            "step 160 , total_loss: 0.9417, data_loss: 0.9417\n",
            "step 180 , total_loss: 0.9017, data_loss: 0.9017\n",
            "step 200 , total_loss: 0.9615, data_loss: 0.9615\n",
            "step 220 , total_loss: 0.9693, data_loss: 0.9693\n",
            "step 240 , total_loss: 0.9163, data_loss: 0.9163\n",
            "step 260 , total_loss: 0.9206, data_loss: 0.9206\n",
            "step 280 , total_loss: 1.0236, data_loss: 1.0236\n",
            "step 300 , total_loss: 0.8935, data_loss: 0.8935\n",
            "step 320 , total_loss: 0.9448, data_loss: 0.9448\n",
            "step 340 , total_loss: 0.9247, data_loss: 0.9247\n",
            "step 360 , total_loss: 0.9026, data_loss: 0.9026\n",
            "step 380 , total_loss: 0.9817, data_loss: 0.9817\n",
            "step 400 , total_loss: 0.9564, data_loss: 0.9564\n",
            "step 420 , total_loss: 0.9019, data_loss: 0.9019\n",
            "step 440 , total_loss: 0.9865, data_loss: 0.9865\n",
            "step 460 , total_loss: 0.9214, data_loss: 0.9214\n",
            "step 480 , total_loss: 1.0011, data_loss: 1.0011\n",
            "step 500 , total_loss: 0.9994, data_loss: 0.9994\n",
            "step 520 , total_loss: 0.9215, data_loss: 0.9215\n",
            "step 540 , total_loss: 0.9815, data_loss: 0.9815\n",
            "step 560 , total_loss: 1.0106, data_loss: 1.0106\n",
            "step 580 , total_loss: 0.9505, data_loss: 0.9505\n",
            "step 600 , total_loss: 0.9840, data_loss: 0.9840\n",
            "step 620 , total_loss: 0.9749, data_loss: 0.9749\n",
            "step 640 , total_loss: 0.9574, data_loss: 0.9574\n",
            "eval valid at epoch 8: auc:0.8324,logloss:0.5115,mean_mrr:0.7538,ndcg@2:0.7368,ndcg@4:0.8134,ndcg@6:0.8165,group_auc:0.837\n",
            "step 20 , total_loss: 0.8780, data_loss: 0.8780\n",
            "step 40 , total_loss: 0.8937, data_loss: 0.8937\n",
            "step 60 , total_loss: 0.9529, data_loss: 0.9529\n",
            "step 80 , total_loss: 0.9680, data_loss: 0.9680\n",
            "step 100 , total_loss: 0.9619, data_loss: 0.9619\n",
            "step 120 , total_loss: 0.9459, data_loss: 0.9459\n",
            "step 140 , total_loss: 0.9378, data_loss: 0.9378\n",
            "step 160 , total_loss: 0.9625, data_loss: 0.9625\n",
            "step 180 , total_loss: 0.9353, data_loss: 0.9353\n",
            "step 200 , total_loss: 0.8374, data_loss: 0.8374\n",
            "step 220 , total_loss: 0.9309, data_loss: 0.9309\n",
            "step 240 , total_loss: 0.9981, data_loss: 0.9981\n",
            "step 260 , total_loss: 0.9473, data_loss: 0.9473\n",
            "step 280 , total_loss: 0.8800, data_loss: 0.8800\n",
            "step 300 , total_loss: 0.9253, data_loss: 0.9253\n",
            "step 320 , total_loss: 1.0053, data_loss: 1.0053\n",
            "step 340 , total_loss: 0.9568, data_loss: 0.9568\n",
            "step 360 , total_loss: 0.9265, data_loss: 0.9265\n",
            "step 380 , total_loss: 0.9745, data_loss: 0.9745\n",
            "step 400 , total_loss: 0.9742, data_loss: 0.9742\n",
            "step 420 , total_loss: 0.9795, data_loss: 0.9795\n",
            "step 440 , total_loss: 0.9057, data_loss: 0.9057\n",
            "step 460 , total_loss: 0.9519, data_loss: 0.9519\n",
            "step 480 , total_loss: 0.9750, data_loss: 0.9750\n",
            "step 500 , total_loss: 0.9408, data_loss: 0.9408\n",
            "step 520 , total_loss: 0.9867, data_loss: 0.9867\n",
            "step 540 , total_loss: 0.9541, data_loss: 0.9541\n",
            "step 560 , total_loss: 0.9087, data_loss: 0.9087\n",
            "step 580 , total_loss: 0.9670, data_loss: 0.9670\n",
            "step 600 , total_loss: 0.9318, data_loss: 0.9318\n",
            "step 620 , total_loss: 0.9942, data_loss: 0.9942\n",
            "step 640 , total_loss: 0.9481, data_loss: 0.9481\n",
            "eval valid at epoch 9: auc:0.8177,logloss:0.4961,mean_mrr:0.7421,ndcg@2:0.7197,ndcg@4:0.8037,ndcg@6:0.8077,group_auc:0.826\n",
            "step 20 , total_loss: 0.9677, data_loss: 0.9677\n",
            "step 40 , total_loss: 0.8974, data_loss: 0.8974\n",
            "step 60 , total_loss: 0.9680, data_loss: 0.9680\n",
            "step 80 , total_loss: 0.8742, data_loss: 0.8742\n",
            "step 100 , total_loss: 0.9234, data_loss: 0.9234\n",
            "step 120 , total_loss: 0.9299, data_loss: 0.9299\n",
            "step 140 , total_loss: 0.8864, data_loss: 0.8864\n",
            "step 160 , total_loss: 0.9207, data_loss: 0.9207\n",
            "step 180 , total_loss: 0.8595, data_loss: 0.8595\n",
            "step 200 , total_loss: 0.9042, data_loss: 0.9042\n",
            "step 220 , total_loss: 0.8143, data_loss: 0.8143\n",
            "step 240 , total_loss: 0.9160, data_loss: 0.9160\n",
            "step 260 , total_loss: 0.9261, data_loss: 0.9261\n",
            "step 280 , total_loss: 0.9443, data_loss: 0.9443\n",
            "step 300 , total_loss: 0.9858, data_loss: 0.9858\n",
            "step 320 , total_loss: 0.9906, data_loss: 0.9906\n",
            "step 340 , total_loss: 0.8510, data_loss: 0.8510\n",
            "step 360 , total_loss: 0.9312, data_loss: 0.9312\n",
            "step 380 , total_loss: 0.9527, data_loss: 0.9527\n",
            "step 400 , total_loss: 0.9086, data_loss: 0.9086\n",
            "step 420 , total_loss: 0.9107, data_loss: 0.9107\n",
            "step 440 , total_loss: 0.9443, data_loss: 0.9443\n",
            "step 460 , total_loss: 0.9097, data_loss: 0.9097\n",
            "step 480 , total_loss: 0.9191, data_loss: 0.9191\n",
            "step 500 , total_loss: 0.8762, data_loss: 0.8762\n",
            "step 520 , total_loss: 0.9412, data_loss: 0.9412\n",
            "step 540 , total_loss: 0.9297, data_loss: 0.9297\n",
            "step 560 , total_loss: 0.9546, data_loss: 0.9546\n",
            "step 580 , total_loss: 0.9019, data_loss: 0.9019\n",
            "step 600 , total_loss: 0.9550, data_loss: 0.9550\n",
            "step 620 , total_loss: 0.8918, data_loss: 0.8918\n",
            "step 640 , total_loss: 0.9568, data_loss: 0.9568\n",
            "eval valid at epoch 10: auc:0.7989,logloss:0.6016,mean_mrr:0.7222,ndcg@2:0.6948,ndcg@4:0.7864,ndcg@6:0.7926,group_auc:0.8079\n",
            "[(1, {'auc': 0.7495, 'logloss': 0.6706, 'mean_mrr': 0.6787, 'ndcg@2': 0.6362, 'ndcg@4': 0.7506, 'ndcg@6': 0.7598, 'group_auc': 0.7693}), (2, {'auc': 0.7377, 'logloss': 0.4539, 'mean_mrr': 0.6752, 'ndcg@2': 0.6328, 'ndcg@4': 0.7493, 'ndcg@6': 0.7573, 'group_auc': 0.7681}), (3, {'auc': 0.7765, 'logloss': 0.4503, 'mean_mrr': 0.6965, 'ndcg@2': 0.6611, 'ndcg@4': 0.7669, 'ndcg@6': 0.7734, 'group_auc': 0.7878}), (4, {'auc': 0.7715, 'logloss': 0.4501, 'mean_mrr': 0.7017, 'ndcg@2': 0.6696, 'ndcg@4': 0.7709, 'ndcg@6': 0.7773, 'group_auc': 0.7921}), (5, {'auc': 0.7994, 'logloss': 0.4615, 'mean_mrr': 0.7225, 'ndcg@2': 0.694, 'ndcg@4': 0.7878, 'ndcg@6': 0.7928, 'group_auc': 0.8083}), (6, {'auc': 0.8165, 'logloss': 0.5647, 'mean_mrr': 0.7373, 'ndcg@2': 0.7162, 'ndcg@4': 0.7994, 'ndcg@6': 0.8041, 'group_auc': 0.8221}), (7, {'auc': 0.8193, 'logloss': 0.5266, 'mean_mrr': 0.7408, 'ndcg@2': 0.7186, 'ndcg@4': 0.8025, 'ndcg@6': 0.8067, 'group_auc': 0.8253}), (8, {'auc': 0.8324, 'logloss': 0.5115, 'mean_mrr': 0.7538, 'ndcg@2': 0.7368, 'ndcg@4': 0.8134, 'ndcg@6': 0.8165, 'group_auc': 0.837}), (9, {'auc': 0.8177, 'logloss': 0.4961, 'mean_mrr': 0.7421, 'ndcg@2': 0.7197, 'ndcg@4': 0.8037, 'ndcg@6': 0.8077, 'group_auc': 0.826}), (10, {'auc': 0.7989, 'logloss': 0.6016, 'mean_mrr': 0.7222, 'ndcg@2': 0.6948, 'ndcg@4': 0.7864, 'ndcg@6': 0.7926, 'group_auc': 0.8079})]\n",
            "best epoch: 8\n",
            "Time cost for training is 133.29 mins\n"
          ]
        }
      ],
      "source": [
        "with Timer() as train_time:\n",
        "    model = model.fit(train_file, valid_file, valid_num_ngs=valid_num_ngs) \n",
        "\n",
        "# valid_num_ngs is the number of negative lines after each positive line in your valid_file \n",
        "# we will evaluate the performance of model on valid_file every epoch\n",
        "print('Time cost for training is {0:.2f} mins'.format(train_time.interval/60.0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gmm3DPjbiY9N",
        "outputId": "6dbb1a1d-8b36-49b1-ef5e-f4a807dddfde"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'auc': 0.7988, 'logloss': 0.6248, 'mean_mrr': 0.5647, 'ndcg@2': 0.4828, 'ndcg@4': 0.6079, 'ndcg@6': 0.6545, 'group_auc': 0.8083}\n"
          ]
        }
      ],
      "source": [
        "res_syn = model.run_eval(test_file, num_ngs=test_num_ngs)\n",
        "print(res_syn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "VB7MBFNkjl5e",
        "outputId": "12b143c9-1d98-4d99-9136-8c779de2d832"
      },
      "outputs": [
        {
          "data": {
            "application/scrapbook.scrap.json+json": {
              "data": {
                "auc": 0.7988,
                "group_auc": 0.8083,
                "logloss": 0.6248,
                "mean_mrr": 0.5647,
                "ndcg@2": 0.4828,
                "ndcg@4": 0.6079,
                "ndcg@6": 0.6545
              },
              "encoder": "json",
              "name": "res_syn",
              "version": 1
            }
          },
          "metadata": {
            "scrapbook": {
              "data": true,
              "display": false,
              "name": "res_syn"
            }
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "sb.glue(\"res_syn\", res_syn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "h2uxYGfFjpL9"
      },
      "outputs": [],
      "source": [
        "model = model.predict(test_file, output_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wGZ26pt-ujeo",
        "outputId": "5cbe4b07-34e3-4891-c64b-40b2a70a6fa7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\조예은\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\recommenders\\models\\deeprec\\models\\sequential\\caser.py:102: UserWarning: `tf.layers.conv1d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv1D` instead.\n",
            "  return tf.compat.v1.layers.conv1d(\n",
            "C:\\Users\\조예은\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\legacy_tf_layers\\convolutional.py:294: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
            "  return layer.apply(inputs)\n",
            "C:\\Users\\조예은\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\recommenders\\models\\deeprec\\models\\sequential\\caser.py:66: UserWarning: `tf.layers.flatten` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Flatten` instead.\n",
            "  out_v = tf.compat.v1.layers.flatten(out_v)\n",
            "C:\\Users\\조예은\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\legacy_tf_layers\\core.py:541: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
            "  return layer.apply(inputs)\n",
            "C:\\Users\\조예은\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\recommenders\\models\\deeprec\\models\\base_model.py:701: UserWarning: `tf.layers.batch_normalization` is deprecated and will be removed in a future version. Please use `tf.keras.layers.BatchNormalization` instead. In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.BatchNormalization` documentation).\n",
            "  curr_hidden_nn_layer = tf.compat.v1.layers.batch_normalization(\n",
            "C:\\Users\\조예은\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\legacy_tf_layers\\normalization.py:463: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
            "  return layer.apply(inputs, training=training)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loading saved model in resources/model\\caser/best_model\n"
          ]
        }
      ],
      "source": [
        "model_best_trained = SeqModel(hparams, input_creator, seed=RANDOM_SEED)\n",
        "path_best_trained = os.path.join(hparams.MODEL_DIR, \"best_model\")\n",
        "print('loading saved model in {0}'.format(path_best_trained))\n",
        "model_best_trained.load_model(path_best_trained)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fKtBHxlGu5-K",
        "outputId": "d398f3b9-bbba-4707-fae1-7c1c817a7e66"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'auc': 0.8284,\n",
              " 'logloss': 0.5215,\n",
              " 'mean_mrr': 0.5944,\n",
              " 'ndcg@2': 0.5212,\n",
              " 'ndcg@4': 0.6423,\n",
              " 'ndcg@6': 0.6831,\n",
              " 'group_auc': 0.83}"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_best_trained.run_eval(test_file, num_ngs=test_num_ngs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XH7L5VOQvARx",
        "outputId": "46721757-0218-4a41-8557-fcd7806b0852"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<recommenders.models.deeprec.models.sequential.caser.CaserModel at 0x1c113e832e0>"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_best_trained.predict(test_file, output_file)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "GRU4REC.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
