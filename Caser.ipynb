{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sknnn2xs-AHi",
        "outputId": "a4f15d3b-23ff-42f1-8d49-ded97e42bf52"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "System version: 3.8.9 (tags/v3.8.9:a743f81, Apr  6 2021, 14:02:34) [MSC v.1928 64 bit (AMD64)]\n",
            "Tensorflow version: 2.8.0\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "import logging\n",
        "import papermill as pm\n",
        "import scrapbook as sb\n",
        "from tempfile import TemporaryDirectory\n",
        "import numpy as np\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.get_logger().setLevel('ERROR') # only show error messages\n",
        "\n",
        "from recommenders.utils.timer import Timer\n",
        "from recommenders.utils.constants import SEED\n",
        "from recommenders.models.deeprec.deeprec_utils import (\n",
        "    prepare_hparams\n",
        ")\n",
        "\n",
        "from resources.data_preprocessing import data_preprocessing\n",
        "# from recommenders.datasets.amazon_reviews import download_and_extract, data_preprocessing\n",
        "from recommenders.datasets.download_utils import maybe_download\n",
        "\n",
        "\n",
        "# from recommenders.models.deeprec.models.sequential.sli_rec import SLI_RECModel as SeqModel\n",
        "####  to use the other model, use one of the following lines:\n",
        "#from recommenders.models.deeprec.models.sequential.asvd import A2SVDModel as SeqModel\n",
        "from recommenders.models.deeprec.models.sequential.caser import CaserModel as SeqModel\n",
        "# from recommenders.models.deeprec.models.sequential.gru4rec import GRU4RecModel as SeqModel\n",
        "# from recommenders.models.deeprec.models.sequential.sum import SUMModel as SeqModel\n",
        "\n",
        "#from recommenders.models.deeprec.models.sequential.nextitnet import NextItNetModel\n",
        "\n",
        "from recommenders.models.deeprec.io.sequential_iterator import SequentialIterator\n",
        "#from recommenders.models.deeprec.io.nextitnet_iterator import NextItNetIterator\n",
        "\n",
        "print(\"System version: {}\".format(sys.version))\n",
        "print(\"Tensorflow version: {}\".format(tf.__version__))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "sxtfUZfP-ZWR"
      },
      "outputs": [],
      "source": [
        "##  ATTENTION: change to the corresponding config file, e.g., caser.yaml for CaserModel, sum.yaml for SUMModel\n",
        "# yaml_file = '../../recommenders/models/deeprec/config/sli_rec.yaml'  \n",
        "yaml_file = './caser.yaml'  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "_NnzhN4h_rI5"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 10\n",
        "BATCH_SIZE = 400\n",
        "RANDOM_SEED = SEED  # Set None for non-deterministic result\n",
        "\n",
        "data_path = os.path.join(\"resources/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "0RAMNcI3AClc"
      },
      "outputs": [],
      "source": [
        "# for test\n",
        "train_file = os.path.join(data_path, r'train_data')\n",
        "valid_file = os.path.join(data_path, r'valid_data')\n",
        "test_file = os.path.join(data_path, r'test_data')\n",
        "user_vocab = os.path.join(data_path, r'user_vocab.pkl')\n",
        "item_vocab = os.path.join(data_path, r'item_vocab.pkl')\n",
        "cate_vocab = os.path.join(data_path, r'cate_vocab.pkl')\n",
        "output_file = os.path.join(data_path, r'output_caser.txt')\n",
        "\n",
        "# reviews_name = 'json'\n",
        "# meta_name = 'json'\n",
        "# reviews_file = os.path.join(data_path, reviews_name)\n",
        "# meta_file = os.path.join(data_path, meta_name)\n",
        "train_num_ngs = 4 # number of negative instances with a positive instance for training\n",
        "valid_num_ngs = 4 # number of negative instances with a positive instance for validation\n",
        "test_num_ngs = 9 # number of negative instances with a positive instance for testing\n",
        "sample_rate = 0.01 # sample a small item set for training and testing here for fast example\n",
        "\n",
        "input_files = [data_path, train_file, valid_file, test_file, user_vocab, item_vocab, cate_vocab]\n",
        "\n",
        "if not os.path.exists(train_file):\n",
        "    # download_and_extract(reviews_name, reviews_file)\n",
        "    # download_and_extract(meta_name, meta_file)\n",
        "    data_preprocessing(*input_files, sample_rate=sample_rate, valid_num_ngs=valid_num_ngs, test_num_ngs=test_num_ngs)\n",
        "    #### uncomment this for the NextItNet model, because it does not need to unfold the user history\n",
        "    # data_preprocessing(*input_files, sample_rate=sample_rate, valid_num_ngs=valid_num_ngs, test_num_ngs=test_num_ngs, is_history_expanding=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "rwTMO3WEWaCu"
      },
      "outputs": [],
      "source": [
        "### NOTE:  \n",
        "### remember to use `_create_vocab(train_file, user_vocab, item_vocab, cate_vocab)` to generate the user_vocab, item_vocab and cate_vocab files, if you are using your own dataset rather than using our demo Amazon dataset.\n",
        "hparams = prepare_hparams(yaml_file, \n",
        "                          embed_l2=0., \n",
        "                          layer_l2=0., \n",
        "                          learning_rate=0.001,  # set to 0.01 if batch normalization is disable\n",
        "                          epochs=EPOCHS,\n",
        "                          batch_size=BATCH_SIZE,\n",
        "                          show_step=20,\n",
        "                          MODEL_DIR=os.path.join(data_path, \"model\", \"caser/\"),\n",
        "                          SUMMARIES_DIR=os.path.join(data_path, \"summary\", \"caser/\"),\n",
        "                          user_vocab=user_vocab,\n",
        "                          item_vocab=item_vocab,\n",
        "                          cate_vocab=cate_vocab,\n",
        "                          need_sample=True,\n",
        "                          train_num_ngs=train_num_ngs, # provides the number of negative instances for each positive instance for loss computation.\n",
        "            )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "4Rvq7gFluVPq"
      },
      "outputs": [],
      "source": [
        "input_creator = SequentialIterator\n",
        "#### uncomment this for the NextItNet model, because it needs a special data iterator for training\n",
        "#input_creator = NextItNetIterator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VFNfgcKNuaVv",
        "outputId": "2fa083f0-f73d-452a-efef-12151b40c7c5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\조예은\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\recommenders\\models\\deeprec\\models\\sequential\\caser.py:102: UserWarning: `tf.layers.conv1d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv1D` instead.\n",
            "  return tf.compat.v1.layers.conv1d(\n",
            "C:\\Users\\조예은\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\legacy_tf_layers\\convolutional.py:294: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
            "  return layer.apply(inputs)\n",
            "C:\\Users\\조예은\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\recommenders\\models\\deeprec\\models\\sequential\\caser.py:66: UserWarning: `tf.layers.flatten` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Flatten` instead.\n",
            "  out_v = tf.compat.v1.layers.flatten(out_v)\n",
            "C:\\Users\\조예은\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\legacy_tf_layers\\core.py:541: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
            "  return layer.apply(inputs)\n",
            "C:\\Users\\조예은\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\recommenders\\models\\deeprec\\models\\base_model.py:701: UserWarning: `tf.layers.batch_normalization` is deprecated and will be removed in a future version. Please use `tf.keras.layers.BatchNormalization` instead. In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.BatchNormalization` documentation).\n",
            "  curr_hidden_nn_layer = tf.compat.v1.layers.batch_normalization(\n",
            "C:\\Users\\조예은\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\legacy_tf_layers\\normalization.py:463: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
            "  return layer.apply(inputs, training=training)\n"
          ]
        }
      ],
      "source": [
        "model = SeqModel(hparams, input_creator, seed=RANDOM_SEED)\n",
        "\n",
        "## sometimes we don't want to train a model from scratch\n",
        "## then we can load a pre-trained model like this: \n",
        "#model.load_model(r'your_model_path')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8AQxPlk-uheU",
        "outputId": "55a5328a-44bd-4213-b10c-96f26313586a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'auc': 0.5014, 'logloss': 0.6932, 'mean_mrr': 0.2802, 'ndcg@2': 0.1474, 'ndcg@4': 0.2426, 'ndcg@6': 0.3137, 'group_auc': 0.5059}\n"
          ]
        }
      ],
      "source": [
        "# test_num_ngs is the number of negative lines after each positive line in your test_file\n",
        "print(model.run_eval(test_file, num_ngs=test_num_ngs)) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QFKQu9rGu0jq",
        "outputId": "c93b6ebf-50cc-43ea-e620-418e4537ad31"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 20 , total_loss: 1.5319, data_loss: 1.5319\n",
            "step 40 , total_loss: 1.4415, data_loss: 1.4415\n",
            "step 60 , total_loss: 1.3981, data_loss: 1.3981\n",
            "step 80 , total_loss: 1.3906, data_loss: 1.3906\n",
            "step 100 , total_loss: 1.3616, data_loss: 1.3616\n",
            "step 120 , total_loss: 1.3522, data_loss: 1.3522\n",
            "step 140 , total_loss: 1.3028, data_loss: 1.3028\n",
            "step 160 , total_loss: 1.2885, data_loss: 1.2885\n",
            "step 180 , total_loss: 1.3292, data_loss: 1.3292\n",
            "step 200 , total_loss: 1.2643, data_loss: 1.2643\n",
            "step 220 , total_loss: 1.2204, data_loss: 1.2204\n",
            "step 240 , total_loss: 1.2101, data_loss: 1.2101\n",
            "step 260 , total_loss: 1.2226, data_loss: 1.2226\n",
            "step 280 , total_loss: 1.2725, data_loss: 1.2725\n",
            "step 300 , total_loss: 1.2822, data_loss: 1.2822\n",
            "step 320 , total_loss: 1.2381, data_loss: 1.2381\n",
            "step 340 , total_loss: 1.1837, data_loss: 1.1837\n",
            "step 360 , total_loss: 1.2315, data_loss: 1.2315\n",
            "step 380 , total_loss: 1.2034, data_loss: 1.2034\n",
            "step 400 , total_loss: 1.2494, data_loss: 1.2494\n",
            "step 420 , total_loss: 1.1541, data_loss: 1.1541\n",
            "step 440 , total_loss: 1.2836, data_loss: 1.2836\n",
            "step 460 , total_loss: 1.1905, data_loss: 1.1905\n",
            "step 480 , total_loss: 1.2606, data_loss: 1.2606\n",
            "step 500 , total_loss: 1.1510, data_loss: 1.1510\n",
            "step 520 , total_loss: 1.2218, data_loss: 1.2218\n",
            "step 540 , total_loss: 1.1748, data_loss: 1.1748\n",
            "step 560 , total_loss: 1.2169, data_loss: 1.2169\n",
            "step 580 , total_loss: 1.1942, data_loss: 1.1942\n",
            "step 600 , total_loss: 1.1588, data_loss: 1.1588\n",
            "step 620 , total_loss: 1.1578, data_loss: 1.1578\n",
            "step 640 , total_loss: 1.1658, data_loss: 1.1658\n",
            "step 660 , total_loss: 1.2072, data_loss: 1.2072\n",
            "step 680 , total_loss: 1.1946, data_loss: 1.1946\n",
            "step 700 , total_loss: 1.2183, data_loss: 1.2183\n",
            "step 720 , total_loss: 1.2192, data_loss: 1.2192\n",
            "step 740 , total_loss: 1.1653, data_loss: 1.1653\n",
            "step 760 , total_loss: 1.1525, data_loss: 1.1525\n",
            "step 780 , total_loss: 1.1748, data_loss: 1.1748\n",
            "step 800 , total_loss: 1.1744, data_loss: 1.1744\n",
            "eval valid at epoch 1: auc:0.7176,logloss:0.5924,mean_mrr:0.6809,ndcg@2:0.6341,ndcg@4:0.7431,ndcg@6:0.7609,group_auc:0.7566\n",
            "step 20 , total_loss: 1.1939, data_loss: 1.1939\n",
            "step 40 , total_loss: 1.2141, data_loss: 1.2141\n",
            "step 60 , total_loss: 1.1530, data_loss: 1.1530\n",
            "step 80 , total_loss: 1.1141, data_loss: 1.1141\n",
            "step 100 , total_loss: 1.1156, data_loss: 1.1156\n",
            "step 120 , total_loss: 1.1947, data_loss: 1.1947\n",
            "step 140 , total_loss: 1.1061, data_loss: 1.1061\n",
            "step 160 , total_loss: 1.1407, data_loss: 1.1407\n",
            "step 180 , total_loss: 1.0925, data_loss: 1.0925\n",
            "step 200 , total_loss: 1.0920, data_loss: 1.0920\n",
            "step 220 , total_loss: 1.0764, data_loss: 1.0764\n",
            "step 240 , total_loss: 1.0259, data_loss: 1.0259\n",
            "step 260 , total_loss: 1.1892, data_loss: 1.1892\n",
            "step 280 , total_loss: 1.1578, data_loss: 1.1578\n",
            "step 300 , total_loss: 1.1446, data_loss: 1.1446\n",
            "step 320 , total_loss: 1.0969, data_loss: 1.0969\n",
            "step 340 , total_loss: 1.1099, data_loss: 1.1099\n",
            "step 360 , total_loss: 1.1819, data_loss: 1.1819\n",
            "step 380 , total_loss: 1.1135, data_loss: 1.1135\n",
            "step 400 , total_loss: 1.1417, data_loss: 1.1417\n",
            "step 420 , total_loss: 1.1887, data_loss: 1.1887\n",
            "step 440 , total_loss: 1.0815, data_loss: 1.0815\n",
            "step 460 , total_loss: 1.1075, data_loss: 1.1075\n",
            "step 480 , total_loss: 1.0610, data_loss: 1.0610\n",
            "step 500 , total_loss: 1.0880, data_loss: 1.0880\n",
            "step 520 , total_loss: 1.1457, data_loss: 1.1457\n",
            "step 540 , total_loss: 1.1447, data_loss: 1.1447\n",
            "step 560 , total_loss: 1.0801, data_loss: 1.0801\n",
            "step 580 , total_loss: 1.0715, data_loss: 1.0715\n",
            "step 600 , total_loss: 1.1202, data_loss: 1.1202\n",
            "step 620 , total_loss: 1.1795, data_loss: 1.1795\n",
            "step 640 , total_loss: 1.0634, data_loss: 1.0634\n",
            "step 660 , total_loss: 1.0635, data_loss: 1.0635\n",
            "step 680 , total_loss: 1.0297, data_loss: 1.0297\n",
            "step 700 , total_loss: 1.1611, data_loss: 1.1611\n",
            "step 720 , total_loss: 1.0603, data_loss: 1.0603\n",
            "step 740 , total_loss: 1.0391, data_loss: 1.0391\n",
            "step 760 , total_loss: 1.0520, data_loss: 1.0520\n",
            "step 780 , total_loss: 1.0483, data_loss: 1.0483\n",
            "step 800 , total_loss: 0.9828, data_loss: 0.9828\n",
            "eval valid at epoch 2: auc:0.7222,logloss:0.4806,mean_mrr:0.6837,ndcg@2:0.6378,ndcg@4:0.7486,ndcg@6:0.7631,group_auc:0.7628\n",
            "step 20 , total_loss: 1.0756, data_loss: 1.0756\n",
            "step 40 , total_loss: 1.0651, data_loss: 1.0651\n",
            "step 60 , total_loss: 1.0738, data_loss: 1.0738\n",
            "step 80 , total_loss: 1.0345, data_loss: 1.0345\n",
            "step 100 , total_loss: 0.9955, data_loss: 0.9955\n",
            "step 120 , total_loss: 1.0523, data_loss: 1.0523\n",
            "step 140 , total_loss: 1.1118, data_loss: 1.1118\n",
            "step 160 , total_loss: 1.1043, data_loss: 1.1043\n",
            "step 180 , total_loss: 1.0401, data_loss: 1.0401\n",
            "step 200 , total_loss: 0.9921, data_loss: 0.9921\n",
            "step 220 , total_loss: 1.1004, data_loss: 1.1004\n",
            "step 240 , total_loss: 1.0292, data_loss: 1.0292\n",
            "step 260 , total_loss: 1.0441, data_loss: 1.0441\n",
            "step 280 , total_loss: 1.0373, data_loss: 1.0373\n",
            "step 300 , total_loss: 0.9770, data_loss: 0.9770\n",
            "step 320 , total_loss: 1.0733, data_loss: 1.0733\n",
            "step 340 , total_loss: 1.0710, data_loss: 1.0710\n",
            "step 360 , total_loss: 1.0649, data_loss: 1.0649\n",
            "step 380 , total_loss: 1.0417, data_loss: 1.0417\n",
            "step 400 , total_loss: 1.0630, data_loss: 1.0630\n",
            "step 420 , total_loss: 1.0441, data_loss: 1.0441\n",
            "step 440 , total_loss: 1.1598, data_loss: 1.1598\n",
            "step 460 , total_loss: 1.0767, data_loss: 1.0767\n",
            "step 480 , total_loss: 1.0235, data_loss: 1.0235\n",
            "step 500 , total_loss: 1.0691, data_loss: 1.0691\n",
            "step 520 , total_loss: 1.0044, data_loss: 1.0044\n",
            "step 540 , total_loss: 1.0474, data_loss: 1.0474\n",
            "step 560 , total_loss: 0.9449, data_loss: 0.9449\n",
            "step 580 , total_loss: 1.0680, data_loss: 1.0680\n",
            "step 600 , total_loss: 1.0384, data_loss: 1.0384\n",
            "step 620 , total_loss: 1.0872, data_loss: 1.0872\n",
            "step 640 , total_loss: 1.0092, data_loss: 1.0092\n",
            "step 660 , total_loss: 1.0003, data_loss: 1.0003\n",
            "step 680 , total_loss: 1.0637, data_loss: 1.0637\n",
            "step 700 , total_loss: 1.0418, data_loss: 1.0418\n",
            "step 720 , total_loss: 1.0239, data_loss: 1.0239\n",
            "step 740 , total_loss: 1.0415, data_loss: 1.0415\n",
            "step 760 , total_loss: 0.9535, data_loss: 0.9535\n",
            "step 780 , total_loss: 1.0749, data_loss: 1.0749\n",
            "step 800 , total_loss: 0.9996, data_loss: 0.9996\n",
            "eval valid at epoch 3: auc:0.7767,logloss:0.5106,mean_mrr:0.7187,ndcg@2:0.6856,ndcg@4:0.7816,ndcg@6:0.7898,group_auc:0.8003\n",
            "step 20 , total_loss: 1.1001, data_loss: 1.1001\n",
            "step 40 , total_loss: 1.0281, data_loss: 1.0281\n",
            "step 60 , total_loss: 1.0478, data_loss: 1.0478\n",
            "step 80 , total_loss: 1.0777, data_loss: 1.0777\n",
            "step 100 , total_loss: 0.9438, data_loss: 0.9438\n",
            "step 120 , total_loss: 1.0533, data_loss: 1.0533\n",
            "step 140 , total_loss: 0.9809, data_loss: 0.9809\n",
            "step 160 , total_loss: 1.0423, data_loss: 1.0423\n",
            "step 180 , total_loss: 1.0944, data_loss: 1.0944\n",
            "step 200 , total_loss: 0.9467, data_loss: 0.9467\n",
            "step 220 , total_loss: 1.0429, data_loss: 1.0429\n",
            "step 240 , total_loss: 1.0567, data_loss: 1.0567\n",
            "step 260 , total_loss: 0.9244, data_loss: 0.9244\n",
            "step 280 , total_loss: 0.9656, data_loss: 0.9656\n",
            "step 300 , total_loss: 0.9379, data_loss: 0.9379\n",
            "step 320 , total_loss: 1.1003, data_loss: 1.1003\n",
            "step 340 , total_loss: 1.0254, data_loss: 1.0254\n",
            "step 360 , total_loss: 1.0086, data_loss: 1.0086\n",
            "step 380 , total_loss: 0.9675, data_loss: 0.9675\n",
            "step 400 , total_loss: 0.8906, data_loss: 0.8906\n",
            "step 420 , total_loss: 0.9951, data_loss: 0.9951\n",
            "step 440 , total_loss: 1.0565, data_loss: 1.0565\n",
            "step 460 , total_loss: 1.0130, data_loss: 1.0130\n",
            "step 480 , total_loss: 1.0460, data_loss: 1.0460\n",
            "step 500 , total_loss: 1.0134, data_loss: 1.0134\n",
            "step 520 , total_loss: 1.0221, data_loss: 1.0221\n",
            "step 540 , total_loss: 1.0254, data_loss: 1.0254\n",
            "step 560 , total_loss: 0.9635, data_loss: 0.9635\n",
            "step 580 , total_loss: 1.0426, data_loss: 1.0426\n",
            "step 600 , total_loss: 1.0043, data_loss: 1.0043\n",
            "step 620 , total_loss: 0.9724, data_loss: 0.9724\n",
            "step 640 , total_loss: 1.0287, data_loss: 1.0287\n",
            "step 660 , total_loss: 0.9489, data_loss: 0.9489\n",
            "step 680 , total_loss: 0.9783, data_loss: 0.9783\n",
            "step 700 , total_loss: 0.9019, data_loss: 0.9019\n",
            "step 720 , total_loss: 0.9888, data_loss: 0.9888\n",
            "step 740 , total_loss: 1.0668, data_loss: 1.0668\n",
            "step 760 , total_loss: 1.0124, data_loss: 1.0124\n",
            "step 780 , total_loss: 0.9472, data_loss: 0.9472\n",
            "step 800 , total_loss: 1.0042, data_loss: 1.0042\n",
            "eval valid at epoch 4: auc:0.7932,logloss:0.6692,mean_mrr:0.7225,ndcg@2:0.6909,ndcg@4:0.7839,ndcg@6:0.7926,group_auc:0.8013\n",
            "step 20 , total_loss: 1.0219, data_loss: 1.0219\n",
            "step 40 , total_loss: 0.9449, data_loss: 0.9449\n",
            "step 60 , total_loss: 0.9492, data_loss: 0.9492\n",
            "step 80 , total_loss: 0.9842, data_loss: 0.9842\n",
            "step 100 , total_loss: 0.9963, data_loss: 0.9963\n",
            "step 120 , total_loss: 0.9477, data_loss: 0.9477\n",
            "step 140 , total_loss: 1.0118, data_loss: 1.0118\n",
            "step 160 , total_loss: 0.9702, data_loss: 0.9702\n",
            "step 180 , total_loss: 1.0483, data_loss: 1.0483\n",
            "step 200 , total_loss: 0.9617, data_loss: 0.9617\n",
            "step 220 , total_loss: 1.0461, data_loss: 1.0461\n",
            "step 240 , total_loss: 1.0139, data_loss: 1.0139\n",
            "step 260 , total_loss: 0.9611, data_loss: 0.9611\n",
            "step 280 , total_loss: 0.9869, data_loss: 0.9869\n",
            "step 300 , total_loss: 0.9192, data_loss: 0.9192\n",
            "step 320 , total_loss: 1.0633, data_loss: 1.0633\n",
            "step 340 , total_loss: 0.9401, data_loss: 0.9401\n",
            "step 360 , total_loss: 0.9615, data_loss: 0.9615\n",
            "step 380 , total_loss: 0.9330, data_loss: 0.9330\n",
            "step 400 , total_loss: 0.9250, data_loss: 0.9250\n",
            "step 420 , total_loss: 0.9488, data_loss: 0.9488\n",
            "step 440 , total_loss: 1.0310, data_loss: 1.0310\n",
            "step 460 , total_loss: 0.9482, data_loss: 0.9482\n",
            "step 480 , total_loss: 1.0940, data_loss: 1.0940\n",
            "step 500 , total_loss: 0.9424, data_loss: 0.9424\n",
            "step 520 , total_loss: 0.9356, data_loss: 0.9356\n",
            "step 540 , total_loss: 0.9342, data_loss: 0.9342\n",
            "step 560 , total_loss: 0.9600, data_loss: 0.9600\n",
            "step 580 , total_loss: 0.9848, data_loss: 0.9848\n",
            "step 600 , total_loss: 0.9856, data_loss: 0.9856\n",
            "step 620 , total_loss: 0.9950, data_loss: 0.9950\n",
            "step 640 , total_loss: 0.9664, data_loss: 0.9664\n",
            "step 660 , total_loss: 1.0236, data_loss: 1.0236\n",
            "step 680 , total_loss: 0.9571, data_loss: 0.9571\n",
            "step 700 , total_loss: 1.0173, data_loss: 1.0173\n",
            "step 720 , total_loss: 0.9641, data_loss: 0.9641\n",
            "step 740 , total_loss: 0.9503, data_loss: 0.9503\n",
            "step 760 , total_loss: 0.9585, data_loss: 0.9585\n",
            "step 780 , total_loss: 1.0393, data_loss: 1.0393\n",
            "step 800 , total_loss: 1.0432, data_loss: 1.0432\n",
            "eval valid at epoch 5: auc:0.7639,logloss:0.4933,mean_mrr:0.7283,ndcg@2:0.6984,ndcg@4:0.7904,ndcg@6:0.7971,group_auc:0.8094\n",
            "step 20 , total_loss: 0.9814, data_loss: 0.9814\n",
            "step 40 , total_loss: 1.0432, data_loss: 1.0432\n",
            "step 60 , total_loss: 0.9637, data_loss: 0.9637\n",
            "step 80 , total_loss: 0.9811, data_loss: 0.9811\n",
            "step 100 , total_loss: 0.9361, data_loss: 0.9361\n",
            "step 120 , total_loss: 0.9724, data_loss: 0.9724\n",
            "step 140 , total_loss: 0.9221, data_loss: 0.9221\n",
            "step 160 , total_loss: 0.9982, data_loss: 0.9982\n",
            "step 180 , total_loss: 0.9684, data_loss: 0.9684\n",
            "step 200 , total_loss: 0.9577, data_loss: 0.9577\n",
            "step 220 , total_loss: 0.8586, data_loss: 0.8586\n",
            "step 240 , total_loss: 0.9385, data_loss: 0.9385\n",
            "step 260 , total_loss: 0.9990, data_loss: 0.9990\n",
            "step 280 , total_loss: 1.0360, data_loss: 1.0360\n",
            "step 300 , total_loss: 0.9441, data_loss: 0.9441\n",
            "step 320 , total_loss: 0.9881, data_loss: 0.9881\n",
            "step 340 , total_loss: 1.0268, data_loss: 1.0268\n",
            "step 360 , total_loss: 0.9596, data_loss: 0.9596\n",
            "step 380 , total_loss: 1.0147, data_loss: 1.0147\n",
            "step 400 , total_loss: 0.8890, data_loss: 0.8890\n",
            "step 420 , total_loss: 0.9565, data_loss: 0.9565\n",
            "step 440 , total_loss: 0.9708, data_loss: 0.9708\n",
            "step 460 , total_loss: 0.9118, data_loss: 0.9118\n",
            "step 480 , total_loss: 0.9121, data_loss: 0.9121\n",
            "step 500 , total_loss: 0.9623, data_loss: 0.9623\n",
            "step 520 , total_loss: 0.9272, data_loss: 0.9272\n",
            "step 540 , total_loss: 0.9293, data_loss: 0.9293\n",
            "step 560 , total_loss: 0.8972, data_loss: 0.8972\n",
            "step 580 , total_loss: 0.9819, data_loss: 0.9819\n",
            "step 600 , total_loss: 0.9800, data_loss: 0.9800\n",
            "step 620 , total_loss: 0.9397, data_loss: 0.9397\n",
            "step 640 , total_loss: 0.9496, data_loss: 0.9496\n",
            "step 660 , total_loss: 0.9067, data_loss: 0.9067\n",
            "step 680 , total_loss: 0.9762, data_loss: 0.9762\n",
            "step 700 , total_loss: 0.9108, data_loss: 0.9108\n",
            "step 720 , total_loss: 1.0028, data_loss: 1.0028\n",
            "step 740 , total_loss: 0.9765, data_loss: 0.9765\n",
            "step 760 , total_loss: 0.9918, data_loss: 0.9918\n",
            "step 780 , total_loss: 1.0224, data_loss: 1.0224\n",
            "step 800 , total_loss: 0.9238, data_loss: 0.9238\n",
            "eval valid at epoch 6: auc:0.8125,logloss:0.5267,mean_mrr:0.7482,ndcg@2:0.7253,ndcg@4:0.8069,ndcg@6:0.8122,group_auc:0.8279\n",
            "step 20 , total_loss: 0.9472, data_loss: 0.9472\n",
            "step 40 , total_loss: 0.9145, data_loss: 0.9145\n",
            "step 60 , total_loss: 0.9160, data_loss: 0.9160\n",
            "step 80 , total_loss: 0.9246, data_loss: 0.9246\n",
            "step 100 , total_loss: 0.9689, data_loss: 0.9689\n",
            "step 120 , total_loss: 0.9457, data_loss: 0.9457\n",
            "step 140 , total_loss: 0.9678, data_loss: 0.9678\n",
            "step 160 , total_loss: 0.9510, data_loss: 0.9510\n",
            "step 180 , total_loss: 0.9878, data_loss: 0.9878\n",
            "step 200 , total_loss: 1.0075, data_loss: 1.0075\n",
            "step 220 , total_loss: 0.9304, data_loss: 0.9304\n",
            "step 240 , total_loss: 0.9125, data_loss: 0.9125\n",
            "step 260 , total_loss: 0.9029, data_loss: 0.9029\n",
            "step 280 , total_loss: 0.9472, data_loss: 0.9472\n",
            "step 300 , total_loss: 0.9503, data_loss: 0.9503\n",
            "step 320 , total_loss: 0.9253, data_loss: 0.9253\n",
            "step 340 , total_loss: 0.9853, data_loss: 0.9853\n",
            "step 360 , total_loss: 0.9403, data_loss: 0.9403\n",
            "step 380 , total_loss: 1.0058, data_loss: 1.0058\n",
            "step 400 , total_loss: 0.9296, data_loss: 0.9296\n",
            "step 420 , total_loss: 0.9132, data_loss: 0.9132\n",
            "step 440 , total_loss: 1.0063, data_loss: 1.0063\n",
            "step 460 , total_loss: 0.9685, data_loss: 0.9685\n",
            "step 480 , total_loss: 0.8598, data_loss: 0.8598\n",
            "step 500 , total_loss: 0.9153, data_loss: 0.9153\n",
            "step 520 , total_loss: 0.9219, data_loss: 0.9219\n",
            "step 540 , total_loss: 0.9649, data_loss: 0.9649\n",
            "step 560 , total_loss: 0.8936, data_loss: 0.8936\n",
            "step 580 , total_loss: 0.9006, data_loss: 0.9006\n",
            "step 600 , total_loss: 0.9197, data_loss: 0.9197\n",
            "step 620 , total_loss: 0.9290, data_loss: 0.9290\n",
            "step 640 , total_loss: 0.9310, data_loss: 0.9310\n",
            "step 660 , total_loss: 0.9596, data_loss: 0.9596\n",
            "step 680 , total_loss: 1.0045, data_loss: 1.0045\n",
            "step 700 , total_loss: 0.9749, data_loss: 0.9749\n",
            "step 720 , total_loss: 0.9994, data_loss: 0.9994\n",
            "step 740 , total_loss: 1.0317, data_loss: 1.0317\n",
            "step 760 , total_loss: 0.9599, data_loss: 0.9599\n",
            "step 780 , total_loss: 0.9885, data_loss: 0.9885\n",
            "step 800 , total_loss: 0.9556, data_loss: 0.9556\n",
            "eval valid at epoch 7: auc:0.7709,logloss:0.4906,mean_mrr:0.728,ndcg@2:0.7002,ndcg@4:0.7904,ndcg@6:0.7969,group_auc:0.8107\n",
            "step 20 , total_loss: 0.9940, data_loss: 0.9940\n",
            "step 40 , total_loss: 0.9219, data_loss: 0.9219\n",
            "step 60 , total_loss: 1.0475, data_loss: 1.0475\n",
            "step 80 , total_loss: 0.8922, data_loss: 0.8922\n",
            "step 100 , total_loss: 0.9515, data_loss: 0.9515\n",
            "step 120 , total_loss: 0.9660, data_loss: 0.9660\n",
            "step 140 , total_loss: 0.9364, data_loss: 0.9364\n",
            "step 160 , total_loss: 0.9644, data_loss: 0.9644\n",
            "step 180 , total_loss: 0.9085, data_loss: 0.9085\n",
            "step 200 , total_loss: 0.9893, data_loss: 0.9893\n",
            "step 220 , total_loss: 1.0255, data_loss: 1.0255\n",
            "step 240 , total_loss: 0.9045, data_loss: 0.9045\n",
            "step 260 , total_loss: 0.9359, data_loss: 0.9359\n",
            "step 280 , total_loss: 0.8842, data_loss: 0.8842\n",
            "step 300 , total_loss: 0.9601, data_loss: 0.9601\n",
            "step 320 , total_loss: 0.9166, data_loss: 0.9166\n",
            "step 340 , total_loss: 0.9998, data_loss: 0.9998\n",
            "step 360 , total_loss: 0.9016, data_loss: 0.9016\n",
            "step 380 , total_loss: 0.9736, data_loss: 0.9736\n",
            "step 400 , total_loss: 0.9727, data_loss: 0.9727\n",
            "step 420 , total_loss: 0.8993, data_loss: 0.8993\n",
            "step 440 , total_loss: 1.0404, data_loss: 1.0404\n",
            "step 460 , total_loss: 0.9599, data_loss: 0.9599\n",
            "step 480 , total_loss: 0.8704, data_loss: 0.8704\n",
            "step 500 , total_loss: 0.8771, data_loss: 0.8771\n",
            "step 520 , total_loss: 0.9543, data_loss: 0.9543\n",
            "step 540 , total_loss: 0.9603, data_loss: 0.9603\n",
            "step 560 , total_loss: 0.9576, data_loss: 0.9576\n",
            "step 580 , total_loss: 0.9035, data_loss: 0.9035\n",
            "step 600 , total_loss: 0.9725, data_loss: 0.9725\n",
            "step 620 , total_loss: 0.9319, data_loss: 0.9319\n",
            "step 640 , total_loss: 0.9123, data_loss: 0.9123\n",
            "step 660 , total_loss: 0.9703, data_loss: 0.9703\n",
            "step 680 , total_loss: 0.9213, data_loss: 0.9213\n",
            "step 700 , total_loss: 0.9937, data_loss: 0.9937\n",
            "step 720 , total_loss: 0.9772, data_loss: 0.9772\n",
            "step 740 , total_loss: 0.9820, data_loss: 0.9820\n",
            "step 760 , total_loss: 0.8808, data_loss: 0.8808\n",
            "step 780 , total_loss: 0.8914, data_loss: 0.8914\n",
            "step 800 , total_loss: 0.9741, data_loss: 0.9741\n",
            "eval valid at epoch 8: auc:0.8207,logloss:0.6566,mean_mrr:0.7502,ndcg@2:0.7292,ndcg@4:0.8087,ndcg@6:0.8136,group_auc:0.8288\n",
            "step 20 , total_loss: 0.9123, data_loss: 0.9123\n",
            "step 40 , total_loss: 0.9661, data_loss: 0.9661\n",
            "step 60 , total_loss: 0.8563, data_loss: 0.8563\n",
            "step 80 , total_loss: 0.9406, data_loss: 0.9406\n",
            "step 100 , total_loss: 0.9444, data_loss: 0.9444\n",
            "step 120 , total_loss: 0.9621, data_loss: 0.9621\n",
            "step 140 , total_loss: 0.9015, data_loss: 0.9015\n",
            "step 160 , total_loss: 0.8836, data_loss: 0.8836\n",
            "step 180 , total_loss: 0.9366, data_loss: 0.9366\n",
            "step 200 , total_loss: 0.9808, data_loss: 0.9808\n",
            "step 220 , total_loss: 0.8829, data_loss: 0.8829\n",
            "step 240 , total_loss: 0.9129, data_loss: 0.9129\n",
            "step 260 , total_loss: 0.9591, data_loss: 0.9591\n",
            "step 280 , total_loss: 0.9691, data_loss: 0.9691\n",
            "step 300 , total_loss: 0.9446, data_loss: 0.9446\n",
            "step 320 , total_loss: 0.8779, data_loss: 0.8779\n",
            "step 340 , total_loss: 0.8905, data_loss: 0.8905\n",
            "step 360 , total_loss: 0.9141, data_loss: 0.9141\n",
            "step 380 , total_loss: 0.9574, data_loss: 0.9574\n",
            "step 400 , total_loss: 1.0406, data_loss: 1.0406\n",
            "step 420 , total_loss: 0.8781, data_loss: 0.8781\n",
            "step 440 , total_loss: 0.9287, data_loss: 0.9287\n",
            "step 460 , total_loss: 0.9252, data_loss: 0.9252\n",
            "step 480 , total_loss: 0.9144, data_loss: 0.9144\n",
            "step 500 , total_loss: 0.9143, data_loss: 0.9143\n",
            "step 520 , total_loss: 0.8922, data_loss: 0.8922\n",
            "step 540 , total_loss: 0.9349, data_loss: 0.9349\n",
            "step 560 , total_loss: 0.9078, data_loss: 0.9078\n",
            "step 580 , total_loss: 0.9592, data_loss: 0.9592\n",
            "step 600 , total_loss: 0.9243, data_loss: 0.9243\n",
            "step 620 , total_loss: 0.9829, data_loss: 0.9829\n",
            "step 640 , total_loss: 0.9479, data_loss: 0.9479\n",
            "step 660 , total_loss: 0.9822, data_loss: 0.9822\n",
            "step 680 , total_loss: 0.9127, data_loss: 0.9127\n",
            "step 700 , total_loss: 0.8807, data_loss: 0.8807\n",
            "step 720 , total_loss: 0.9511, data_loss: 0.9511\n",
            "step 740 , total_loss: 0.9187, data_loss: 0.9187\n",
            "step 760 , total_loss: 1.0028, data_loss: 1.0028\n",
            "step 780 , total_loss: 0.8768, data_loss: 0.8768\n",
            "step 800 , total_loss: 0.9366, data_loss: 0.9366\n",
            "eval valid at epoch 9: auc:0.7773,logloss:0.5333,mean_mrr:0.7275,ndcg@2:0.6977,ndcg@4:0.7901,ndcg@6:0.7964,group_auc:0.809\n",
            "step 20 , total_loss: 0.9130, data_loss: 0.9130\n",
            "step 40 , total_loss: 0.9895, data_loss: 0.9895\n",
            "step 60 , total_loss: 0.9839, data_loss: 0.9839\n",
            "step 80 , total_loss: 0.9217, data_loss: 0.9217\n",
            "step 100 , total_loss: 0.9118, data_loss: 0.9118\n",
            "step 120 , total_loss: 0.9970, data_loss: 0.9970\n",
            "step 140 , total_loss: 0.9305, data_loss: 0.9305\n",
            "step 160 , total_loss: 0.9200, data_loss: 0.9200\n",
            "step 180 , total_loss: 0.8958, data_loss: 0.8958\n",
            "step 200 , total_loss: 0.9206, data_loss: 0.9206\n",
            "step 220 , total_loss: 0.9549, data_loss: 0.9549\n",
            "step 240 , total_loss: 0.9742, data_loss: 0.9742\n",
            "step 260 , total_loss: 0.9609, data_loss: 0.9609\n",
            "step 280 , total_loss: 0.9140, data_loss: 0.9140\n",
            "step 300 , total_loss: 0.9296, data_loss: 0.9296\n",
            "step 320 , total_loss: 0.8919, data_loss: 0.8919\n",
            "step 340 , total_loss: 0.9700, data_loss: 0.9700\n",
            "step 360 , total_loss: 0.9071, data_loss: 0.9071\n",
            "step 380 , total_loss: 0.9373, data_loss: 0.9373\n",
            "step 400 , total_loss: 1.0386, data_loss: 1.0386\n",
            "step 420 , total_loss: 0.9313, data_loss: 0.9313\n",
            "step 440 , total_loss: 0.9505, data_loss: 0.9505\n",
            "step 460 , total_loss: 0.9854, data_loss: 0.9854\n",
            "step 480 , total_loss: 0.9242, data_loss: 0.9242\n",
            "step 500 , total_loss: 0.9231, data_loss: 0.9231\n",
            "step 520 , total_loss: 0.8024, data_loss: 0.8024\n",
            "step 540 , total_loss: 0.9412, data_loss: 0.9412\n",
            "step 560 , total_loss: 0.9272, data_loss: 0.9272\n",
            "step 580 , total_loss: 0.9198, data_loss: 0.9198\n",
            "step 600 , total_loss: 0.9012, data_loss: 0.9012\n",
            "step 620 , total_loss: 0.9193, data_loss: 0.9193\n",
            "step 640 , total_loss: 0.9211, data_loss: 0.9211\n",
            "step 660 , total_loss: 0.9551, data_loss: 0.9551\n",
            "step 680 , total_loss: 0.8543, data_loss: 0.8543\n",
            "step 700 , total_loss: 0.8861, data_loss: 0.8861\n",
            "step 720 , total_loss: 0.8936, data_loss: 0.8936\n",
            "step 740 , total_loss: 0.8547, data_loss: 0.8547\n",
            "step 760 , total_loss: 0.8815, data_loss: 0.8815\n",
            "step 780 , total_loss: 0.9394, data_loss: 0.9394\n",
            "step 800 , total_loss: 0.9049, data_loss: 0.9049\n",
            "eval valid at epoch 10: auc:0.8207,logloss:0.537,mean_mrr:0.7533,ndcg@2:0.7301,ndcg@4:0.8114,ndcg@6:0.8159,group_auc:0.8313\n",
            "[(1, {'auc': 0.7176, 'logloss': 0.5924, 'mean_mrr': 0.6809, 'ndcg@2': 0.6341, 'ndcg@4': 0.7431, 'ndcg@6': 0.7609, 'group_auc': 0.7566}), (2, {'auc': 0.7222, 'logloss': 0.4806, 'mean_mrr': 0.6837, 'ndcg@2': 0.6378, 'ndcg@4': 0.7486, 'ndcg@6': 0.7631, 'group_auc': 0.7628}), (3, {'auc': 0.7767, 'logloss': 0.5106, 'mean_mrr': 0.7187, 'ndcg@2': 0.6856, 'ndcg@4': 0.7816, 'ndcg@6': 0.7898, 'group_auc': 0.8003}), (4, {'auc': 0.7932, 'logloss': 0.6692, 'mean_mrr': 0.7225, 'ndcg@2': 0.6909, 'ndcg@4': 0.7839, 'ndcg@6': 0.7926, 'group_auc': 0.8013}), (5, {'auc': 0.7639, 'logloss': 0.4933, 'mean_mrr': 0.7283, 'ndcg@2': 0.6984, 'ndcg@4': 0.7904, 'ndcg@6': 0.7971, 'group_auc': 0.8094}), (6, {'auc': 0.8125, 'logloss': 0.5267, 'mean_mrr': 0.7482, 'ndcg@2': 0.7253, 'ndcg@4': 0.8069, 'ndcg@6': 0.8122, 'group_auc': 0.8279}), (7, {'auc': 0.7709, 'logloss': 0.4906, 'mean_mrr': 0.728, 'ndcg@2': 0.7002, 'ndcg@4': 0.7904, 'ndcg@6': 0.7969, 'group_auc': 0.8107}), (8, {'auc': 0.8207, 'logloss': 0.6566, 'mean_mrr': 0.7502, 'ndcg@2': 0.7292, 'ndcg@4': 0.8087, 'ndcg@6': 0.8136, 'group_auc': 0.8288}), (9, {'auc': 0.7773, 'logloss': 0.5333, 'mean_mrr': 0.7275, 'ndcg@2': 0.6977, 'ndcg@4': 0.7901, 'ndcg@6': 0.7964, 'group_auc': 0.809}), (10, {'auc': 0.8207, 'logloss': 0.537, 'mean_mrr': 0.7533, 'ndcg@2': 0.7301, 'ndcg@4': 0.8114, 'ndcg@6': 0.8159, 'group_auc': 0.8313})]\n",
            "best epoch: 10\n",
            "Time cost for training is 142.56 mins\n"
          ]
        }
      ],
      "source": [
        "with Timer() as train_time:\n",
        "    model = model.fit(train_file, valid_file, valid_num_ngs=valid_num_ngs) \n",
        "\n",
        "# valid_num_ngs is the number of negative lines after each positive line in your valid_file \n",
        "# we will evaluate the performance of model on valid_file every epoch\n",
        "print('Time cost for training is {0:.2f} mins'.format(train_time.interval/60.0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gmm3DPjbiY9N",
        "outputId": "6dbb1a1d-8b36-49b1-ef5e-f4a807dddfde"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'auc': 0.8162, 'logloss': 0.5477, 'mean_mrr': 0.5981, 'ndcg@2': 0.5278, 'ndcg@4': 0.6422, 'ndcg@6': 0.684, 'group_auc': 0.8286}\n"
          ]
        }
      ],
      "source": [
        "res_syn = model.run_eval(test_file, num_ngs=test_num_ngs)\n",
        "print(res_syn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "VB7MBFNkjl5e",
        "outputId": "12b143c9-1d98-4d99-9136-8c779de2d832"
      },
      "outputs": [
        {
          "data": {
            "application/scrapbook.scrap.json+json": {
              "data": {
                "auc": 0.8162,
                "group_auc": 0.8286,
                "logloss": 0.5477,
                "mean_mrr": 0.5981,
                "ndcg@2": 0.5278,
                "ndcg@4": 0.6422,
                "ndcg@6": 0.684
              },
              "encoder": "json",
              "name": "res_syn",
              "version": 1
            }
          },
          "metadata": {
            "scrapbook": {
              "data": true,
              "display": false,
              "name": "res_syn"
            }
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "sb.glue(\"res_syn\", res_syn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "h2uxYGfFjpL9"
      },
      "outputs": [],
      "source": [
        "model = model.predict(test_file, output_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wGZ26pt-ujeo",
        "outputId": "5cbe4b07-34e3-4891-c64b-40b2a70a6fa7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\조예은\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\recommenders\\models\\deeprec\\models\\sequential\\caser.py:102: UserWarning: `tf.layers.conv1d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv1D` instead.\n",
            "  return tf.compat.v1.layers.conv1d(\n",
            "C:\\Users\\조예은\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\legacy_tf_layers\\convolutional.py:294: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
            "  return layer.apply(inputs)\n",
            "C:\\Users\\조예은\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\recommenders\\models\\deeprec\\models\\sequential\\caser.py:66: UserWarning: `tf.layers.flatten` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Flatten` instead.\n",
            "  out_v = tf.compat.v1.layers.flatten(out_v)\n",
            "C:\\Users\\조예은\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\legacy_tf_layers\\core.py:541: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
            "  return layer.apply(inputs)\n",
            "C:\\Users\\조예은\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\recommenders\\models\\deeprec\\models\\base_model.py:701: UserWarning: `tf.layers.batch_normalization` is deprecated and will be removed in a future version. Please use `tf.keras.layers.BatchNormalization` instead. In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.BatchNormalization` documentation).\n",
            "  curr_hidden_nn_layer = tf.compat.v1.layers.batch_normalization(\n",
            "C:\\Users\\조예은\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\legacy_tf_layers\\normalization.py:463: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
            "  return layer.apply(inputs, training=training)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loading saved model in resources/model\\caser/best_model\n"
          ]
        }
      ],
      "source": [
        "model_best_trained = SeqModel(hparams, input_creator, seed=RANDOM_SEED)\n",
        "path_best_trained = os.path.join(hparams.MODEL_DIR, \"best_model\")\n",
        "print('loading saved model in {0}'.format(path_best_trained))\n",
        "model_best_trained.load_model(path_best_trained)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fKtBHxlGu5-K",
        "outputId": "d398f3b9-bbba-4707-fae1-7c1c817a7e66"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'auc': 0.8162,\n",
              " 'logloss': 0.5477,\n",
              " 'mean_mrr': 0.5981,\n",
              " 'ndcg@2': 0.5278,\n",
              " 'ndcg@4': 0.6422,\n",
              " 'ndcg@6': 0.684,\n",
              " 'group_auc': 0.8286}"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_best_trained.run_eval(test_file, num_ngs=test_num_ngs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XH7L5VOQvARx",
        "outputId": "46721757-0218-4a41-8557-fcd7806b0852"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<recommenders.models.deeprec.models.sequential.caser.CaserModel at 0x2183e0043a0>"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_best_trained.predict(test_file, output_file)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "GRU4REC.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
