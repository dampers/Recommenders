{"cells":[{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8296,"status":"ok","timestamp":1649980980504,"user":{"displayName":"조예은","userId":"06018350812551678054"},"user_tz":-540},"id":"Sknnn2xs-AHi","outputId":"80551ecd-5a8e-4fc5-fab5-74699c879c18"},"outputs":[{"name":"stdout","output_type":"stream","text":["System version: 3.7.13 (default, Mar 16 2022, 17:37:17) \n","[GCC 7.5.0]\n","Tensorflow version: 2.8.0\n"]}],"source":["import sys\n","import os\n","import logging\n","import papermill as pm\n","import scrapbook as sb\n","from tempfile import TemporaryDirectory\n","import numpy as np\n","import tensorflow.compat.v1 as tf\n","tf.get_logger().setLevel('ERROR') # only show error messages\n","\n","from recommenders.utils.timer import Timer\n","from recommenders.utils.constants import SEED\n","from recommenders.models.deeprec.deeprec_utils import (\n","    prepare_hparams\n",")\n","\n","from resources.data_preprocessing import data_preprocessing\n","# from recommenders.datasets.amazon_reviews import download_and_extract, data_preprocessing\n","from recommenders.datasets.download_utils import maybe_download\n","\n","\n","from recommenders.models.deeprec.models.sequential.sli_rec import SLI_RECModel as SeqModel\n","####  to use the other model, use one of the following lines:\n","#from recommenders.models.deeprec.models.sequential.asvd import A2SVDModel as SeqModel\n","# from recommenders.models.deeprec.models.sequential.caser import CaserModel as SeqModel\n","# from recommenders.models.deeprec.models.sequential.gru4rec import GRU4RecModel as SeqModel\n","# from recommenders.models.deeprec.models.sequential.sum import SUMModel as SeqModel\n","\n","#from recommenders.models.deeprec.models.sequential.nextitnet import NextItNetModel\n","\n","from recommenders.models.deeprec.io.sequential_iterator import SequentialIterator\n","#from recommenders.models.deeprec.io.nextitnet_iterator import NextItNetIterator\n","\n","print(\"System version: {}\".format(sys.version))\n","print(\"Tensorflow version: {}\".format(tf.__version__))"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":424,"status":"ok","timestamp":1649980983686,"user":{"displayName":"조예은","userId":"06018350812551678054"},"user_tz":-540},"id":"sxtfUZfP-ZWR"},"outputs":[],"source":["##  ATTENTION: change to the corresponding config file, e.g., caser.yaml for CaserModel, sum.yaml for SUMModel\n","# yaml_file = '../../recommenders/models/deeprec/config/sli_rec.yaml'  \n","yaml_file = './sli_rec.yaml'  "]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":415,"status":"ok","timestamp":1649981001550,"user":{"displayName":"조예은","userId":"06018350812551678054"},"user_tz":-540},"id":"_NnzhN4h_rI5"},"outputs":[],"source":["EPOCHS = 10\n","BATCH_SIZE = 400\n","RANDOM_SEED = SEED  # Set None for non-deterministic result\n","\n","data_path = os.path.join(\"resources/\")"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":419,"status":"ok","timestamp":1649981009184,"user":{"displayName":"조예은","userId":"06018350812551678054"},"user_tz":-540},"id":"0RAMNcI3AClc"},"outputs":[],"source":["# for test\n","train_file = os.path.join(data_path, r'train_data')\n","valid_file = os.path.join(data_path, r'valid_data')\n","test_file = os.path.join(data_path, r'test_data')\n","user_vocab = os.path.join(data_path, r'user_vocab.pkl')\n","item_vocab = os.path.join(data_path, r'item_vocab.pkl')\n","cate_vocab = os.path.join(data_path, r'cate_vocab.pkl')\n","output_file = os.path.join(data_path, r'output_sli_rec.txt')\n","\n","# reviews_name = 'json'\n","# meta_name = 'json'\n","# reviews_file = os.path.join(data_path, reviews_name)\n","# meta_file = os.path.join(data_path, meta_name)\n","train_num_ngs = 4 # number of negative instances with a positive instance for training\n","valid_num_ngs = 4 # number of negative instances with a positive instance for validation\n","test_num_ngs = 9 # number of negative instances with a positive instance for testing\n","sample_rate = 0.01 # sample a small item set for training and testing here for fast example\n","\n","input_files = [data_path, train_file, valid_file, test_file, user_vocab, item_vocab, cate_vocab]\n","\n","if not os.path.exists(train_file):\n","    # download_and_extract(reviews_name, reviews_file)\n","    # download_and_extract(meta_name, meta_file)\n","    data_preprocessing(*input_files, sample_rate=sample_rate, valid_num_ngs=valid_num_ngs, test_num_ngs=test_num_ngs)\n","    #### uncomment this for the NextItNet model, because it does not need to unfold the user history\n","    # data_preprocessing(*input_files, sample_rate=sample_rate, valid_num_ngs=valid_num_ngs, test_num_ngs=test_num_ngs, is_history_expanding=False)\n"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":1013,"status":"ok","timestamp":1649981039553,"user":{"displayName":"조예은","userId":"06018350812551678054"},"user_tz":-540},"id":"rwTMO3WEWaCu"},"outputs":[],"source":["### NOTE:  \n","### remember to use `_create_vocab(train_file, user_vocab, item_vocab, cate_vocab)` to generate the user_vocab, item_vocab and cate_vocab files, if you are using your own dataset rather than using our demo Amazon dataset.\n","hparams = prepare_hparams(yaml_file, \n","                          embed_l2=0., \n","                          layer_l2=0., \n","                          learning_rate=0.001,  # set to 0.01 if batch normalization is disable\n","                          epochs=EPOCHS,\n","                          batch_size=BATCH_SIZE,\n","                          show_step=20,\n","                          MODEL_DIR=os.path.join(data_path, \"model\", \"sli_rec/\"),\n","                          SUMMARIES_DIR=os.path.join(data_path, \"summary\", \"sli_rec/\"),\n","                          user_vocab=user_vocab,\n","                          item_vocab=item_vocab,\n","                          cate_vocab=cate_vocab,\n","                          need_sample=True,\n","                          train_num_ngs=train_num_ngs, # provides the number of negative instances for each positive instance for loss computation.\n","            )"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":420,"status":"ok","timestamp":1649981043565,"user":{"displayName":"조예은","userId":"06018350812551678054"},"user_tz":-540},"id":"4Rvq7gFluVPq"},"outputs":[],"source":["input_creator = SequentialIterator\n","#### uncomment this for the NextItNet model, because it needs a special data iterator for training\n","#input_creator = NextItNetIterator"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11139,"status":"ok","timestamp":1649981057379,"user":{"displayName":"조예은","userId":"06018350812551678054"},"user_tz":-540},"id":"VFNfgcKNuaVv","outputId":"2f195cde-2963-41ff-aa84-0a17d9a9cd3d"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/recommenders/models/deeprec/models/base_model.py:705: UserWarning: `tf.layers.batch_normalization` is deprecated and will be removed in a future version. Please use `tf.keras.layers.BatchNormalization` instead. In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.BatchNormalization` documentation).\n","  training=self.is_train_stage,\n","/usr/local/lib/python3.7/dist-packages/keras/legacy_tf_layers/normalization.py:463: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n","  return layer.apply(inputs, training=training)\n"]}],"source":["model = SeqModel(hparams, input_creator, seed=RANDOM_SEED)\n","\n","## sometimes we don't want to train a model from scratch\n","## then we can load a pre-trained model like this: \n","#model.load_model(r'your_model_path')"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":63766,"status":"ok","timestamp":1649981126167,"user":{"displayName":"조예은","userId":"06018350812551678054"},"user_tz":-540},"id":"8AQxPlk-uheU","outputId":"23ee2335-3935-437b-c4f2-0c01fee93d8f"},"outputs":[{"name":"stdout","output_type":"stream","text":["{'auc': 0.5339, 'logloss': 0.6931, 'mean_mrr': 0.2909, 'ndcg@2': 0.158, 'ndcg@4': 0.2545, 'ndcg@6': 0.3326, 'group_auc': 0.5339}\n"]}],"source":["# test_num_ngs is the number of negative lines after each positive line in your test_file\n","print(model.run_eval(test_file, num_ngs=test_num_ngs)) "]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19408573,"status":"ok","timestamp":1650000544829,"user":{"displayName":"조예은","userId":"06018350812551678054"},"user_tz":-540},"id":"QFKQu9rGu0jq","outputId":"68371df4-d8f1-4844-8f13-d634505f1b33"},"outputs":[{"name":"stdout","output_type":"stream","text":["step 20 , total_loss: 1.5454, data_loss: 1.5454\n","step 40 , total_loss: 1.4382, data_loss: 1.4382\n","step 60 , total_loss: 1.4560, data_loss: 1.4560\n","step 80 , total_loss: 1.3815, data_loss: 1.3815\n","step 100 , total_loss: 1.3516, data_loss: 1.3516\n","step 120 , total_loss: 1.2937, data_loss: 1.2937\n","step 140 , total_loss: 1.3227, data_loss: 1.3227\n","step 160 , total_loss: 1.3556, data_loss: 1.3556\n","step 180 , total_loss: 1.2994, data_loss: 1.2994\n","step 200 , total_loss: 1.3157, data_loss: 1.3157\n","step 220 , total_loss: 1.2972, data_loss: 1.2972\n","step 240 , total_loss: 1.3546, data_loss: 1.3546\n","step 260 , total_loss: 1.2766, data_loss: 1.2766\n","step 280 , total_loss: 1.3071, data_loss: 1.3071\n","step 300 , total_loss: 1.3313, data_loss: 1.3313\n","step 320 , total_loss: 1.2846, data_loss: 1.2846\n","step 340 , total_loss: 1.3081, data_loss: 1.3081\n","step 360 , total_loss: 1.3228, data_loss: 1.3228\n","step 380 , total_loss: 1.2918, data_loss: 1.2918\n","step 400 , total_loss: 1.2388, data_loss: 1.2388\n","step 420 , total_loss: 1.2943, data_loss: 1.2943\n","step 440 , total_loss: 1.1945, data_loss: 1.1945\n","step 460 , total_loss: 1.1669, data_loss: 1.1669\n","step 480 , total_loss: 1.2402, data_loss: 1.2402\n","step 500 , total_loss: 1.2363, data_loss: 1.2363\n","step 520 , total_loss: 1.1603, data_loss: 1.1603\n","step 540 , total_loss: 1.1862, data_loss: 1.1862\n","step 560 , total_loss: 1.2674, data_loss: 1.2674\n","step 580 , total_loss: 1.1923, data_loss: 1.1923\n","step 600 , total_loss: 1.1846, data_loss: 1.1846\n","step 620 , total_loss: 1.2760, data_loss: 1.2760\n","step 640 , total_loss: 1.2411, data_loss: 1.2411\n","step 660 , total_loss: 1.2000, data_loss: 1.2000\n","step 680 , total_loss: 1.1767, data_loss: 1.1767\n","step 700 , total_loss: 1.2268, data_loss: 1.2268\n","step 720 , total_loss: 1.2291, data_loss: 1.2291\n","step 740 , total_loss: 1.1899, data_loss: 1.1899\n","step 760 , total_loss: 1.2074, data_loss: 1.2074\n","step 780 , total_loss: 1.2094, data_loss: 1.2094\n","step 800 , total_loss: 1.1736, data_loss: 1.1736\n","step 820 , total_loss: 1.1327, data_loss: 1.1327\n","step 840 , total_loss: 1.1302, data_loss: 1.1302\n","step 860 , total_loss: 1.1659, data_loss: 1.1659\n","step 880 , total_loss: 1.1624, data_loss: 1.1624\n","step 900 , total_loss: 1.1860, data_loss: 1.1860\n","step 920 , total_loss: 1.1135, data_loss: 1.1135\n","eval valid at epoch 1: auc:0.7666,logloss:0.6064,mean_mrr:0.7021,ndcg@2:0.662,ndcg@4:0.7619,ndcg@6:0.777,group_auc:0.7762\n","step 20 , total_loss: 1.1901, data_loss: 1.1901\n","step 40 , total_loss: 1.1406, data_loss: 1.1406\n","step 60 , total_loss: 1.1408, data_loss: 1.1408\n","step 80 , total_loss: 1.1310, data_loss: 1.1310\n","step 100 , total_loss: 1.1055, data_loss: 1.1055\n","step 120 , total_loss: 1.0653, data_loss: 1.0653\n","step 140 , total_loss: 1.1214, data_loss: 1.1214\n","step 160 , total_loss: 1.1653, data_loss: 1.1653\n","step 180 , total_loss: 1.0889, data_loss: 1.0889\n","step 200 , total_loss: 1.1615, data_loss: 1.1615\n","step 220 , total_loss: 1.0216, data_loss: 1.0216\n","step 240 , total_loss: 1.0880, data_loss: 1.0880\n","step 260 , total_loss: 1.0508, data_loss: 1.0508\n","step 280 , total_loss: 1.0573, data_loss: 1.0573\n","step 300 , total_loss: 1.1255, data_loss: 1.1255\n","step 320 , total_loss: 1.0184, data_loss: 1.0184\n","step 340 , total_loss: 1.0278, data_loss: 1.0278\n","step 360 , total_loss: 0.9998, data_loss: 0.9998\n","step 380 , total_loss: 0.9648, data_loss: 0.9648\n","step 400 , total_loss: 0.8879, data_loss: 0.8879\n","step 420 , total_loss: 0.9567, data_loss: 0.9567\n","step 440 , total_loss: 0.9648, data_loss: 0.9648\n","step 460 , total_loss: 0.9516, data_loss: 0.9516\n","step 480 , total_loss: 0.8304, data_loss: 0.8304\n","step 500 , total_loss: 0.9437, data_loss: 0.9437\n","step 520 , total_loss: 0.9040, data_loss: 0.9040\n","step 540 , total_loss: 0.8986, data_loss: 0.8986\n","step 560 , total_loss: 0.9073, data_loss: 0.9073\n","step 580 , total_loss: 0.9297, data_loss: 0.9297\n","step 600 , total_loss: 0.8899, data_loss: 0.8899\n","step 620 , total_loss: 0.9088, data_loss: 0.9088\n","step 640 , total_loss: 0.8311, data_loss: 0.8311\n","step 660 , total_loss: 0.8111, data_loss: 0.8111\n","step 680 , total_loss: 0.8694, data_loss: 0.8694\n","step 700 , total_loss: 0.8332, data_loss: 0.8332\n","step 720 , total_loss: 0.8801, data_loss: 0.8801\n","step 740 , total_loss: 0.8419, data_loss: 0.8419\n","step 760 , total_loss: 0.9370, data_loss: 0.9370\n","step 780 , total_loss: 0.8512, data_loss: 0.8512\n","step 800 , total_loss: 0.8575, data_loss: 0.8575\n","step 820 , total_loss: 0.8323, data_loss: 0.8323\n","step 840 , total_loss: 0.7907, data_loss: 0.7907\n","step 860 , total_loss: 0.8904, data_loss: 0.8904\n","step 880 , total_loss: 0.7993, data_loss: 0.7993\n","step 900 , total_loss: 0.8120, data_loss: 0.8120\n","step 920 , total_loss: 0.8968, data_loss: 0.8968\n","eval valid at epoch 2: auc:0.8846,logloss:0.4107,mean_mrr:0.8158,ndcg@2:0.8121,ndcg@4:0.8612,ndcg@6:0.8629,group_auc:0.8837\n","step 20 , total_loss: 0.8880, data_loss: 0.8880\n","step 40 , total_loss: 0.7985, data_loss: 0.7985\n","step 60 , total_loss: 0.7806, data_loss: 0.7806\n","step 80 , total_loss: 0.8258, data_loss: 0.8258\n","step 100 , total_loss: 0.8256, data_loss: 0.8256\n","step 120 , total_loss: 0.9050, data_loss: 0.9050\n","step 140 , total_loss: 0.8853, data_loss: 0.8853\n","step 160 , total_loss: 0.8099, data_loss: 0.8099\n","step 180 , total_loss: 0.8376, data_loss: 0.8376\n","step 200 , total_loss: 0.7577, data_loss: 0.7577\n","step 220 , total_loss: 0.8724, data_loss: 0.8724\n","step 240 , total_loss: 0.9494, data_loss: 0.9494\n","step 260 , total_loss: 0.8103, data_loss: 0.8103\n","step 280 , total_loss: 0.8019, data_loss: 0.8019\n","step 300 , total_loss: 0.8131, data_loss: 0.8131\n","step 320 , total_loss: 0.7888, data_loss: 0.7888\n","step 340 , total_loss: 0.8309, data_loss: 0.8309\n","step 360 , total_loss: 0.8778, data_loss: 0.8778\n","step 380 , total_loss: 0.7910, data_loss: 0.7910\n","step 400 , total_loss: 0.8691, data_loss: 0.8691\n","step 420 , total_loss: 0.8326, data_loss: 0.8326\n","step 440 , total_loss: 0.8431, data_loss: 0.8431\n","step 460 , total_loss: 0.8220, data_loss: 0.8220\n","step 480 , total_loss: 0.7471, data_loss: 0.7471\n","step 500 , total_loss: 0.7531, data_loss: 0.7531\n","step 520 , total_loss: 0.8159, data_loss: 0.8159\n","step 540 , total_loss: 0.7095, data_loss: 0.7095\n","step 560 , total_loss: 0.7980, data_loss: 0.7980\n","step 580 , total_loss: 0.7844, data_loss: 0.7844\n","step 600 , total_loss: 0.8389, data_loss: 0.8389\n","step 620 , total_loss: 0.8043, data_loss: 0.8043\n","step 640 , total_loss: 0.8498, data_loss: 0.8498\n","step 660 , total_loss: 0.7865, data_loss: 0.7865\n","step 680 , total_loss: 0.7799, data_loss: 0.7799\n","step 700 , total_loss: 0.8218, data_loss: 0.8218\n","step 720 , total_loss: 0.8580, data_loss: 0.8580\n","step 740 , total_loss: 0.8156, data_loss: 0.8156\n","step 760 , total_loss: 0.7281, data_loss: 0.7281\n","step 780 , total_loss: 0.8275, data_loss: 0.8275\n","step 800 , total_loss: 0.7714, data_loss: 0.7714\n","step 820 , total_loss: 0.7846, data_loss: 0.7846\n","step 840 , total_loss: 0.8011, data_loss: 0.8011\n","step 860 , total_loss: 0.8082, data_loss: 0.8082\n","step 880 , total_loss: 0.8117, data_loss: 0.8117\n","step 900 , total_loss: 0.7692, data_loss: 0.7692\n","step 920 , total_loss: 0.8170, data_loss: 0.8170\n","eval valid at epoch 3: auc:0.8923,logloss:0.383,mean_mrr:0.8253,ndcg@2:0.8236,ndcg@4:0.8685,ndcg@6:0.8701,group_auc:0.8904\n","step 20 , total_loss: 0.8504, data_loss: 0.8504\n","step 40 , total_loss: 0.8036, data_loss: 0.8036\n","step 60 , total_loss: 0.8254, data_loss: 0.8254\n","step 80 , total_loss: 0.8687, data_loss: 0.8687\n","step 100 , total_loss: 0.7896, data_loss: 0.7896\n","step 120 , total_loss: 0.8280, data_loss: 0.8280\n","step 140 , total_loss: 0.7564, data_loss: 0.7564\n","step 160 , total_loss: 0.8504, data_loss: 0.8504\n","step 180 , total_loss: 0.8877, data_loss: 0.8877\n","step 200 , total_loss: 0.8151, data_loss: 0.8151\n","step 220 , total_loss: 0.7981, data_loss: 0.7981\n","step 240 , total_loss: 0.8500, data_loss: 0.8500\n","step 260 , total_loss: 0.8137, data_loss: 0.8137\n","step 280 , total_loss: 0.7757, data_loss: 0.7757\n","step 300 , total_loss: 0.8198, data_loss: 0.8198\n","step 320 , total_loss: 0.8100, data_loss: 0.8100\n","step 340 , total_loss: 0.8205, data_loss: 0.8205\n","step 360 , total_loss: 0.7943, data_loss: 0.7943\n","step 380 , total_loss: 0.8058, data_loss: 0.8058\n","step 400 , total_loss: 0.8196, data_loss: 0.8196\n","step 420 , total_loss: 0.8149, data_loss: 0.8149\n","step 440 , total_loss: 0.7985, data_loss: 0.7985\n","step 460 , total_loss: 0.8072, data_loss: 0.8072\n","step 480 , total_loss: 0.8248, data_loss: 0.8248\n","step 500 , total_loss: 0.7673, data_loss: 0.7673\n","step 520 , total_loss: 0.7830, data_loss: 0.7830\n","step 540 , total_loss: 0.7678, data_loss: 0.7678\n","step 560 , total_loss: 0.8258, data_loss: 0.8258\n","step 580 , total_loss: 0.7932, data_loss: 0.7932\n","step 600 , total_loss: 0.7308, data_loss: 0.7308\n","step 620 , total_loss: 0.7459, data_loss: 0.7459\n","step 640 , total_loss: 0.8119, data_loss: 0.8119\n","step 660 , total_loss: 0.8078, data_loss: 0.8078\n","step 680 , total_loss: 0.8052, data_loss: 0.8052\n","step 700 , total_loss: 0.7326, data_loss: 0.7326\n","step 720 , total_loss: 0.7823, data_loss: 0.7823\n","step 740 , total_loss: 0.8668, data_loss: 0.8668\n","step 760 , total_loss: 0.7706, data_loss: 0.7706\n","step 780 , total_loss: 0.7215, data_loss: 0.7215\n","step 800 , total_loss: 0.8557, data_loss: 0.8557\n","step 820 , total_loss: 0.8000, data_loss: 0.8000\n","step 840 , total_loss: 0.8257, data_loss: 0.8257\n","step 860 , total_loss: 0.7159, data_loss: 0.7159\n","step 880 , total_loss: 0.7707, data_loss: 0.7707\n","step 900 , total_loss: 0.8279, data_loss: 0.8279\n","step 920 , total_loss: 0.8047, data_loss: 0.8047\n","eval valid at epoch 4: auc:0.8962,logloss:0.3797,mean_mrr:0.829,ndcg@2:0.827,ndcg@4:0.8712,ndcg@6:0.8728,group_auc:0.8927\n","step 20 , total_loss: 0.7944, data_loss: 0.7944\n","step 40 , total_loss: 0.7180, data_loss: 0.7180\n","step 60 , total_loss: 0.7453, data_loss: 0.7453\n","step 80 , total_loss: 0.7942, data_loss: 0.7942\n","step 100 , total_loss: 0.7799, data_loss: 0.7799\n","step 120 , total_loss: 0.8615, data_loss: 0.8615\n","step 140 , total_loss: 0.7491, data_loss: 0.7491\n","step 160 , total_loss: 0.7499, data_loss: 0.7499\n","step 180 , total_loss: 0.7814, data_loss: 0.7814\n","step 200 , total_loss: 0.7725, data_loss: 0.7725\n","step 220 , total_loss: 0.7788, data_loss: 0.7788\n","step 240 , total_loss: 0.8300, data_loss: 0.8300\n","step 260 , total_loss: 0.7622, data_loss: 0.7622\n","step 280 , total_loss: 0.7574, data_loss: 0.7574\n","step 300 , total_loss: 0.7547, data_loss: 0.7547\n","step 320 , total_loss: 0.8096, data_loss: 0.8096\n","step 340 , total_loss: 0.8286, data_loss: 0.8286\n","step 360 , total_loss: 0.7982, data_loss: 0.7982\n","step 380 , total_loss: 0.8310, data_loss: 0.8310\n","step 400 , total_loss: 0.7653, data_loss: 0.7653\n","step 420 , total_loss: 0.7165, data_loss: 0.7165\n","step 440 , total_loss: 0.7984, data_loss: 0.7984\n","step 460 , total_loss: 0.7532, data_loss: 0.7532\n","step 480 , total_loss: 0.8192, data_loss: 0.8192\n","step 500 , total_loss: 0.7855, data_loss: 0.7855\n","step 520 , total_loss: 0.6993, data_loss: 0.6993\n","step 540 , total_loss: 0.7852, data_loss: 0.7852\n","step 560 , total_loss: 0.6882, data_loss: 0.6882\n","step 580 , total_loss: 0.7694, data_loss: 0.7694\n","step 600 , total_loss: 0.7690, data_loss: 0.7690\n","step 620 , total_loss: 0.7634, data_loss: 0.7634\n","step 640 , total_loss: 0.7446, data_loss: 0.7446\n","step 660 , total_loss: 0.8503, data_loss: 0.8503\n","step 680 , total_loss: 0.7407, data_loss: 0.7407\n","step 700 , total_loss: 0.8319, data_loss: 0.8319\n","step 720 , total_loss: 0.7148, data_loss: 0.7148\n","step 740 , total_loss: 0.7555, data_loss: 0.7555\n","step 760 , total_loss: 0.7974, data_loss: 0.7974\n","step 780 , total_loss: 0.7462, data_loss: 0.7462\n","step 800 , total_loss: 0.7755, data_loss: 0.7755\n","step 820 , total_loss: 0.7765, data_loss: 0.7765\n","step 840 , total_loss: 0.8625, data_loss: 0.8625\n","step 860 , total_loss: 0.8074, data_loss: 0.8074\n","step 880 , total_loss: 0.8135, data_loss: 0.8135\n","step 900 , total_loss: 0.7432, data_loss: 0.7432\n","step 920 , total_loss: 0.7928, data_loss: 0.7928\n","eval valid at epoch 5: auc:0.8978,logloss:0.3767,mean_mrr:0.8342,ndcg@2:0.8328,ndcg@4:0.8753,ndcg@6:0.8767,group_auc:0.8961\n","step 20 , total_loss: 0.7875, data_loss: 0.7875\n","step 40 , total_loss: 0.7762, data_loss: 0.7762\n","step 60 , total_loss: 0.7937, data_loss: 0.7937\n","step 80 , total_loss: 0.8025, data_loss: 0.8025\n","step 100 , total_loss: 0.8066, data_loss: 0.8066\n","step 120 , total_loss: 0.7638, data_loss: 0.7638\n","step 140 , total_loss: 0.7573, data_loss: 0.7573\n","step 160 , total_loss: 0.8071, data_loss: 0.8071\n","step 180 , total_loss: 0.7580, data_loss: 0.7580\n","step 200 , total_loss: 0.7355, data_loss: 0.7355\n","step 220 , total_loss: 0.7550, data_loss: 0.7550\n","step 240 , total_loss: 0.7579, data_loss: 0.7579\n","step 260 , total_loss: 0.7702, data_loss: 0.7702\n","step 280 , total_loss: 0.7241, data_loss: 0.7241\n","step 300 , total_loss: 0.7313, data_loss: 0.7313\n","step 320 , total_loss: 0.7382, data_loss: 0.7382\n","step 340 , total_loss: 0.7694, data_loss: 0.7694\n","step 360 , total_loss: 0.7745, data_loss: 0.7745\n","step 380 , total_loss: 0.7933, data_loss: 0.7933\n","step 400 , total_loss: 0.7583, data_loss: 0.7583\n","step 420 , total_loss: 0.7442, data_loss: 0.7442\n","step 440 , total_loss: 0.7518, data_loss: 0.7518\n","step 460 , total_loss: 0.7438, data_loss: 0.7438\n","step 480 , total_loss: 0.8197, data_loss: 0.8197\n","step 500 , total_loss: 0.7349, data_loss: 0.7349\n","step 520 , total_loss: 0.7165, data_loss: 0.7165\n","step 540 , total_loss: 0.7446, data_loss: 0.7446\n","step 560 , total_loss: 0.7385, data_loss: 0.7385\n","step 580 , total_loss: 0.8137, data_loss: 0.8137\n","step 600 , total_loss: 0.7569, data_loss: 0.7569\n","step 620 , total_loss: 0.7023, data_loss: 0.7023\n","step 640 , total_loss: 0.7842, data_loss: 0.7842\n","step 660 , total_loss: 0.7957, data_loss: 0.7957\n","step 680 , total_loss: 0.8039, data_loss: 0.8039\n","step 700 , total_loss: 0.7147, data_loss: 0.7147\n","step 720 , total_loss: 0.7925, data_loss: 0.7925\n","step 740 , total_loss: 0.7713, data_loss: 0.7713\n","step 760 , total_loss: 0.7716, data_loss: 0.7716\n","step 780 , total_loss: 0.7283, data_loss: 0.7283\n","step 800 , total_loss: 0.8143, data_loss: 0.8143\n","step 820 , total_loss: 0.7399, data_loss: 0.7399\n","step 840 , total_loss: 0.7159, data_loss: 0.7159\n","step 860 , total_loss: 0.7666, data_loss: 0.7666\n","step 880 , total_loss: 0.7769, data_loss: 0.7769\n","step 900 , total_loss: 0.7639, data_loss: 0.7639\n","step 920 , total_loss: 0.8118, data_loss: 0.8118\n","eval valid at epoch 6: auc:0.8977,logloss:0.3789,mean_mrr:0.8342,ndcg@2:0.8331,ndcg@4:0.8751,ndcg@6:0.8767,group_auc:0.8962\n","step 20 , total_loss: 0.7986, data_loss: 0.7986\n","step 40 , total_loss: 0.7467, data_loss: 0.7467\n","step 60 , total_loss: 0.7710, data_loss: 0.7710\n","step 80 , total_loss: 0.7514, data_loss: 0.7514\n","step 100 , total_loss: 0.7976, data_loss: 0.7976\n","step 120 , total_loss: 0.8060, data_loss: 0.8060\n","step 140 , total_loss: 0.7756, data_loss: 0.7756\n","step 160 , total_loss: 0.7530, data_loss: 0.7530\n","step 180 , total_loss: 0.7282, data_loss: 0.7282\n","step 200 , total_loss: 0.7682, data_loss: 0.7682\n","step 220 , total_loss: 0.7419, data_loss: 0.7419\n","step 240 , total_loss: 0.7382, data_loss: 0.7382\n","step 260 , total_loss: 0.7510, data_loss: 0.7510\n","step 280 , total_loss: 0.7519, data_loss: 0.7519\n","step 300 , total_loss: 0.7161, data_loss: 0.7161\n","step 320 , total_loss: 0.7765, data_loss: 0.7765\n","step 340 , total_loss: 0.7244, data_loss: 0.7244\n","step 360 , total_loss: 0.7604, data_loss: 0.7604\n","step 380 , total_loss: 0.7417, data_loss: 0.7417\n","step 400 , total_loss: 0.7975, data_loss: 0.7975\n","step 420 , total_loss: 0.7147, data_loss: 0.7147\n","step 440 , total_loss: 0.7776, data_loss: 0.7776\n","step 460 , total_loss: 0.7744, data_loss: 0.7744\n","step 480 , total_loss: 0.7955, data_loss: 0.7955\n","step 500 , total_loss: 0.8021, data_loss: 0.8021\n","step 520 , total_loss: 0.7189, data_loss: 0.7189\n","step 540 , total_loss: 0.7369, data_loss: 0.7369\n","step 560 , total_loss: 0.7113, data_loss: 0.7113\n","step 580 , total_loss: 0.7208, data_loss: 0.7208\n","step 600 , total_loss: 0.7522, data_loss: 0.7522\n","step 620 , total_loss: 0.7906, data_loss: 0.7906\n","step 640 , total_loss: 0.7890, data_loss: 0.7890\n","step 660 , total_loss: 0.7793, data_loss: 0.7793\n","step 680 , total_loss: 0.7489, data_loss: 0.7489\n","step 700 , total_loss: 0.7354, data_loss: 0.7354\n","step 720 , total_loss: 0.7138, data_loss: 0.7138\n","step 740 , total_loss: 0.7682, data_loss: 0.7682\n","step 760 , total_loss: 0.7608, data_loss: 0.7608\n","step 780 , total_loss: 0.7578, data_loss: 0.7578\n","step 800 , total_loss: 0.7670, data_loss: 0.7670\n","step 820 , total_loss: 0.6660, data_loss: 0.6660\n","step 840 , total_loss: 0.7903, data_loss: 0.7903\n","step 860 , total_loss: 0.8279, data_loss: 0.8279\n","step 880 , total_loss: 0.7441, data_loss: 0.7441\n","step 900 , total_loss: 0.7793, data_loss: 0.7793\n","step 920 , total_loss: 0.7735, data_loss: 0.7735\n","eval valid at epoch 7: auc:0.8987,logloss:0.3789,mean_mrr:0.8349,ndcg@2:0.834,ndcg@4:0.8759,ndcg@6:0.8772,group_auc:0.8968\n","step 20 , total_loss: 0.8484, data_loss: 0.8484\n","step 40 , total_loss: 0.7578, data_loss: 0.7578\n","step 60 , total_loss: 0.7939, data_loss: 0.7939\n","step 80 , total_loss: 0.7844, data_loss: 0.7844\n","step 100 , total_loss: 0.7638, data_loss: 0.7638\n","step 120 , total_loss: 0.7694, data_loss: 0.7694\n","step 140 , total_loss: 0.7642, data_loss: 0.7642\n","step 160 , total_loss: 0.7377, data_loss: 0.7377\n","step 180 , total_loss: 0.7901, data_loss: 0.7901\n","step 200 , total_loss: 0.7846, data_loss: 0.7846\n","step 220 , total_loss: 0.7691, data_loss: 0.7691\n","step 240 , total_loss: 0.7163, data_loss: 0.7163\n","step 260 , total_loss: 0.7759, data_loss: 0.7759\n","step 280 , total_loss: 0.7244, data_loss: 0.7244\n","step 300 , total_loss: 0.7578, data_loss: 0.7578\n","step 320 , total_loss: 0.7840, data_loss: 0.7840\n","step 340 , total_loss: 0.8228, data_loss: 0.8228\n","step 360 , total_loss: 0.8546, data_loss: 0.8546\n","step 380 , total_loss: 0.7657, data_loss: 0.7657\n","step 400 , total_loss: 0.7834, data_loss: 0.7834\n","step 420 , total_loss: 0.6800, data_loss: 0.6800\n","step 440 , total_loss: 0.7599, data_loss: 0.7599\n","step 460 , total_loss: 0.7530, data_loss: 0.7530\n","step 480 , total_loss: 0.7454, data_loss: 0.7454\n","step 500 , total_loss: 0.7147, data_loss: 0.7147\n","step 520 , total_loss: 0.7170, data_loss: 0.7170\n","step 540 , total_loss: 0.7863, data_loss: 0.7863\n","step 560 , total_loss: 0.7034, data_loss: 0.7034\n","step 580 , total_loss: 0.7148, data_loss: 0.7148\n","step 600 , total_loss: 0.7835, data_loss: 0.7835\n","step 620 , total_loss: 0.7818, data_loss: 0.7818\n","step 640 , total_loss: 0.7802, data_loss: 0.7802\n","step 660 , total_loss: 0.8623, data_loss: 0.8623\n","step 680 , total_loss: 0.6623, data_loss: 0.6623\n","step 700 , total_loss: 0.7574, data_loss: 0.7574\n","step 720 , total_loss: 0.7507, data_loss: 0.7507\n","step 740 , total_loss: 0.7672, data_loss: 0.7672\n","step 760 , total_loss: 0.7914, data_loss: 0.7914\n","step 780 , total_loss: 0.7075, data_loss: 0.7075\n","step 800 , total_loss: 0.7648, data_loss: 0.7648\n","step 820 , total_loss: 0.7524, data_loss: 0.7524\n","step 840 , total_loss: 0.7388, data_loss: 0.7388\n","step 860 , total_loss: 0.7257, data_loss: 0.7257\n","step 880 , total_loss: 0.7631, data_loss: 0.7631\n","step 900 , total_loss: 0.7085, data_loss: 0.7085\n","step 920 , total_loss: 0.7945, data_loss: 0.7945\n","eval valid at epoch 8: auc:0.8995,logloss:0.3856,mean_mrr:0.8364,ndcg@2:0.836,ndcg@4:0.8772,ndcg@6:0.8783,group_auc:0.8983\n","step 20 , total_loss: 0.7169, data_loss: 0.7169\n","step 40 , total_loss: 0.7914, data_loss: 0.7914\n","step 60 , total_loss: 0.7876, data_loss: 0.7876\n","step 80 , total_loss: 0.7443, data_loss: 0.7443\n","step 100 , total_loss: 0.7482, data_loss: 0.7482\n","step 120 , total_loss: 0.7004, data_loss: 0.7004\n","step 140 , total_loss: 0.7260, data_loss: 0.7260\n","step 160 , total_loss: 0.6909, data_loss: 0.6909\n","step 180 , total_loss: 0.7387, data_loss: 0.7387\n","step 200 , total_loss: 0.7397, data_loss: 0.7397\n","step 220 , total_loss: 0.7617, data_loss: 0.7617\n","step 240 , total_loss: 0.7693, data_loss: 0.7693\n","step 260 , total_loss: 0.7511, data_loss: 0.7511\n","step 280 , total_loss: 0.7448, data_loss: 0.7448\n","step 300 , total_loss: 0.6675, data_loss: 0.6675\n","step 320 , total_loss: 0.6865, data_loss: 0.6865\n","step 340 , total_loss: 0.8139, data_loss: 0.8139\n","step 360 , total_loss: 0.7877, data_loss: 0.7877\n","step 380 , total_loss: 0.7750, data_loss: 0.7750\n","step 400 , total_loss: 0.7681, data_loss: 0.7681\n","step 420 , total_loss: 0.7865, data_loss: 0.7865\n","step 440 , total_loss: 0.7642, data_loss: 0.7642\n","step 460 , total_loss: 0.7474, data_loss: 0.7474\n","step 480 , total_loss: 0.7269, data_loss: 0.7269\n","step 500 , total_loss: 0.7342, data_loss: 0.7342\n","step 520 , total_loss: 0.7250, data_loss: 0.7250\n","step 540 , total_loss: 0.7747, data_loss: 0.7747\n","step 560 , total_loss: 0.7099, data_loss: 0.7099\n","step 580 , total_loss: 0.8111, data_loss: 0.8111\n","step 600 , total_loss: 0.7344, data_loss: 0.7344\n","step 620 , total_loss: 0.7298, data_loss: 0.7298\n","step 640 , total_loss: 0.7488, data_loss: 0.7488\n","step 660 , total_loss: 0.7467, data_loss: 0.7467\n","step 680 , total_loss: 0.7080, data_loss: 0.7080\n","step 700 , total_loss: 0.7007, data_loss: 0.7007\n","step 720 , total_loss: 0.7449, data_loss: 0.7449\n","step 740 , total_loss: 0.7498, data_loss: 0.7498\n","step 760 , total_loss: 0.8078, data_loss: 0.8078\n","step 780 , total_loss: 0.7866, data_loss: 0.7866\n","step 800 , total_loss: 0.7117, data_loss: 0.7117\n","step 820 , total_loss: 0.8334, data_loss: 0.8334\n","step 840 , total_loss: 0.7481, data_loss: 0.7481\n","step 860 , total_loss: 0.6748, data_loss: 0.6748\n","step 880 , total_loss: 0.7293, data_loss: 0.7293\n","step 900 , total_loss: 0.6585, data_loss: 0.6585\n","step 920 , total_loss: 0.8240, data_loss: 0.8240\n","eval valid at epoch 9: auc:0.9007,logloss:0.393,mean_mrr:0.8344,ndcg@2:0.8344,ndcg@4:0.8758,ndcg@6:0.8769,group_auc:0.8973\n","step 20 , total_loss: 0.7142, data_loss: 0.7142\n","step 40 , total_loss: 0.7313, data_loss: 0.7313\n","step 60 , total_loss: 0.6477, data_loss: 0.6477\n","step 80 , total_loss: 0.7234, data_loss: 0.7234\n","step 100 , total_loss: 0.7045, data_loss: 0.7045\n","step 120 , total_loss: 0.7580, data_loss: 0.7580\n","step 140 , total_loss: 0.8401, data_loss: 0.8401\n","step 160 , total_loss: 0.7448, data_loss: 0.7448\n","step 180 , total_loss: 0.7398, data_loss: 0.7398\n","step 200 , total_loss: 0.7297, data_loss: 0.7297\n","step 220 , total_loss: 0.7897, data_loss: 0.7897\n","step 240 , total_loss: 0.7568, data_loss: 0.7568\n","step 260 , total_loss: 0.7246, data_loss: 0.7246\n","step 280 , total_loss: 0.7400, data_loss: 0.7400\n","step 300 , total_loss: 0.7986, data_loss: 0.7986\n","step 320 , total_loss: 0.7538, data_loss: 0.7538\n","step 340 , total_loss: 0.7382, data_loss: 0.7382\n","step 360 , total_loss: 0.8056, data_loss: 0.8056\n","step 380 , total_loss: 0.6910, data_loss: 0.6910\n","step 400 , total_loss: 0.7795, data_loss: 0.7795\n","step 420 , total_loss: 0.7549, data_loss: 0.7549\n","step 440 , total_loss: 0.6486, data_loss: 0.6486\n","step 460 , total_loss: 0.6819, data_loss: 0.6819\n","step 480 , total_loss: 0.8344, data_loss: 0.8344\n","step 500 , total_loss: 0.7812, data_loss: 0.7812\n","step 520 , total_loss: 0.7172, data_loss: 0.7172\n","step 540 , total_loss: 0.7319, data_loss: 0.7319\n","step 560 , total_loss: 0.7294, data_loss: 0.7294\n","step 580 , total_loss: 0.7230, data_loss: 0.7230\n","step 600 , total_loss: 0.7039, data_loss: 0.7039\n","step 620 , total_loss: 0.6493, data_loss: 0.6493\n","step 640 , total_loss: 0.7092, data_loss: 0.7092\n","step 660 , total_loss: 0.7247, data_loss: 0.7247\n","step 680 , total_loss: 0.7502, data_loss: 0.7502\n","step 700 , total_loss: 0.7584, data_loss: 0.7584\n","step 720 , total_loss: 0.8484, data_loss: 0.8484\n","step 740 , total_loss: 0.6795, data_loss: 0.6795\n","step 760 , total_loss: 0.8205, data_loss: 0.8205\n","step 780 , total_loss: 0.7687, data_loss: 0.7687\n","step 800 , total_loss: 0.7523, data_loss: 0.7523\n","step 820 , total_loss: 0.7198, data_loss: 0.7198\n","step 840 , total_loss: 0.6885, data_loss: 0.6885\n","step 860 , total_loss: 0.8010, data_loss: 0.8010\n","step 880 , total_loss: 0.7716, data_loss: 0.7716\n","step 900 , total_loss: 0.7291, data_loss: 0.7291\n","step 920 , total_loss: 0.7704, data_loss: 0.7704\n","eval valid at epoch 10: auc:0.9022,logloss:0.3941,mean_mrr:0.8365,ndcg@2:0.8364,ndcg@4:0.8772,ndcg@6:0.8784,group_auc:0.8984\n","[(1, {'auc': 0.7666, 'logloss': 0.6064, 'mean_mrr': 0.7021, 'ndcg@2': 0.662, 'ndcg@4': 0.7619, 'ndcg@6': 0.777, 'group_auc': 0.7762}), (2, {'auc': 0.8846, 'logloss': 0.4107, 'mean_mrr': 0.8158, 'ndcg@2': 0.8121, 'ndcg@4': 0.8612, 'ndcg@6': 0.8629, 'group_auc': 0.8837}), (3, {'auc': 0.8923, 'logloss': 0.383, 'mean_mrr': 0.8253, 'ndcg@2': 0.8236, 'ndcg@4': 0.8685, 'ndcg@6': 0.8701, 'group_auc': 0.8904}), (4, {'auc': 0.8962, 'logloss': 0.3797, 'mean_mrr': 0.829, 'ndcg@2': 0.827, 'ndcg@4': 0.8712, 'ndcg@6': 0.8728, 'group_auc': 0.8927}), (5, {'auc': 0.8978, 'logloss': 0.3767, 'mean_mrr': 0.8342, 'ndcg@2': 0.8328, 'ndcg@4': 0.8753, 'ndcg@6': 0.8767, 'group_auc': 0.8961}), (6, {'auc': 0.8977, 'logloss': 0.3789, 'mean_mrr': 0.8342, 'ndcg@2': 0.8331, 'ndcg@4': 0.8751, 'ndcg@6': 0.8767, 'group_auc': 0.8962}), (7, {'auc': 0.8987, 'logloss': 0.3789, 'mean_mrr': 0.8349, 'ndcg@2': 0.834, 'ndcg@4': 0.8759, 'ndcg@6': 0.8772, 'group_auc': 0.8968}), (8, {'auc': 0.8995, 'logloss': 0.3856, 'mean_mrr': 0.8364, 'ndcg@2': 0.836, 'ndcg@4': 0.8772, 'ndcg@6': 0.8783, 'group_auc': 0.8983}), (9, {'auc': 0.9007, 'logloss': 0.393, 'mean_mrr': 0.8344, 'ndcg@2': 0.8344, 'ndcg@4': 0.8758, 'ndcg@6': 0.8769, 'group_auc': 0.8973}), (10, {'auc': 0.9022, 'logloss': 0.3941, 'mean_mrr': 0.8365, 'ndcg@2': 0.8364, 'ndcg@4': 0.8772, 'ndcg@6': 0.8784, 'group_auc': 0.8984})]\n","best epoch: 10\n","Time cost for training is 323.47 mins\n"]}],"source":["with Timer() as train_time:\n","    model = model.fit(train_file, valid_file, valid_num_ngs=valid_num_ngs) \n","\n","# valid_num_ngs is the number of negative lines after each positive line in your valid_file \n","# we will evaluate the performance of model on valid_file every epoch\n","print('Time cost for training is {0:.2f} mins'.format(train_time.interval/60.0))"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":53342,"status":"ok","timestamp":1650000644460,"user":{"displayName":"조예은","userId":"06018350812551678054"},"user_tz":-540},"id":"gmm3DPjbiY9N","outputId":"8d589301-91a9-446f-99dc-ebcff115c0a7"},"outputs":[{"name":"stdout","output_type":"stream","text":["{'auc': 0.9018, 'logloss': 0.3918, 'mean_mrr': 0.7208, 'ndcg@2': 0.6864, 'ndcg@4': 0.77, 'ndcg@6': 0.7871, 'group_auc': 0.8998}\n"]}],"source":["res_syn = model.run_eval(test_file, num_ngs=test_num_ngs)\n","print(res_syn)"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"executionInfo":{"elapsed":428,"status":"ok","timestamp":1650000657669,"user":{"displayName":"조예은","userId":"06018350812551678054"},"user_tz":-540},"id":"VB7MBFNkjl5e","outputId":"976697e7-e35e-416d-9e5a-436c9032db89"},"outputs":[{"data":{"application/scrapbook.scrap.json+json":{"data":{"auc":0.9018,"group_auc":0.8998,"logloss":0.3918,"mean_mrr":0.7208,"ndcg@2":0.6864,"ndcg@4":0.77,"ndcg@6":0.7871},"encoder":"json","name":"res_syn","version":1}},"metadata":{"scrapbook":{"data":true,"display":false,"name":"res_syn"}},"output_type":"display_data"}],"source":["sb.glue(\"res_syn\", res_syn)"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":35776,"status":"ok","timestamp":1650000695071,"user":{"displayName":"조예은","userId":"06018350812551678054"},"user_tz":-540},"id":"h2uxYGfFjpL9"},"outputs":[],"source":["model = model.predict(test_file, output_file)"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7124,"status":"ok","timestamp":1650000712820,"user":{"displayName":"조예은","userId":"06018350812551678054"},"user_tz":-540},"id":"wGZ26pt-ujeo","outputId":"5e57cd48-8e85-463f-e278-ec02afe67ef6"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/recommenders/models/deeprec/models/base_model.py:705: UserWarning: `tf.layers.batch_normalization` is deprecated and will be removed in a future version. Please use `tf.keras.layers.BatchNormalization` instead. In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.BatchNormalization` documentation).\n","  training=self.is_train_stage,\n","/usr/local/lib/python3.7/dist-packages/keras/legacy_tf_layers/normalization.py:463: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n","  return layer.apply(inputs, training=training)\n"]},{"name":"stdout","output_type":"stream","text":["loading saved model in resources/Recommenders/model/best_model\n"]}],"source":["model_best_trained = SeqModel(hparams, input_creator, seed=RANDOM_SEED)\n","path_best_trained = os.path.join(hparams.MODEL_DIR, \"best_model\")\n","print('loading saved model in {0}'.format(path_best_trained))\n","model_best_trained.load_model(path_best_trained)"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":75245,"status":"ok","timestamp":1650000790316,"user":{"displayName":"조예은","userId":"06018350812551678054"},"user_tz":-540},"id":"fKtBHxlGu5-K","outputId":"d3fdcadf-665d-4120-c4fa-78af49376d1b"},"outputs":[{"data":{"text/plain":["{'auc': 0.9018,\n"," 'group_auc': 0.8998,\n"," 'logloss': 0.3918,\n"," 'mean_mrr': 0.7208,\n"," 'ndcg@2': 0.6864,\n"," 'ndcg@4': 0.77,\n"," 'ndcg@6': 0.7871}"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["model_best_trained.run_eval(test_file, num_ngs=test_num_ngs)"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":48189,"status":"ok","timestamp":1650000881133,"user":{"displayName":"조예은","userId":"06018350812551678054"},"user_tz":-540},"id":"XH7L5VOQvARx","outputId":"048eb05a-af69-47d3-9111-62281609532d"},"outputs":[{"data":{"text/plain":["<recommenders.models.deeprec.models.sequential.sli_rec.SLI_RECModel at 0x7f2f69884910>"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["model_best_trained.predict(test_file, output_file)"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPMZYkqYRJvrImpPUSQoA76","collapsed_sections":[],"mount_file_id":"1aN-Xq8xQ0gMEOoDsqX8RKLj3SKskndqj","name":"SLI_REC.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
