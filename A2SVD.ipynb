{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4785,"status":"ok","timestamp":1649946944115,"user":{"displayName":"조예은","userId":"06018350812551678054"},"user_tz":-540},"id":"Sknnn2xs-AHi","outputId":"ce8fcf33-cadf-4747-95ea-84eb61493cee"},"outputs":[{"name":"stdout","output_type":"stream","text":["System version: 3.8.9 (tags/v3.8.9:a743f81, Apr  6 2021, 14:02:34) [MSC v.1928 64 bit (AMD64)]\n","Tensorflow version: 2.8.0\n"]}],"source":["import sys\n","import os\n","import logging\n","import papermill as pm\n","import scrapbook as sb\n","from tempfile import TemporaryDirectory\n","import numpy as np\n","import tensorflow.compat.v1 as tf\n","tf.get_logger().setLevel('ERROR') # only show error messages\n","\n","from recommenders.utils.timer import Timer\n","from recommenders.utils.constants import SEED\n","from recommenders.models.deeprec.deeprec_utils import (\n","    prepare_hparams\n",")\n","\n","from resources.data_preprocessing import data_preprocessing\n","# from recommenders.datasets.amazon_reviews import download_and_extract, data_preprocessing\n","from recommenders.datasets.download_utils import maybe_download\n","\n","\n","#from recommenders.models.deeprec.models.sequential.sli_rec import SLI_RECModel as SeqModel\n","####  to use the other model, use one of the following lines:\n","from recommenders.models.deeprec.models.sequential.asvd import A2SVDModel as SeqModel\n","# from recommenders.models.deeprec.models.sequential.caser import CaserModel as SeqModel\n","# from recommenders.models.deeprec.models.sequential.gru4rec import GRU4RecModel as SeqModel\n","# from recommenders.models.deeprec.models.sequential.sum import SUMModel as SeqModel\n","\n","#from recommenders.models.deeprec.models.sequential.nextitnet import NextItNetModel\n","\n","from recommenders.models.deeprec.io.sequential_iterator import SequentialIterator\n","#from recommenders.models.deeprec.io.nextitnet_iterator import NextItNetIterator\n","\n","print(\"System version: {}\".format(sys.version))\n","print(\"Tensorflow version: {}\".format(tf.__version__))"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":401,"status":"ok","timestamp":1649946958359,"user":{"displayName":"조예은","userId":"06018350812551678054"},"user_tz":-540},"id":"sxtfUZfP-ZWR"},"outputs":[],"source":["##  ATTENTION: change to the corresponding config file, e.g., caser.yaml for CaserModel, sum.yaml for SUMModel\n","# yaml_file = '../../recommenders/models/deeprec/config/sli_rec.yaml'  \n","yaml_file = './asvd.yaml'  "]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":322,"status":"ok","timestamp":1649947140491,"user":{"displayName":"조예은","userId":"06018350812551678054"},"user_tz":-540},"id":"_NnzhN4h_rI5"},"outputs":[],"source":["EPOCHS = 10\n","BATCH_SIZE = 400\n","RANDOM_SEED = SEED  # Set None for non-deterministic result\n","\n","data_path = os.path.join(\"resources/\")"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":45584,"status":"ok","timestamp":1649947187284,"user":{"displayName":"조예은","userId":"06018350812551678054"},"user_tz":-540},"id":"0RAMNcI3AClc","outputId":"8b52057f-b32f-4f24-8ed6-52571d5e203c"},"outputs":[],"source":["# for test\n","train_file = os.path.join(data_path, r'train_data')\n","valid_file = os.path.join(data_path, r'valid_data')\n","test_file = os.path.join(data_path, r'test_data')\n","user_vocab = os.path.join(data_path, r'user_vocab.pkl')\n","item_vocab = os.path.join(data_path, r'item_vocab.pkl')\n","cate_vocab = os.path.join(data_path, r'cate_vocab.pkl')\n","output_file = os.path.join(data_path, r'output_a2svd.txt')\n","\n","# reviews_name = 'json'\n","# meta_name = 'json'\n","# reviews_file = os.path.join(data_path, reviews_name)\n","# meta_file = os.path.join(data_path, meta_name)\n","train_num_ngs = 4 # number of negative instances with a positive instance for training\n","valid_num_ngs = 4 # number of negative instances with a positive instance for validation\n","test_num_ngs = 9 # number of negative instances with a positive instance for testing\n","sample_rate = 0.01 # sample a small item set for training and testing here for fast example\n","\n","input_files = [data_path, train_file, valid_file, test_file, user_vocab, item_vocab, cate_vocab]\n","\n","if not os.path.exists(train_file):\n","    # download_and_extract(reviews_name, reviews_file)\n","    # download_and_extract(meta_name, meta_file)\n","    data_preprocessing(*input_files, sample_rate=sample_rate, valid_num_ngs=valid_num_ngs, test_num_ngs=test_num_ngs)\n","    #### uncomment this for the NextItNet model, because it does not need to unfold the user history\n","    # data_preprocessing(*input_files, sample_rate=sample_rate, valid_num_ngs=valid_num_ngs, test_num_ngs=test_num_ngs, is_history_expanding=False)\n"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":261,"status":"ok","timestamp":1649947208847,"user":{"displayName":"조예은","userId":"06018350812551678054"},"user_tz":-540},"id":"rwTMO3WEWaCu"},"outputs":[],"source":["### NOTE:  \n","### remember to use `_create_vocab(train_file, user_vocab, item_vocab, cate_vocab)` to generate the user_vocab, item_vocab and cate_vocab files, if you are using your own dataset rather than using our demo Amazon dataset.\n","hparams = prepare_hparams(yaml_file, \n","                          embed_l2=0., \n","                          layer_l2=0., \n","                          learning_rate=0.001,  # set to 0.01 if batch normalization is disable\n","                          epochs=EPOCHS,\n","                          batch_size=BATCH_SIZE,\n","                          show_step=20,\n","                          MODEL_DIR=os.path.join(data_path, \"model\", \"a2svd/\"),\n","                          SUMMARIES_DIR=os.path.join(data_path, \"summary\", \"a2svd/\"),\n","                          user_vocab=user_vocab,\n","                          item_vocab=item_vocab,\n","                          cate_vocab=cate_vocab,\n","                          need_sample=True,\n","                          train_num_ngs=train_num_ngs, # provides the number of negative instances for each positive instance for loss computation.\n","                          attention_size = 40\n","            )"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":281,"status":"ok","timestamp":1649947211274,"user":{"displayName":"조예은","userId":"06018350812551678054"},"user_tz":-540},"id":"4Rvq7gFluVPq"},"outputs":[],"source":["input_creator = SequentialIterator\n","#### uncomment this for the NextItNet model, because it needs a special data iterator for training\n","#input_creator = NextItNetIterator"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2511,"status":"ok","timestamp":1649947217731,"user":{"displayName":"조예은","userId":"06018350812551678054"},"user_tz":-540},"id":"VFNfgcKNuaVv","outputId":"c93a77b7-4d79-4c6f-ab1e-9ba4014a97e3"},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\조예은\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\recommenders\\models\\deeprec\\models\\base_model.py:701: UserWarning: `tf.layers.batch_normalization` is deprecated and will be removed in a future version. Please use `tf.keras.layers.BatchNormalization` instead. In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.BatchNormalization` documentation).\n","  curr_hidden_nn_layer = tf.compat.v1.layers.batch_normalization(\n","C:\\Users\\조예은\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\legacy_tf_layers\\normalization.py:463: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n","  return layer.apply(inputs, training=training)\n"]}],"source":["model = SeqModel(hparams, input_creator, seed=RANDOM_SEED)\n","\n","## sometimes we don't want to train a model from scratch\n","## then we can load a pre-trained model like this: \n","#model.load_model(r'your_model_path')"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":29393,"status":"ok","timestamp":1649947248801,"user":{"displayName":"조예은","userId":"06018350812551678054"},"user_tz":-540},"id":"8AQxPlk-uheU","outputId":"045944e8-4cf5-463a-fd55-a6cd565ab6ba"},"outputs":[{"name":"stdout","output_type":"stream","text":["{'auc': 0.5312, 'logloss': 0.6931, 'mean_mrr': 0.2925, 'ndcg@2': 0.1621, 'ndcg@4': 0.2551, 'ndcg@6': 0.3341, 'group_auc': 0.5341}\n"]}],"source":["# test_num_ngs is the number of negative lines after each positive line in your test_file\n","print(model.run_eval(test_file, num_ngs=test_num_ngs)) "]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2946200,"status":"ok","timestamp":1649950492313,"user":{"displayName":"조예은","userId":"06018350812551678054"},"user_tz":-540},"id":"QFKQu9rGu0jq","outputId":"818611ad-d718-4214-a3bd-163be4668815"},"outputs":[{"name":"stdout","output_type":"stream","text":["step 20 , total_loss: 1.5353, data_loss: 1.5353\n","step 40 , total_loss: 1.4842, data_loss: 1.4842\n","step 60 , total_loss: 1.4838, data_loss: 1.4838\n","step 80 , total_loss: 1.4324, data_loss: 1.4324\n","step 100 , total_loss: 1.4486, data_loss: 1.4486\n","step 120 , total_loss: 1.3758, data_loss: 1.3758\n","step 140 , total_loss: 1.4137, data_loss: 1.4137\n","step 160 , total_loss: 1.4174, data_loss: 1.4174\n","step 180 , total_loss: 1.3178, data_loss: 1.3178\n","step 200 , total_loss: 1.4523, data_loss: 1.4523\n","step 220 , total_loss: 1.4111, data_loss: 1.4111\n","step 240 , total_loss: 1.4190, data_loss: 1.4190\n","step 260 , total_loss: 1.3954, data_loss: 1.3954\n","step 280 , total_loss: 1.4089, data_loss: 1.4089\n","step 300 , total_loss: 1.4262, data_loss: 1.4262\n","step 320 , total_loss: 1.4145, data_loss: 1.4145\n","step 340 , total_loss: 1.4501, data_loss: 1.4501\n","step 360 , total_loss: 1.3968, data_loss: 1.3968\n","step 380 , total_loss: 1.3793, data_loss: 1.3793\n","step 400 , total_loss: 1.3580, data_loss: 1.3580\n","step 420 , total_loss: 1.3781, data_loss: 1.3781\n","step 440 , total_loss: 1.3137, data_loss: 1.3137\n","step 460 , total_loss: 1.3149, data_loss: 1.3149\n","step 480 , total_loss: 1.3558, data_loss: 1.3558\n","step 500 , total_loss: 1.3719, data_loss: 1.3719\n","step 520 , total_loss: 1.3239, data_loss: 1.3239\n","step 540 , total_loss: 1.3326, data_loss: 1.3326\n","step 560 , total_loss: 1.3727, data_loss: 1.3727\n","step 580 , total_loss: 1.3241, data_loss: 1.3241\n","step 600 , total_loss: 1.3282, data_loss: 1.3282\n","step 620 , total_loss: 1.3841, data_loss: 1.3841\n","step 640 , total_loss: 1.3738, data_loss: 1.3738\n","step 660 , total_loss: 1.3276, data_loss: 1.3276\n","step 680 , total_loss: 1.3012, data_loss: 1.3012\n","step 700 , total_loss: 1.3654, data_loss: 1.3654\n","step 720 , total_loss: 1.3542, data_loss: 1.3542\n","step 740 , total_loss: 1.2958, data_loss: 1.2958\n","step 760 , total_loss: 1.3719, data_loss: 1.3719\n","step 780 , total_loss: 1.3349, data_loss: 1.3349\n","step 800 , total_loss: 1.3398, data_loss: 1.3398\n","step 820 , total_loss: 1.2769, data_loss: 1.2769\n","step 840 , total_loss: 1.2802, data_loss: 1.2802\n","step 860 , total_loss: 1.3142, data_loss: 1.3142\n","step 880 , total_loss: 1.2662, data_loss: 1.2662\n","step 900 , total_loss: 1.3569, data_loss: 1.3569\n","step 920 , total_loss: 1.3120, data_loss: 1.3120\n","eval valid at epoch 1: auc:0.7377,logloss:0.6454,mean_mrr:0.6527,ndcg@2:0.5972,ndcg@4:0.7197,ndcg@6:0.7397,group_auc:0.7315\n","step 20 , total_loss: 1.3174, data_loss: 1.3174\n","step 40 , total_loss: 1.3103, data_loss: 1.3103\n","step 60 , total_loss: 1.2976, data_loss: 1.2976\n","step 80 , total_loss: 1.3002, data_loss: 1.3002\n","step 100 , total_loss: 1.3066, data_loss: 1.3066\n","step 120 , total_loss: 1.2905, data_loss: 1.2905\n","step 140 , total_loss: 1.3148, data_loss: 1.3148\n","step 160 , total_loss: 1.3078, data_loss: 1.3078\n","step 180 , total_loss: 1.2451, data_loss: 1.2451\n","step 200 , total_loss: 1.3098, data_loss: 1.3098\n","step 220 , total_loss: 1.2592, data_loss: 1.2592\n","step 240 , total_loss: 1.3383, data_loss: 1.3383\n","step 260 , total_loss: 1.3242, data_loss: 1.3242\n","step 280 , total_loss: 1.3381, data_loss: 1.3381\n","step 300 , total_loss: 1.3250, data_loss: 1.3250\n","step 320 , total_loss: 1.3414, data_loss: 1.3414\n","step 340 , total_loss: 1.2982, data_loss: 1.2982\n","step 360 , total_loss: 1.2729, data_loss: 1.2729\n","step 380 , total_loss: 1.2917, data_loss: 1.2917\n","step 400 , total_loss: 1.2868, data_loss: 1.2868\n","step 420 , total_loss: 1.2687, data_loss: 1.2687\n","step 440 , total_loss: 1.2646, data_loss: 1.2646\n","step 460 , total_loss: 1.2583, data_loss: 1.2583\n","step 480 , total_loss: 1.2109, data_loss: 1.2109\n","step 500 , total_loss: 1.2991, data_loss: 1.2991\n","step 520 , total_loss: 1.2869, data_loss: 1.2869\n","step 540 , total_loss: 1.2860, data_loss: 1.2860\n","step 560 , total_loss: 1.2664, data_loss: 1.2664\n","step 580 , total_loss: 1.2681, data_loss: 1.2681\n","step 600 , total_loss: 1.3141, data_loss: 1.3141\n","step 620 , total_loss: 1.2597, data_loss: 1.2597\n","step 640 , total_loss: 1.2590, data_loss: 1.2590\n","step 660 , total_loss: 1.2364, data_loss: 1.2364\n","step 680 , total_loss: 1.3196, data_loss: 1.3196\n","step 700 , total_loss: 1.2770, data_loss: 1.2770\n","step 720 , total_loss: 1.2444, data_loss: 1.2444\n","step 740 , total_loss: 1.2727, data_loss: 1.2727\n","step 760 , total_loss: 1.2856, data_loss: 1.2856\n","step 780 , total_loss: 1.2391, data_loss: 1.2391\n","step 800 , total_loss: 1.3169, data_loss: 1.3169\n","step 820 , total_loss: 1.2167, data_loss: 1.2167\n","step 840 , total_loss: 1.2052, data_loss: 1.2052\n","step 860 , total_loss: 1.2809, data_loss: 1.2809\n","step 880 , total_loss: 1.2556, data_loss: 1.2556\n","step 900 , total_loss: 1.2715, data_loss: 1.2715\n","step 920 , total_loss: 1.2867, data_loss: 1.2867\n","eval valid at epoch 2: auc:0.7493,logloss:0.639,mean_mrr:0.665,ndcg@2:0.6158,ndcg@4:0.7322,ndcg@6:0.7491,group_auc:0.7465\n","step 20 , total_loss: 1.2854, data_loss: 1.2854\n","step 40 , total_loss: 1.2979, data_loss: 1.2979\n","step 60 , total_loss: 1.2226, data_loss: 1.2226\n","step 80 , total_loss: 1.2749, data_loss: 1.2749\n","step 100 , total_loss: 1.2574, data_loss: 1.2574\n","step 120 , total_loss: 1.2478, data_loss: 1.2478\n","step 140 , total_loss: 1.2814, data_loss: 1.2814\n","step 160 , total_loss: 1.2184, data_loss: 1.2184\n","step 180 , total_loss: 1.2509, data_loss: 1.2509\n","step 200 , total_loss: 1.1797, data_loss: 1.1797\n","step 220 , total_loss: 1.3039, data_loss: 1.3039\n","step 240 , total_loss: 1.2773, data_loss: 1.2773\n","step 260 , total_loss: 1.2746, data_loss: 1.2746\n","step 280 , total_loss: 1.2135, data_loss: 1.2135\n","step 300 , total_loss: 1.2527, data_loss: 1.2527\n","step 320 , total_loss: 1.2200, data_loss: 1.2200\n","step 340 , total_loss: 1.1886, data_loss: 1.1886\n","step 360 , total_loss: 1.3277, data_loss: 1.3277\n","step 380 , total_loss: 1.1902, data_loss: 1.1902\n","step 400 , total_loss: 1.3096, data_loss: 1.3096\n","step 420 , total_loss: 1.2396, data_loss: 1.2396\n","step 440 , total_loss: 1.2313, data_loss: 1.2313\n","step 460 , total_loss: 1.2017, data_loss: 1.2017\n","step 480 , total_loss: 1.1558, data_loss: 1.1558\n","step 500 , total_loss: 1.1750, data_loss: 1.1750\n","step 520 , total_loss: 1.2278, data_loss: 1.2278\n","step 540 , total_loss: 1.1398, data_loss: 1.1398\n","step 560 , total_loss: 1.2116, data_loss: 1.2116\n","step 580 , total_loss: 1.3058, data_loss: 1.3058\n","step 600 , total_loss: 1.2881, data_loss: 1.2881\n","step 620 , total_loss: 1.2282, data_loss: 1.2282\n","step 640 , total_loss: 1.2748, data_loss: 1.2748\n","step 660 , total_loss: 1.1726, data_loss: 1.1726\n","step 680 , total_loss: 1.1917, data_loss: 1.1917\n","step 700 , total_loss: 1.2535, data_loss: 1.2535\n","step 720 , total_loss: 1.2381, data_loss: 1.2381\n","step 740 , total_loss: 1.2515, data_loss: 1.2515\n","step 760 , total_loss: 1.2330, data_loss: 1.2330\n","step 780 , total_loss: 1.2667, data_loss: 1.2667\n","step 800 , total_loss: 1.1506, data_loss: 1.1506\n","step 820 , total_loss: 1.1560, data_loss: 1.1560\n","step 840 , total_loss: 1.2122, data_loss: 1.2122\n","step 860 , total_loss: 1.2494, data_loss: 1.2494\n","step 880 , total_loss: 1.2231, data_loss: 1.2231\n","step 900 , total_loss: 1.1513, data_loss: 1.1513\n","step 920 , total_loss: 1.2920, data_loss: 1.2920\n","eval valid at epoch 3: auc:0.7646,logloss:0.6224,mean_mrr:0.6803,ndcg@2:0.6377,ndcg@4:0.7478,ndcg@6:0.7608,group_auc:0.7643\n","step 20 , total_loss: 1.2175, data_loss: 1.2175\n","step 40 , total_loss: 1.2243, data_loss: 1.2243\n","step 60 , total_loss: 1.2951, data_loss: 1.2951\n","step 80 , total_loss: 1.2722, data_loss: 1.2722\n","step 100 , total_loss: 1.1992, data_loss: 1.1992\n","step 120 , total_loss: 1.2147, data_loss: 1.2147\n","step 140 , total_loss: 1.1649, data_loss: 1.1649\n","step 160 , total_loss: 1.2108, data_loss: 1.2108\n","step 180 , total_loss: 1.2983, data_loss: 1.2983\n","step 200 , total_loss: 1.1517, data_loss: 1.1517\n","step 220 , total_loss: 1.1516, data_loss: 1.1516\n","step 240 , total_loss: 1.1655, data_loss: 1.1655\n","step 260 , total_loss: 1.2343, data_loss: 1.2343\n","step 280 , total_loss: 1.1707, data_loss: 1.1707\n","step 300 , total_loss: 1.1815, data_loss: 1.1815\n","step 320 , total_loss: 1.2201, data_loss: 1.2201\n","step 340 , total_loss: 1.2264, data_loss: 1.2264\n","step 360 , total_loss: 1.1239, data_loss: 1.1239\n","step 380 , total_loss: 1.1455, data_loss: 1.1455\n","step 400 , total_loss: 1.2158, data_loss: 1.2158\n","step 420 , total_loss: 1.2216, data_loss: 1.2216\n","step 440 , total_loss: 1.1458, data_loss: 1.1458\n","step 460 , total_loss: 1.1893, data_loss: 1.1893\n","step 480 , total_loss: 1.2751, data_loss: 1.2751\n","step 500 , total_loss: 1.1749, data_loss: 1.1749\n","step 520 , total_loss: 1.2095, data_loss: 1.2095\n","step 540 , total_loss: 1.1482, data_loss: 1.1482\n","step 560 , total_loss: 1.2012, data_loss: 1.2012\n","step 580 , total_loss: 1.2131, data_loss: 1.2131\n","step 600 , total_loss: 1.0797, data_loss: 1.0797\n","step 620 , total_loss: 1.1695, data_loss: 1.1695\n","step 640 , total_loss: 1.1978, data_loss: 1.1978\n","step 660 , total_loss: 1.1569, data_loss: 1.1569\n","step 680 , total_loss: 1.1768, data_loss: 1.1768\n","step 700 , total_loss: 1.1465, data_loss: 1.1465\n","step 720 , total_loss: 1.1840, data_loss: 1.1840\n","step 740 , total_loss: 1.2823, data_loss: 1.2823\n","step 760 , total_loss: 1.1478, data_loss: 1.1478\n","step 780 , total_loss: 1.1093, data_loss: 1.1093\n","step 800 , total_loss: 1.1766, data_loss: 1.1766\n","step 820 , total_loss: 1.1655, data_loss: 1.1655\n","step 840 , total_loss: 1.1750, data_loss: 1.1750\n","step 860 , total_loss: 1.1217, data_loss: 1.1217\n","step 880 , total_loss: 1.1618, data_loss: 1.1618\n","step 900 , total_loss: 1.1857, data_loss: 1.1857\n","step 920 , total_loss: 1.2915, data_loss: 1.2915\n","eval valid at epoch 4: auc:0.7757,logloss:0.6484,mean_mrr:0.6869,ndcg@2:0.6465,ndcg@4:0.7554,ndcg@6:0.7658,group_auc:0.7726\n","step 20 , total_loss: 1.1198, data_loss: 1.1198\n","step 40 , total_loss: 1.1096, data_loss: 1.1096\n","step 60 , total_loss: 1.1645, data_loss: 1.1645\n","step 80 , total_loss: 1.1439, data_loss: 1.1439\n","step 100 , total_loss: 1.1343, data_loss: 1.1343\n","step 120 , total_loss: 1.1787, data_loss: 1.1787\n","step 140 , total_loss: 1.1127, data_loss: 1.1127\n","step 160 , total_loss: 1.1104, data_loss: 1.1104\n","step 180 , total_loss: 1.1478, data_loss: 1.1478\n","step 200 , total_loss: 1.1755, data_loss: 1.1755\n","step 220 , total_loss: 1.2003, data_loss: 1.2003\n","step 240 , total_loss: 1.1652, data_loss: 1.1652\n","step 260 , total_loss: 1.1721, data_loss: 1.1721\n","step 280 , total_loss: 1.1017, data_loss: 1.1017\n","step 300 , total_loss: 1.1377, data_loss: 1.1377\n","step 320 , total_loss: 1.2514, data_loss: 1.2514\n","step 340 , total_loss: 1.1370, data_loss: 1.1370\n","step 360 , total_loss: 1.1953, data_loss: 1.1953\n","step 380 , total_loss: 1.1140, data_loss: 1.1140\n","step 400 , total_loss: 1.1815, data_loss: 1.1815\n","step 420 , total_loss: 1.1626, data_loss: 1.1626\n","step 440 , total_loss: 1.1489, data_loss: 1.1489\n","step 460 , total_loss: 1.1754, data_loss: 1.1754\n","step 480 , total_loss: 1.2106, data_loss: 1.2106\n","step 500 , total_loss: 1.1149, data_loss: 1.1149\n","step 520 , total_loss: 1.0918, data_loss: 1.0918\n","step 540 , total_loss: 1.1429, data_loss: 1.1429\n","step 560 , total_loss: 1.0892, data_loss: 1.0892\n","step 580 , total_loss: 1.1126, data_loss: 1.1126\n","step 600 , total_loss: 1.1353, data_loss: 1.1353\n","step 620 , total_loss: 1.1069, data_loss: 1.1069\n","step 640 , total_loss: 1.1956, data_loss: 1.1956\n","step 660 , total_loss: 1.1885, data_loss: 1.1885\n","step 680 , total_loss: 1.1443, data_loss: 1.1443\n","step 700 , total_loss: 1.1300, data_loss: 1.1300\n","step 720 , total_loss: 1.0918, data_loss: 1.0918\n","step 740 , total_loss: 1.1489, data_loss: 1.1489\n","step 760 , total_loss: 1.2065, data_loss: 1.2065\n","step 780 , total_loss: 1.1335, data_loss: 1.1335\n","step 800 , total_loss: 1.2013, data_loss: 1.2013\n","step 820 , total_loss: 1.1558, data_loss: 1.1558\n","step 840 , total_loss: 1.2561, data_loss: 1.2561\n","step 860 , total_loss: 1.1526, data_loss: 1.1526\n","step 880 , total_loss: 1.1742, data_loss: 1.1742\n","step 900 , total_loss: 1.1011, data_loss: 1.1011\n","step 920 , total_loss: 1.0493, data_loss: 1.0493\n","eval valid at epoch 5: auc:0.7867,logloss:0.6188,mean_mrr:0.7017,ndcg@2:0.6673,ndcg@4:0.7667,ndcg@6:0.7771,group_auc:0.7866\n","step 20 , total_loss: 1.0991, data_loss: 1.0991\n","step 40 , total_loss: 1.1700, data_loss: 1.1700\n","step 60 , total_loss: 1.1697, data_loss: 1.1697\n","step 80 , total_loss: 1.1830, data_loss: 1.1830\n","step 100 , total_loss: 1.1280, data_loss: 1.1280\n","step 120 , total_loss: 1.1806, data_loss: 1.1806\n","step 140 , total_loss: 1.1439, data_loss: 1.1439\n","step 160 , total_loss: 1.1186, data_loss: 1.1186\n","step 180 , total_loss: 1.0756, data_loss: 1.0756\n","step 200 , total_loss: 1.0945, data_loss: 1.0945\n","step 220 , total_loss: 1.1408, data_loss: 1.1408\n","step 240 , total_loss: 1.1132, data_loss: 1.1132\n","step 260 , total_loss: 1.1694, data_loss: 1.1694\n","step 280 , total_loss: 1.1055, data_loss: 1.1055\n","step 300 , total_loss: 1.1672, data_loss: 1.1672\n","step 320 , total_loss: 1.0908, data_loss: 1.0908\n","step 340 , total_loss: 1.1436, data_loss: 1.1436\n","step 360 , total_loss: 1.1315, data_loss: 1.1315\n","step 380 , total_loss: 1.1551, data_loss: 1.1551\n","step 400 , total_loss: 1.1033, data_loss: 1.1033\n","step 420 , total_loss: 1.0641, data_loss: 1.0641\n","step 440 , total_loss: 1.1656, data_loss: 1.1656\n","step 460 , total_loss: 1.1292, data_loss: 1.1292\n","step 480 , total_loss: 1.1695, data_loss: 1.1695\n","step 500 , total_loss: 1.1021, data_loss: 1.1021\n","step 520 , total_loss: 1.0718, data_loss: 1.0718\n","step 540 , total_loss: 1.1888, data_loss: 1.1888\n","step 560 , total_loss: 1.0409, data_loss: 1.0409\n","step 580 , total_loss: 1.1810, data_loss: 1.1810\n","step 600 , total_loss: 1.0762, data_loss: 1.0762\n","step 620 , total_loss: 1.0872, data_loss: 1.0872\n","step 640 , total_loss: 1.0612, data_loss: 1.0612\n","step 660 , total_loss: 1.1784, data_loss: 1.1784\n","step 680 , total_loss: 1.1069, data_loss: 1.1069\n","step 700 , total_loss: 1.1320, data_loss: 1.1320\n","step 720 , total_loss: 1.1246, data_loss: 1.1246\n","step 740 , total_loss: 1.1155, data_loss: 1.1155\n","step 760 , total_loss: 1.0905, data_loss: 1.0905\n","step 780 , total_loss: 1.1273, data_loss: 1.1273\n","step 800 , total_loss: 1.1314, data_loss: 1.1314\n","step 820 , total_loss: 1.1071, data_loss: 1.1071\n","step 840 , total_loss: 1.1475, data_loss: 1.1475\n","step 860 , total_loss: 1.1053, data_loss: 1.1053\n","step 880 , total_loss: 1.1485, data_loss: 1.1485\n","step 900 , total_loss: 1.0752, data_loss: 1.0752\n","step 920 , total_loss: 1.1158, data_loss: 1.1158\n","eval valid at epoch 6: auc:0.7975,logloss:0.6487,mean_mrr:0.7107,ndcg@2:0.6794,ndcg@4:0.776,ndcg@6:0.7839,group_auc:0.7961\n","step 20 , total_loss: 1.1394, data_loss: 1.1394\n","step 40 , total_loss: 1.0738, data_loss: 1.0738\n","step 60 , total_loss: 1.1434, data_loss: 1.1434\n","step 80 , total_loss: 1.0950, data_loss: 1.0950\n","step 100 , total_loss: 1.1306, data_loss: 1.1306\n","step 120 , total_loss: 1.1148, data_loss: 1.1148\n","step 140 , total_loss: 1.2037, data_loss: 1.2037\n","step 160 , total_loss: 1.1418, data_loss: 1.1418\n","step 180 , total_loss: 1.0892, data_loss: 1.0892\n","step 200 , total_loss: 1.1644, data_loss: 1.1644\n","step 220 , total_loss: 1.1102, data_loss: 1.1102\n","step 240 , total_loss: 1.0915, data_loss: 1.0915\n","step 260 , total_loss: 1.0839, data_loss: 1.0839\n","step 280 , total_loss: 1.1488, data_loss: 1.1488\n","step 300 , total_loss: 1.1058, data_loss: 1.1058\n","step 320 , total_loss: 1.0740, data_loss: 1.0740\n","step 340 , total_loss: 1.1180, data_loss: 1.1180\n","step 360 , total_loss: 1.0687, data_loss: 1.0687\n","step 380 , total_loss: 1.0473, data_loss: 1.0473\n","step 400 , total_loss: 1.1170, data_loss: 1.1170\n","step 420 , total_loss: 1.1073, data_loss: 1.1073\n","step 440 , total_loss: 1.0504, data_loss: 1.0504\n","step 460 , total_loss: 1.1630, data_loss: 1.1630\n","step 480 , total_loss: 1.1490, data_loss: 1.1490\n","step 500 , total_loss: 1.1196, data_loss: 1.1196\n","step 520 , total_loss: 1.0447, data_loss: 1.0447\n","step 540 , total_loss: 1.0689, data_loss: 1.0689\n","step 560 , total_loss: 1.0765, data_loss: 1.0765\n","step 580 , total_loss: 1.0731, data_loss: 1.0731\n","step 600 , total_loss: 1.0961, data_loss: 1.0961\n","step 620 , total_loss: 1.1500, data_loss: 1.1500\n","step 640 , total_loss: 1.1206, data_loss: 1.1206\n","step 660 , total_loss: 1.0709, data_loss: 1.0709\n","step 680 , total_loss: 1.0914, data_loss: 1.0914\n","step 700 , total_loss: 1.0871, data_loss: 1.0871\n","step 720 , total_loss: 1.1041, data_loss: 1.1041\n","step 740 , total_loss: 1.0805, data_loss: 1.0805\n","step 760 , total_loss: 1.0666, data_loss: 1.0666\n","step 780 , total_loss: 1.0843, data_loss: 1.0843\n","step 800 , total_loss: 1.0843, data_loss: 1.0843\n","step 820 , total_loss: 1.0073, data_loss: 1.0073\n","step 840 , total_loss: 1.1526, data_loss: 1.1526\n","step 860 , total_loss: 1.1290, data_loss: 1.1290\n","step 880 , total_loss: 1.1111, data_loss: 1.1111\n","step 900 , total_loss: 1.1847, data_loss: 1.1847\n","step 920 , total_loss: 1.0483, data_loss: 1.0483\n","eval valid at epoch 7: auc:0.8082,logloss:0.6587,mean_mrr:0.7201,ndcg@2:0.6931,ndcg@4:0.7837,ndcg@6:0.791,group_auc:0.8051\n","step 20 , total_loss: 1.1325, data_loss: 1.1325\n","step 40 , total_loss: 1.0866, data_loss: 1.0866\n","step 60 , total_loss: 1.1080, data_loss: 1.1080\n","step 80 , total_loss: 1.1388, data_loss: 1.1388\n","step 100 , total_loss: 1.1260, data_loss: 1.1260\n","step 120 , total_loss: 1.0955, data_loss: 1.0955\n","step 140 , total_loss: 1.0769, data_loss: 1.0769\n","step 160 , total_loss: 1.0683, data_loss: 1.0683\n","step 180 , total_loss: 1.1443, data_loss: 1.1443\n","step 200 , total_loss: 1.1034, data_loss: 1.1034\n","step 220 , total_loss: 1.0899, data_loss: 1.0899\n","step 240 , total_loss: 1.0461, data_loss: 1.0461\n","step 260 , total_loss: 1.1251, data_loss: 1.1251\n","step 280 , total_loss: 1.0670, data_loss: 1.0670\n","step 300 , total_loss: 1.0514, data_loss: 1.0514\n","step 320 , total_loss: 1.1036, data_loss: 1.1036\n","step 340 , total_loss: 1.1137, data_loss: 1.1137\n","step 360 , total_loss: 1.1547, data_loss: 1.1547\n","step 380 , total_loss: 1.1247, data_loss: 1.1247\n","step 400 , total_loss: 1.0947, data_loss: 1.0947\n","step 420 , total_loss: 1.0092, data_loss: 1.0092\n","step 440 , total_loss: 1.1286, data_loss: 1.1286\n","step 460 , total_loss: 1.0835, data_loss: 1.0835\n","step 480 , total_loss: 1.0715, data_loss: 1.0715\n","step 500 , total_loss: 1.0835, data_loss: 1.0835\n","step 520 , total_loss: 1.0577, data_loss: 1.0577\n","step 540 , total_loss: 1.0819, data_loss: 1.0819\n","step 560 , total_loss: 1.0461, data_loss: 1.0461\n","step 580 , total_loss: 1.0451, data_loss: 1.0451\n","step 600 , total_loss: 1.0870, data_loss: 1.0870\n","step 620 , total_loss: 1.1181, data_loss: 1.1181\n","step 640 , total_loss: 1.1291, data_loss: 1.1291\n","step 660 , total_loss: 1.1561, data_loss: 1.1561\n","step 680 , total_loss: 1.0177, data_loss: 1.0177\n","step 700 , total_loss: 1.0930, data_loss: 1.0930\n","step 720 , total_loss: 1.0482, data_loss: 1.0482\n","step 740 , total_loss: 1.1296, data_loss: 1.1296\n","step 760 , total_loss: 1.1227, data_loss: 1.1227\n","step 780 , total_loss: 1.0619, data_loss: 1.0619\n","step 800 , total_loss: 1.1111, data_loss: 1.1111\n","step 820 , total_loss: 1.0980, data_loss: 1.0980\n","step 840 , total_loss: 1.0659, data_loss: 1.0659\n","step 860 , total_loss: 1.1081, data_loss: 1.1081\n","step 880 , total_loss: 1.1162, data_loss: 1.1162\n","step 900 , total_loss: 1.0391, data_loss: 1.0391\n","step 920 , total_loss: 1.0825, data_loss: 1.0825\n","eval valid at epoch 8: auc:0.8082,logloss:0.6645,mean_mrr:0.7181,ndcg@2:0.6887,ndcg@4:0.7829,ndcg@6:0.7895,group_auc:0.804\n","step 20 , total_loss: 1.0161, data_loss: 1.0161\n","step 40 , total_loss: 1.0929, data_loss: 1.0929\n","step 60 , total_loss: 1.1081, data_loss: 1.1081\n","step 80 , total_loss: 1.0108, data_loss: 1.0108\n","step 100 , total_loss: 1.1002, data_loss: 1.1002\n","step 120 , total_loss: 1.0008, data_loss: 1.0008\n","step 140 , total_loss: 1.1044, data_loss: 1.1044\n","step 160 , total_loss: 1.0198, data_loss: 1.0198\n","step 180 , total_loss: 1.0721, data_loss: 1.0721\n","step 200 , total_loss: 1.0846, data_loss: 1.0846\n","step 220 , total_loss: 1.0634, data_loss: 1.0634\n","step 240 , total_loss: 1.1321, data_loss: 1.1321\n","step 260 , total_loss: 1.0575, data_loss: 1.0575\n","step 280 , total_loss: 1.1159, data_loss: 1.1159\n","step 300 , total_loss: 1.0434, data_loss: 1.0434\n","step 320 , total_loss: 1.0953, data_loss: 1.0953\n","step 340 , total_loss: 1.0820, data_loss: 1.0820\n","step 360 , total_loss: 1.0472, data_loss: 1.0472\n","step 380 , total_loss: 1.0668, data_loss: 1.0668\n","step 400 , total_loss: 1.0853, data_loss: 1.0853\n","step 420 , total_loss: 1.0931, data_loss: 1.0931\n","step 440 , total_loss: 1.0907, data_loss: 1.0907\n","step 460 , total_loss: 1.0944, data_loss: 1.0944\n","step 480 , total_loss: 1.1331, data_loss: 1.1331\n","step 500 , total_loss: 1.0136, data_loss: 1.0136\n","step 520 , total_loss: 1.0868, data_loss: 1.0868\n","step 540 , total_loss: 1.1577, data_loss: 1.1577\n","step 560 , total_loss: 1.0253, data_loss: 1.0253\n","step 580 , total_loss: 1.0499, data_loss: 1.0499\n","step 600 , total_loss: 1.1219, data_loss: 1.1219\n","step 620 , total_loss: 1.1045, data_loss: 1.1045\n","step 640 , total_loss: 1.0960, data_loss: 1.0960\n","step 660 , total_loss: 1.0988, data_loss: 1.0988\n","step 680 , total_loss: 1.0536, data_loss: 1.0536\n","step 700 , total_loss: 1.0665, data_loss: 1.0665\n","step 720 , total_loss: 1.0967, data_loss: 1.0967\n","step 740 , total_loss: 1.0760, data_loss: 1.0760\n","step 760 , total_loss: 1.1028, data_loss: 1.1028\n","step 780 , total_loss: 1.1828, data_loss: 1.1828\n","step 800 , total_loss: 1.0350, data_loss: 1.0350\n","step 820 , total_loss: 1.1095, data_loss: 1.1095\n","step 840 , total_loss: 1.0724, data_loss: 1.0724\n","step 860 , total_loss: 1.0562, data_loss: 1.0562\n","step 880 , total_loss: 0.9917, data_loss: 0.9917\n","step 900 , total_loss: 1.0261, data_loss: 1.0261\n","step 920 , total_loss: 1.1858, data_loss: 1.1858\n","eval valid at epoch 9: auc:0.8093,logloss:0.7197,mean_mrr:0.7219,ndcg@2:0.6958,ndcg@4:0.7859,ndcg@6:0.7924,group_auc:0.808\n","step 20 , total_loss: 0.9958, data_loss: 0.9958\n","step 40 , total_loss: 1.0813, data_loss: 1.0813\n","step 60 , total_loss: 1.0258, data_loss: 1.0258\n","step 80 , total_loss: 1.0687, data_loss: 1.0687\n","step 100 , total_loss: 1.0932, data_loss: 1.0932\n","step 120 , total_loss: 1.0919, data_loss: 1.0919\n","step 140 , total_loss: 1.1100, data_loss: 1.1100\n","step 160 , total_loss: 1.0332, data_loss: 1.0332\n","step 180 , total_loss: 1.0293, data_loss: 1.0293\n","step 200 , total_loss: 1.0157, data_loss: 1.0157\n","step 220 , total_loss: 1.0875, data_loss: 1.0875\n","step 240 , total_loss: 1.0659, data_loss: 1.0659\n","step 260 , total_loss: 1.0594, data_loss: 1.0594\n","step 280 , total_loss: 1.0690, data_loss: 1.0690\n","step 300 , total_loss: 1.1278, data_loss: 1.1278\n","step 320 , total_loss: 1.0430, data_loss: 1.0430\n","step 340 , total_loss: 1.1116, data_loss: 1.1116\n","step 360 , total_loss: 1.0843, data_loss: 1.0843\n","step 380 , total_loss: 1.0690, data_loss: 1.0690\n","step 400 , total_loss: 1.0967, data_loss: 1.0967\n","step 420 , total_loss: 1.0298, data_loss: 1.0298\n","step 440 , total_loss: 1.0054, data_loss: 1.0054\n","step 460 , total_loss: 1.0718, data_loss: 1.0718\n","step 480 , total_loss: 1.1980, data_loss: 1.1980\n","step 500 , total_loss: 1.0501, data_loss: 1.0501\n","step 520 , total_loss: 1.0418, data_loss: 1.0418\n","step 540 , total_loss: 1.0556, data_loss: 1.0556\n","step 560 , total_loss: 1.0178, data_loss: 1.0178\n","step 580 , total_loss: 1.0629, data_loss: 1.0629\n","step 600 , total_loss: 1.0459, data_loss: 1.0459\n","step 620 , total_loss: 0.9611, data_loss: 0.9611\n","step 640 , total_loss: 1.0521, data_loss: 1.0521\n","step 660 , total_loss: 1.0424, data_loss: 1.0424\n","step 680 , total_loss: 1.1751, data_loss: 1.1751\n","step 700 , total_loss: 1.0557, data_loss: 1.0557\n","step 720 , total_loss: 1.1451, data_loss: 1.1451\n","step 740 , total_loss: 1.0219, data_loss: 1.0219\n","step 760 , total_loss: 1.1480, data_loss: 1.1480\n","step 780 , total_loss: 1.0190, data_loss: 1.0190\n","step 800 , total_loss: 1.0926, data_loss: 1.0926\n","step 820 , total_loss: 1.0498, data_loss: 1.0498\n","step 840 , total_loss: 0.9913, data_loss: 0.9913\n","step 860 , total_loss: 1.0696, data_loss: 1.0696\n","step 880 , total_loss: 1.0914, data_loss: 1.0914\n","step 900 , total_loss: 1.0980, data_loss: 1.0980\n","step 920 , total_loss: 1.0933, data_loss: 1.0933\n","eval valid at epoch 10: auc:0.8179,logloss:0.7379,mean_mrr:0.7287,ndcg@2:0.7034,ndcg@4:0.7921,ndcg@6:0.7976,group_auc:0.8144\n","[(1, {'auc': 0.7377, 'logloss': 0.6454, 'mean_mrr': 0.6527, 'ndcg@2': 0.5972, 'ndcg@4': 0.7197, 'ndcg@6': 0.7397, 'group_auc': 0.7315}), (2, {'auc': 0.7493, 'logloss': 0.639, 'mean_mrr': 0.665, 'ndcg@2': 0.6158, 'ndcg@4': 0.7322, 'ndcg@6': 0.7491, 'group_auc': 0.7465}), (3, {'auc': 0.7646, 'logloss': 0.6224, 'mean_mrr': 0.6803, 'ndcg@2': 0.6377, 'ndcg@4': 0.7478, 'ndcg@6': 0.7608, 'group_auc': 0.7643}), (4, {'auc': 0.7757, 'logloss': 0.6484, 'mean_mrr': 0.6869, 'ndcg@2': 0.6465, 'ndcg@4': 0.7554, 'ndcg@6': 0.7658, 'group_auc': 0.7726}), (5, {'auc': 0.7867, 'logloss': 0.6188, 'mean_mrr': 0.7017, 'ndcg@2': 0.6673, 'ndcg@4': 0.7667, 'ndcg@6': 0.7771, 'group_auc': 0.7866}), (6, {'auc': 0.7975, 'logloss': 0.6487, 'mean_mrr': 0.7107, 'ndcg@2': 0.6794, 'ndcg@4': 0.776, 'ndcg@6': 0.7839, 'group_auc': 0.7961}), (7, {'auc': 0.8082, 'logloss': 0.6587, 'mean_mrr': 0.7201, 'ndcg@2': 0.6931, 'ndcg@4': 0.7837, 'ndcg@6': 0.791, 'group_auc': 0.8051}), (8, {'auc': 0.8082, 'logloss': 0.6645, 'mean_mrr': 0.7181, 'ndcg@2': 0.6887, 'ndcg@4': 0.7829, 'ndcg@6': 0.7895, 'group_auc': 0.804}), (9, {'auc': 0.8093, 'logloss': 0.7197, 'mean_mrr': 0.7219, 'ndcg@2': 0.6958, 'ndcg@4': 0.7859, 'ndcg@6': 0.7924, 'group_auc': 0.808}), (10, {'auc': 0.8179, 'logloss': 0.7379, 'mean_mrr': 0.7287, 'ndcg@2': 0.7034, 'ndcg@4': 0.7921, 'ndcg@6': 0.7976, 'group_auc': 0.8144})]\n","best epoch: 10\n","Time cost for training is 43.02 mins\n"]}],"source":["with Timer() as train_time:\n","    model = model.fit(train_file, valid_file, valid_num_ngs=valid_num_ngs) \n","\n","# valid_num_ngs is the number of negative lines after each positive line in your valid_file \n","# we will evaluate the performance of model on valid_file every epoch\n","print('Time cost for training is {0:.2f} mins'.format(train_time.interval/60.0))"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17488,"status":"ok","timestamp":1649950583730,"user":{"displayName":"조예은","userId":"06018350812551678054"},"user_tz":-540},"id":"gmm3DPjbiY9N","outputId":"c182caed-d64d-46a2-a247-712b1fddc16c"},"outputs":[{"name":"stdout","output_type":"stream","text":["{'auc': 0.8121, 'logloss': 0.8038, 'mean_mrr': 0.5606, 'ndcg@2': 0.4812, 'ndcg@4': 0.6054, 'ndcg@6': 0.6537, 'group_auc': 0.8095}\n"]}],"source":["res_syn = model.run_eval(test_file, num_ngs=test_num_ngs)\n","print(res_syn)"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"executionInfo":{"elapsed":314,"status":"ok","timestamp":1649950725237,"user":{"displayName":"조예은","userId":"06018350812551678054"},"user_tz":-540},"id":"VB7MBFNkjl5e","outputId":"6b8716b5-6a53-4c94-dd34-1f40f609e2a0"},"outputs":[{"data":{"application/scrapbook.scrap.json+json":{"data":{"auc":0.8121,"group_auc":0.8095,"logloss":0.8038,"mean_mrr":0.5606,"ndcg@2":0.4812,"ndcg@4":0.6054,"ndcg@6":0.6537},"encoder":"json","name":"res_syn","version":1}},"metadata":{"scrapbook":{"data":true,"display":false,"name":"res_syn"}},"output_type":"display_data"}],"source":["sb.glue(\"res_syn\", res_syn)"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":4777,"status":"ok","timestamp":1649950739673,"user":{"displayName":"조예은","userId":"06018350812551678054"},"user_tz":-540},"id":"h2uxYGfFjpL9"},"outputs":[],"source":["model = model.predict(test_file, output_file)"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1983,"status":"ok","timestamp":1649950829170,"user":{"displayName":"조예은","userId":"06018350812551678054"},"user_tz":-540},"id":"wGZ26pt-ujeo","outputId":"612311bb-64ec-4bee-86ad-1b325a4fad92"},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\조예은\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\recommenders\\models\\deeprec\\models\\base_model.py:701: UserWarning: `tf.layers.batch_normalization` is deprecated and will be removed in a future version. Please use `tf.keras.layers.BatchNormalization` instead. In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.BatchNormalization` documentation).\n","  curr_hidden_nn_layer = tf.compat.v1.layers.batch_normalization(\n","C:\\Users\\조예은\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\legacy_tf_layers\\normalization.py:463: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n","  return layer.apply(inputs, training=training)\n"]},{"name":"stdout","output_type":"stream","text":["loading saved model in resources\\model\\a2svd/best_model\n"]}],"source":["model_best_trained = SeqModel(hparams, input_creator, seed=RANDOM_SEED)\n","path_best_trained = os.path.join(hparams.MODEL_DIR, \"best_model\")\n","print('loading saved model in {0}'.format(path_best_trained))\n","model_best_trained.load_model(path_best_trained)"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":30944,"status":"ok","timestamp":1649950883835,"user":{"displayName":"조예은","userId":"06018350812551678054"},"user_tz":-540},"id":"fKtBHxlGu5-K","outputId":"d06c44fd-7737-41cb-944b-d0e50a27bb7b"},"outputs":[{"data":{"text/plain":["{'auc': 0.8121,\n"," 'logloss': 0.8038,\n"," 'mean_mrr': 0.5606,\n"," 'ndcg@2': 0.4812,\n"," 'ndcg@4': 0.6054,\n"," 'ndcg@6': 0.6537,\n"," 'group_auc': 0.8095}"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["model_best_trained.run_eval(test_file, num_ngs=test_num_ngs)"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4464,"status":"ok","timestamp":1649950901128,"user":{"displayName":"조예은","userId":"06018350812551678054"},"user_tz":-540},"id":"XH7L5VOQvARx","outputId":"a7b4e142-ba8b-49ab-b5ae-8362dc0118fa"},"outputs":[{"data":{"text/plain":["<recommenders.models.deeprec.models.sequential.asvd.A2SVDModel at 0x2bc932d8fa0>"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["model_best_trained.predict(test_file, output_file)"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyNgc+iagyrVe0GsNEd+o6Qh","collapsed_sections":[],"mount_file_id":"1PuuQmOQ1j7N7VGZU0cASgl1CYkF5Mi3f","name":"A2SVD.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.9"}},"nbformat":4,"nbformat_minor":0}
