{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sknnn2xs-AHi",
        "outputId": "a4f15d3b-23ff-42f1-8d49-ded97e42bf52"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "System version: 3.8.9 (tags/v3.8.9:a743f81, Apr  6 2021, 14:02:34) [MSC v.1928 64 bit (AMD64)]\n",
            "Tensorflow version: 2.8.0\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "import logging\n",
        "import papermill as pm\n",
        "import scrapbook as sb\n",
        "from tempfile import TemporaryDirectory\n",
        "import numpy as np\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.get_logger().setLevel('ERROR') # only show error messages\n",
        "\n",
        "from recommenders.utils.timer import Timer\n",
        "from recommenders.utils.constants import SEED\n",
        "from recommenders.models.deeprec.deeprec_utils import (\n",
        "    prepare_hparams\n",
        ")\n",
        "\n",
        "from resources.data_preprocessing import data_preprocessing\n",
        "# from recommenders.datasets.amazon_reviews import download_and_extract, data_preprocessing\n",
        "from recommenders.datasets.download_utils import maybe_download\n",
        "\n",
        "\n",
        "# from recommenders.models.deeprec.models.sequential.sli_rec import SLI_RECModel as SeqModel\n",
        "####  to use the other model, use one of the following lines:\n",
        "#from recommenders.models.deeprec.models.sequential.asvd import A2SVDModel as SeqModel\n",
        "# from recommenders.models.deeprec.models.sequential.caser import CaserModel as SeqModel\n",
        "# from recommenders.models.deeprec.models.sequential.gru4rec import GRU4RecModel as SeqModel\n",
        "from recommenders.models.deeprec.models.sequential.sum import SUMModel as SeqModel\n",
        "\n",
        "#from recommenders.models.deeprec.models.sequential.nextitnet import NextItNetModel\n",
        "\n",
        "from recommenders.models.deeprec.io.sequential_iterator import SequentialIterator\n",
        "#from recommenders.models.deeprec.io.nextitnet_iterator import NextItNetIterator\n",
        "\n",
        "print(\"System version: {}\".format(sys.version))\n",
        "print(\"Tensorflow version: {}\".format(tf.__version__))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "sxtfUZfP-ZWR"
      },
      "outputs": [],
      "source": [
        "##  ATTENTION: change to the corresponding config file, e.g., caser.yaml for CaserModel, sum.yaml for SUMModel\n",
        "# yaml_file = '../../recommenders/models/deeprec/config/sli_rec.yaml'  \n",
        "yaml_file = './sum.yaml'  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "_NnzhN4h_rI5"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 10\n",
        "BATCH_SIZE = 400\n",
        "RANDOM_SEED = SEED  # Set None for non-deterministic result\n",
        "\n",
        "data_path = os.path.join(\"resources/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "0RAMNcI3AClc"
      },
      "outputs": [],
      "source": [
        "# for test\n",
        "train_file = os.path.join(data_path, r'train_data')\n",
        "valid_file = os.path.join(data_path, r'valid_data')\n",
        "test_file = os.path.join(data_path, r'test_data')\n",
        "user_vocab = os.path.join(data_path, r'user_vocab.pkl')\n",
        "item_vocab = os.path.join(data_path, r'item_vocab.pkl')\n",
        "cate_vocab = os.path.join(data_path, r'cate_vocab.pkl')\n",
        "output_file = os.path.join(data_path, r'output_sum.txt')\n",
        "\n",
        "# reviews_name = 'json'\n",
        "# meta_name = 'json'\n",
        "# reviews_file = os.path.join(data_path, reviews_name)\n",
        "# meta_file = os.path.join(data_path, meta_name)\n",
        "train_num_ngs = 4 # number of negative instances with a positive instance for training\n",
        "valid_num_ngs = 4 # number of negative instances with a positive instance for validation\n",
        "test_num_ngs = 9 # number of negative instances with a positive instance for testing\n",
        "sample_rate = 0.01 # sample a small item set for training and testing here for fast example\n",
        "\n",
        "input_files = [data_path, train_file, valid_file, test_file, user_vocab, item_vocab, cate_vocab]\n",
        "\n",
        "if not os.path.exists(train_file):\n",
        "    # download_and_extract(reviews_name, reviews_file)\n",
        "    # download_and_extract(meta_name, meta_file)\n",
        "    data_preprocessing(*input_files, sample_rate=sample_rate, valid_num_ngs=valid_num_ngs, test_num_ngs=test_num_ngs)\n",
        "    #### uncomment this for the NextItNet model, because it does not need to unfold the user history\n",
        "    # data_preprocessing(*input_files, sample_rate=sample_rate, valid_num_ngs=valid_num_ngs, test_num_ngs=test_num_ngs, is_history_expanding=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "rwTMO3WEWaCu"
      },
      "outputs": [],
      "source": [
        "### NOTE:  \n",
        "### remember to use `_create_vocab(train_file, user_vocab, item_vocab, cate_vocab)` to generate the user_vocab, item_vocab and cate_vocab files, if you are using your own dataset rather than using our demo Amazon dataset.\n",
        "hparams = prepare_hparams(yaml_file, \n",
        "                          embed_l2=0., \n",
        "                          layer_l2=0., \n",
        "                          learning_rate=0.001,  # set to 0.01 if batch normalization is disable\n",
        "                          epochs=EPOCHS,\n",
        "                          batch_size=BATCH_SIZE,\n",
        "                          show_step=20,\n",
        "                          MODEL_DIR=os.path.join(data_path, \"model\", \"sum/\"),\n",
        "                          SUMMARIES_DIR=os.path.join(data_path, \"summary\", \"sum/\"),\n",
        "                          user_vocab=user_vocab,\n",
        "                          item_vocab=item_vocab,\n",
        "                          cate_vocab=cate_vocab,\n",
        "                          need_sample=True,\n",
        "                          train_num_ngs=train_num_ngs, # provides the number of negative instances for each positive instance for loss computation.\n",
        "            )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "4Rvq7gFluVPq"
      },
      "outputs": [],
      "source": [
        "input_creator = SequentialIterator\n",
        "#### uncomment this for the NextItNet model, because it needs a special data iterator for training\n",
        "#input_creator = NextItNetIterator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VFNfgcKNuaVv",
        "outputId": "2fa083f0-f73d-452a-efef-12151b40c7c5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\조예은\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\recommenders\\models\\deeprec\\models\\sequential\\sum_cells.py:78: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  self._erase_W = self.add_variable(\n",
            "C:\\Users\\조예은\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\recommenders\\models\\deeprec\\models\\sequential\\sum_cells.py:81: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  self._erase_b = self.add_variable(\n",
            "C:\\Users\\조예은\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\recommenders\\models\\deeprec\\models\\sequential\\sum_cells.py:91: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  self._reset_W = self.add_variable(\n",
            "C:\\Users\\조예은\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\recommenders\\models\\deeprec\\models\\sequential\\sum_cells.py:94: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  self._reset_b = self.add_variable(\n",
            "C:\\Users\\조예은\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\recommenders\\models\\deeprec\\models\\sequential\\sum_cells.py:104: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  self._add_W = self.add_variable(\n",
            "C:\\Users\\조예은\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\recommenders\\models\\deeprec\\models\\sequential\\sum_cells.py:107: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  self._add_b = self.add_variable(\n",
            "C:\\Users\\조예은\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\recommenders\\models\\deeprec\\models\\sequential\\sum_cells.py:116: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  self.heads = self.add_variable(\n",
            "C:\\Users\\조예은\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\recommenders\\models\\deeprec\\models\\sequential\\sum_cells.py:120: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  self._beta = self.add_variable(\n",
            "C:\\Users\\조예은\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\recommenders\\models\\deeprec\\models\\sequential\\sum_cells.py:127: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  self._alpha = self.add_variable(\n",
            "C:\\Users\\조예은\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\recommenders\\models\\deeprec\\models\\base_model.py:701: UserWarning: `tf.layers.batch_normalization` is deprecated and will be removed in a future version. Please use `tf.keras.layers.BatchNormalization` instead. In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.BatchNormalization` documentation).\n",
            "  curr_hidden_nn_layer = tf.compat.v1.layers.batch_normalization(\n",
            "C:\\Users\\조예은\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\legacy_tf_layers\\normalization.py:463: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
            "  return layer.apply(inputs, training=training)\n"
          ]
        }
      ],
      "source": [
        "model = SeqModel(hparams, input_creator, seed=RANDOM_SEED)\n",
        "\n",
        "## sometimes we don't want to train a model from scratch\n",
        "## then we can load a pre-trained model like this: \n",
        "#model.load_model(r'your_model_path')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8AQxPlk-uheU",
        "outputId": "55a5328a-44bd-4213-b10c-96f26313586a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'auc': 0.5123, 'logloss': 0.6932, 'mean_mrr': 0.2811, 'ndcg@2': 0.1449, 'ndcg@4': 0.2484, 'ndcg@6': 0.3269, 'group_auc': 0.5127}\n"
          ]
        }
      ],
      "source": [
        "# test_num_ngs is the number of negative lines after each positive line in your test_file\n",
        "print(model.run_eval(test_file, num_ngs=test_num_ngs)) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QFKQu9rGu0jq",
        "outputId": "c93b6ebf-50cc-43ea-e620-418e4537ad31"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 20 , total_loss: 1.4799, data_loss: 1.4799\n",
            "step 40 , total_loss: 1.3720, data_loss: 1.3720\n",
            "step 60 , total_loss: 1.2900, data_loss: 1.2900\n",
            "step 80 , total_loss: 1.3291, data_loss: 1.3291\n",
            "step 100 , total_loss: 1.2899, data_loss: 1.2899\n",
            "step 120 , total_loss: 1.2870, data_loss: 1.2870\n",
            "step 140 , total_loss: 1.2741, data_loss: 1.2741\n",
            "step 160 , total_loss: 1.3022, data_loss: 1.3022\n",
            "step 180 , total_loss: 1.2206, data_loss: 1.2206\n",
            "step 200 , total_loss: 1.2368, data_loss: 1.2368\n",
            "step 220 , total_loss: 1.2290, data_loss: 1.2290\n",
            "step 240 , total_loss: 1.2345, data_loss: 1.2345\n",
            "step 260 , total_loss: 1.2742, data_loss: 1.2742\n",
            "step 280 , total_loss: 1.1996, data_loss: 1.1996\n",
            "step 300 , total_loss: 1.2116, data_loss: 1.2116\n",
            "step 320 , total_loss: 1.2233, data_loss: 1.2233\n",
            "step 340 , total_loss: 1.1729, data_loss: 1.1729\n",
            "step 360 , total_loss: 1.2119, data_loss: 1.2119\n",
            "step 380 , total_loss: 1.2462, data_loss: 1.2462\n",
            "step 400 , total_loss: 1.2387, data_loss: 1.2387\n",
            "step 420 , total_loss: 1.1567, data_loss: 1.1567\n",
            "step 440 , total_loss: 1.1821, data_loss: 1.1821\n",
            "step 460 , total_loss: 1.2806, data_loss: 1.2806\n",
            "step 480 , total_loss: 1.1401, data_loss: 1.1401\n",
            "step 500 , total_loss: 1.1463, data_loss: 1.1463\n",
            "step 520 , total_loss: 1.1119, data_loss: 1.1119\n",
            "step 540 , total_loss: 1.1229, data_loss: 1.1229\n",
            "step 560 , total_loss: 1.1404, data_loss: 1.1404\n",
            "step 580 , total_loss: 1.1481, data_loss: 1.1481\n",
            "step 600 , total_loss: 1.1774, data_loss: 1.1774\n",
            "step 620 , total_loss: 1.1670, data_loss: 1.1670\n",
            "step 640 , total_loss: 1.1806, data_loss: 1.1806\n",
            "step 660 , total_loss: 1.1042, data_loss: 1.1042\n",
            "step 680 , total_loss: 1.1694, data_loss: 1.1694\n",
            "step 700 , total_loss: 1.1132, data_loss: 1.1132\n",
            "step 720 , total_loss: 1.1425, data_loss: 1.1425\n",
            "step 740 , total_loss: 1.1069, data_loss: 1.1069\n",
            "step 760 , total_loss: 1.1039, data_loss: 1.1039\n",
            "eval valid at epoch 1: auc:0.7435,logloss:0.616,mean_mrr:0.6639,ndcg@2:0.6162,ndcg@4:0.7345,ndcg@6:0.7484,group_auc:0.7499\n",
            "step 20 , total_loss: 1.0488, data_loss: 1.0488\n",
            "step 40 , total_loss: 1.1032, data_loss: 1.1032\n",
            "step 60 , total_loss: 1.1946, data_loss: 1.1946\n",
            "step 80 , total_loss: 1.1142, data_loss: 1.1142\n",
            "step 100 , total_loss: 1.0862, data_loss: 1.0862\n",
            "step 120 , total_loss: 1.1208, data_loss: 1.1208\n",
            "step 140 , total_loss: 1.0634, data_loss: 1.0634\n",
            "step 160 , total_loss: 1.1940, data_loss: 1.1940\n",
            "step 180 , total_loss: 1.0175, data_loss: 1.0175\n",
            "step 200 , total_loss: 1.0884, data_loss: 1.0884\n",
            "step 220 , total_loss: 1.1026, data_loss: 1.1026\n",
            "step 240 , total_loss: 1.0338, data_loss: 1.0338\n",
            "step 260 , total_loss: 1.0699, data_loss: 1.0699\n",
            "step 280 , total_loss: 1.0768, data_loss: 1.0768\n",
            "step 300 , total_loss: 1.0790, data_loss: 1.0790\n",
            "step 320 , total_loss: 0.9763, data_loss: 0.9763\n",
            "step 340 , total_loss: 1.1005, data_loss: 1.1005\n",
            "step 360 , total_loss: 1.0632, data_loss: 1.0632\n",
            "step 380 , total_loss: 1.1418, data_loss: 1.1418\n",
            "step 400 , total_loss: 1.0100, data_loss: 1.0100\n",
            "step 420 , total_loss: 1.1592, data_loss: 1.1592\n",
            "step 440 , total_loss: 1.0679, data_loss: 1.0679\n",
            "step 460 , total_loss: 0.9938, data_loss: 0.9938\n",
            "step 480 , total_loss: 1.0769, data_loss: 1.0769\n",
            "step 500 , total_loss: 1.0835, data_loss: 1.0835\n",
            "step 520 , total_loss: 1.0868, data_loss: 1.0868\n",
            "step 540 , total_loss: 1.0184, data_loss: 1.0184\n",
            "step 560 , total_loss: 1.0194, data_loss: 1.0194\n",
            "step 580 , total_loss: 1.0317, data_loss: 1.0317\n",
            "step 600 , total_loss: 1.1000, data_loss: 1.1000\n",
            "step 620 , total_loss: 1.0508, data_loss: 1.0508\n",
            "step 640 , total_loss: 1.0247, data_loss: 1.0247\n",
            "step 660 , total_loss: 0.9697, data_loss: 0.9697\n",
            "step 680 , total_loss: 1.0288, data_loss: 1.0288\n",
            "step 700 , total_loss: 0.9613, data_loss: 0.9613\n",
            "step 720 , total_loss: 1.0988, data_loss: 1.0988\n",
            "step 740 , total_loss: 1.0713, data_loss: 1.0713\n",
            "step 760 , total_loss: 1.0436, data_loss: 1.0436\n",
            "eval valid at epoch 2: auc:0.7433,logloss:0.831,mean_mrr:0.6825,ndcg@2:0.6385,ndcg@4:0.7527,ndcg@6:0.7625,group_auc:0.7694\n",
            "step 20 , total_loss: 0.9727, data_loss: 0.9727\n",
            "step 40 , total_loss: 1.0432, data_loss: 1.0432\n",
            "step 60 , total_loss: 1.0163, data_loss: 1.0163\n",
            "step 80 , total_loss: 1.0543, data_loss: 1.0543\n",
            "step 100 , total_loss: 0.9843, data_loss: 0.9843\n",
            "step 120 , total_loss: 0.9613, data_loss: 0.9613\n",
            "step 140 , total_loss: 1.0046, data_loss: 1.0046\n",
            "step 160 , total_loss: 1.0141, data_loss: 1.0141\n",
            "step 180 , total_loss: 1.0110, data_loss: 1.0110\n",
            "step 200 , total_loss: 1.0778, data_loss: 1.0778\n",
            "step 220 , total_loss: 1.0693, data_loss: 1.0693\n",
            "step 240 , total_loss: 1.0015, data_loss: 1.0015\n",
            "step 260 , total_loss: 0.9672, data_loss: 0.9672\n",
            "step 280 , total_loss: 1.0331, data_loss: 1.0331\n",
            "step 300 , total_loss: 0.9562, data_loss: 0.9562\n",
            "step 320 , total_loss: 0.9512, data_loss: 0.9512\n",
            "step 340 , total_loss: 1.0546, data_loss: 1.0546\n",
            "step 360 , total_loss: 0.9553, data_loss: 0.9553\n",
            "step 380 , total_loss: 0.9795, data_loss: 0.9795\n",
            "step 400 , total_loss: 1.0083, data_loss: 1.0083\n",
            "step 420 , total_loss: 1.0504, data_loss: 1.0504\n",
            "step 440 , total_loss: 1.0538, data_loss: 1.0538\n",
            "step 460 , total_loss: 1.0035, data_loss: 1.0035\n",
            "step 480 , total_loss: 1.0143, data_loss: 1.0143\n",
            "step 500 , total_loss: 0.9922, data_loss: 0.9922\n",
            "step 520 , total_loss: 0.9807, data_loss: 0.9807\n",
            "step 540 , total_loss: 1.0444, data_loss: 1.0444\n",
            "step 560 , total_loss: 0.9907, data_loss: 0.9907\n",
            "step 580 , total_loss: 1.0033, data_loss: 1.0033\n",
            "step 600 , total_loss: 1.0261, data_loss: 1.0261\n",
            "step 620 , total_loss: 0.9754, data_loss: 0.9754\n",
            "step 640 , total_loss: 0.9499, data_loss: 0.9499\n",
            "step 660 , total_loss: 1.0074, data_loss: 1.0074\n",
            "step 680 , total_loss: 1.0428, data_loss: 1.0428\n",
            "step 700 , total_loss: 1.0009, data_loss: 1.0009\n",
            "step 720 , total_loss: 0.9777, data_loss: 0.9777\n",
            "step 740 , total_loss: 0.9548, data_loss: 0.9548\n",
            "step 760 , total_loss: 0.9717, data_loss: 0.9717\n",
            "eval valid at epoch 3: auc:0.7458,logloss:0.4998,mean_mrr:0.694,ndcg@2:0.6557,ndcg@4:0.7624,ndcg@6:0.7713,group_auc:0.7814\n",
            "step 20 , total_loss: 0.9083, data_loss: 0.9083\n",
            "step 40 , total_loss: 0.9907, data_loss: 0.9907\n",
            "step 60 , total_loss: 0.9533, data_loss: 0.9533\n",
            "step 80 , total_loss: 1.0034, data_loss: 1.0034\n",
            "step 100 , total_loss: 0.9955, data_loss: 0.9955\n",
            "step 120 , total_loss: 1.0178, data_loss: 1.0178\n",
            "step 140 , total_loss: 1.0045, data_loss: 1.0045\n",
            "step 160 , total_loss: 1.0411, data_loss: 1.0411\n",
            "step 180 , total_loss: 0.9464, data_loss: 0.9464\n",
            "step 200 , total_loss: 0.9138, data_loss: 0.9138\n",
            "step 220 , total_loss: 1.0134, data_loss: 1.0134\n",
            "step 240 , total_loss: 0.9436, data_loss: 0.9436\n",
            "step 260 , total_loss: 0.9485, data_loss: 0.9485\n",
            "step 280 , total_loss: 0.9530, data_loss: 0.9530\n",
            "step 300 , total_loss: 0.9151, data_loss: 0.9151\n",
            "step 320 , total_loss: 0.9700, data_loss: 0.9700\n",
            "step 340 , total_loss: 1.0139, data_loss: 1.0139\n",
            "step 360 , total_loss: 0.9888, data_loss: 0.9888\n",
            "step 380 , total_loss: 0.9453, data_loss: 0.9453\n",
            "step 400 , total_loss: 1.0266, data_loss: 1.0266\n",
            "step 420 , total_loss: 0.9617, data_loss: 0.9617\n",
            "step 440 , total_loss: 0.9381, data_loss: 0.9381\n",
            "step 460 , total_loss: 0.9780, data_loss: 0.9780\n",
            "step 480 , total_loss: 0.8836, data_loss: 0.8836\n",
            "step 500 , total_loss: 0.9310, data_loss: 0.9310\n",
            "step 520 , total_loss: 0.9316, data_loss: 0.9316\n",
            "step 540 , total_loss: 0.9880, data_loss: 0.9880\n",
            "step 560 , total_loss: 0.9670, data_loss: 0.9670\n",
            "step 580 , total_loss: 0.8549, data_loss: 0.8549\n",
            "step 600 , total_loss: 0.9362, data_loss: 0.9362\n",
            "step 620 , total_loss: 0.9428, data_loss: 0.9428\n",
            "step 640 , total_loss: 0.9010, data_loss: 0.9010\n",
            "step 660 , total_loss: 0.9436, data_loss: 0.9436\n",
            "step 680 , total_loss: 0.9389, data_loss: 0.9389\n",
            "step 700 , total_loss: 0.9829, data_loss: 0.9829\n",
            "step 720 , total_loss: 0.9031, data_loss: 0.9031\n",
            "step 740 , total_loss: 0.9665, data_loss: 0.9665\n",
            "step 760 , total_loss: 0.9511, data_loss: 0.9511\n",
            "eval valid at epoch 4: auc:0.7684,logloss:0.6132,mean_mrr:0.7163,ndcg@2:0.6849,ndcg@4:0.7821,ndcg@6:0.7881,group_auc:0.8023\n",
            "step 20 , total_loss: 0.9369, data_loss: 0.9369\n",
            "step 40 , total_loss: 0.9367, data_loss: 0.9367\n",
            "step 60 , total_loss: 0.9194, data_loss: 0.9194\n",
            "step 80 , total_loss: 1.0499, data_loss: 1.0499\n",
            "step 100 , total_loss: 0.8897, data_loss: 0.8897\n",
            "step 120 , total_loss: 0.9177, data_loss: 0.9177\n",
            "step 140 , total_loss: 0.9109, data_loss: 0.9109\n",
            "step 160 , total_loss: 0.9096, data_loss: 0.9096\n",
            "step 180 , total_loss: 0.9331, data_loss: 0.9331\n",
            "step 200 , total_loss: 0.9544, data_loss: 0.9544\n",
            "step 220 , total_loss: 0.9409, data_loss: 0.9409\n",
            "step 240 , total_loss: 0.9326, data_loss: 0.9326\n",
            "step 260 , total_loss: 1.0229, data_loss: 1.0229\n",
            "step 280 , total_loss: 0.9681, data_loss: 0.9681\n",
            "step 300 , total_loss: 0.8787, data_loss: 0.8787\n",
            "step 320 , total_loss: 0.8961, data_loss: 0.8961\n",
            "step 340 , total_loss: 1.0610, data_loss: 1.0610\n",
            "step 360 , total_loss: 0.8690, data_loss: 0.8690\n",
            "step 380 , total_loss: 0.8915, data_loss: 0.8915\n",
            "step 400 , total_loss: 0.8675, data_loss: 0.8675\n",
            "step 420 , total_loss: 0.9425, data_loss: 0.9425\n",
            "step 440 , total_loss: 0.9063, data_loss: 0.9063\n",
            "step 460 , total_loss: 0.9143, data_loss: 0.9143\n",
            "step 480 , total_loss: 0.9233, data_loss: 0.9233\n",
            "step 500 , total_loss: 0.9162, data_loss: 0.9162\n",
            "step 520 , total_loss: 0.9723, data_loss: 0.9723\n",
            "step 540 , total_loss: 0.9774, data_loss: 0.9774\n",
            "step 560 , total_loss: 0.8574, data_loss: 0.8574\n",
            "step 580 , total_loss: 0.9486, data_loss: 0.9486\n",
            "step 600 , total_loss: 0.9987, data_loss: 0.9987\n",
            "step 620 , total_loss: 0.8899, data_loss: 0.8899\n",
            "step 640 , total_loss: 0.8909, data_loss: 0.8909\n",
            "step 660 , total_loss: 0.9839, data_loss: 0.9839\n",
            "step 680 , total_loss: 0.9664, data_loss: 0.9664\n",
            "step 700 , total_loss: 0.9584, data_loss: 0.9584\n",
            "step 720 , total_loss: 0.9217, data_loss: 0.9217\n",
            "step 740 , total_loss: 0.9880, data_loss: 0.9880\n",
            "step 760 , total_loss: 0.9233, data_loss: 0.9233\n",
            "eval valid at epoch 5: auc:0.7907,logloss:0.6108,mean_mrr:0.7228,ndcg@2:0.6941,ndcg@4:0.7877,ndcg@6:0.793,group_auc:0.8076\n",
            "step 20 , total_loss: 0.9278, data_loss: 0.9278\n",
            "step 40 , total_loss: 0.9301, data_loss: 0.9301\n",
            "step 60 , total_loss: 0.9500, data_loss: 0.9500\n",
            "step 80 , total_loss: 0.9059, data_loss: 0.9059\n",
            "step 100 , total_loss: 0.8867, data_loss: 0.8867\n",
            "step 120 , total_loss: 0.9861, data_loss: 0.9861\n",
            "step 140 , total_loss: 0.9637, data_loss: 0.9637\n",
            "step 160 , total_loss: 0.8998, data_loss: 0.8998\n",
            "step 180 , total_loss: 0.8450, data_loss: 0.8450\n",
            "step 200 , total_loss: 0.9108, data_loss: 0.9108\n",
            "step 220 , total_loss: 0.8868, data_loss: 0.8868\n",
            "step 240 , total_loss: 0.9897, data_loss: 0.9897\n",
            "step 260 , total_loss: 0.9031, data_loss: 0.9031\n",
            "step 280 , total_loss: 0.9450, data_loss: 0.9450\n",
            "step 300 , total_loss: 0.9510, data_loss: 0.9510\n",
            "step 320 , total_loss: 0.8923, data_loss: 0.8923\n",
            "step 340 , total_loss: 0.9405, data_loss: 0.9405\n",
            "step 360 , total_loss: 0.9177, data_loss: 0.9177\n",
            "step 380 , total_loss: 0.9199, data_loss: 0.9199\n",
            "step 400 , total_loss: 0.9005, data_loss: 0.9005\n",
            "step 420 , total_loss: 0.9146, data_loss: 0.9146\n",
            "step 440 , total_loss: 0.8711, data_loss: 0.8711\n",
            "step 460 , total_loss: 0.8613, data_loss: 0.8613\n",
            "step 480 , total_loss: 0.9741, data_loss: 0.9741\n",
            "step 500 , total_loss: 0.8559, data_loss: 0.8559\n",
            "step 520 , total_loss: 0.8669, data_loss: 0.8669\n",
            "step 540 , total_loss: 0.9465, data_loss: 0.9465\n",
            "step 560 , total_loss: 0.8694, data_loss: 0.8694\n",
            "step 580 , total_loss: 0.8070, data_loss: 0.8070\n",
            "step 600 , total_loss: 0.8689, data_loss: 0.8689\n",
            "step 620 , total_loss: 0.9423, data_loss: 0.9423\n",
            "step 640 , total_loss: 0.8908, data_loss: 0.8908\n",
            "step 660 , total_loss: 0.8995, data_loss: 0.8995\n",
            "step 680 , total_loss: 0.8960, data_loss: 0.8960\n",
            "step 700 , total_loss: 0.9138, data_loss: 0.9138\n",
            "step 720 , total_loss: 0.8882, data_loss: 0.8882\n",
            "step 740 , total_loss: 0.9254, data_loss: 0.9254\n",
            "step 760 , total_loss: 0.8590, data_loss: 0.8590\n",
            "eval valid at epoch 6: auc:0.7955,logloss:0.5273,mean_mrr:0.7298,ndcg@2:0.7024,ndcg@4:0.7933,ndcg@6:0.7983,group_auc:0.8136\n",
            "step 20 , total_loss: 0.8852, data_loss: 0.8852\n",
            "step 40 , total_loss: 0.9444, data_loss: 0.9444\n",
            "step 60 , total_loss: 0.9276, data_loss: 0.9276\n",
            "step 80 , total_loss: 0.8875, data_loss: 0.8875\n",
            "step 100 , total_loss: 0.8710, data_loss: 0.8710\n",
            "step 120 , total_loss: 0.8859, data_loss: 0.8859\n",
            "step 140 , total_loss: 0.9448, data_loss: 0.9448\n",
            "step 160 , total_loss: 0.8547, data_loss: 0.8547\n",
            "step 180 , total_loss: 0.8688, data_loss: 0.8688\n",
            "step 200 , total_loss: 0.8482, data_loss: 0.8482\n",
            "step 220 , total_loss: 0.9506, data_loss: 0.9506\n",
            "step 240 , total_loss: 0.9329, data_loss: 0.9329\n",
            "step 260 , total_loss: 0.8264, data_loss: 0.8264\n",
            "step 280 , total_loss: 0.8904, data_loss: 0.8904\n",
            "step 300 , total_loss: 0.9498, data_loss: 0.9498\n",
            "step 320 , total_loss: 0.8612, data_loss: 0.8612\n",
            "step 340 , total_loss: 0.8531, data_loss: 0.8531\n",
            "step 360 , total_loss: 0.7566, data_loss: 0.7566\n",
            "step 380 , total_loss: 0.8843, data_loss: 0.8843\n",
            "step 400 , total_loss: 0.9700, data_loss: 0.9700\n",
            "step 420 , total_loss: 0.8972, data_loss: 0.8972\n",
            "step 440 , total_loss: 0.7964, data_loss: 0.7964\n",
            "step 460 , total_loss: 0.9273, data_loss: 0.9273\n",
            "step 480 , total_loss: 0.9147, data_loss: 0.9147\n",
            "step 500 , total_loss: 0.8863, data_loss: 0.8863\n",
            "step 520 , total_loss: 0.9605, data_loss: 0.9605\n",
            "step 540 , total_loss: 0.8178, data_loss: 0.8178\n",
            "step 560 , total_loss: 0.9498, data_loss: 0.9498\n",
            "step 580 , total_loss: 0.9312, data_loss: 0.9312\n",
            "step 600 , total_loss: 0.8367, data_loss: 0.8367\n",
            "step 620 , total_loss: 0.8438, data_loss: 0.8438\n",
            "step 640 , total_loss: 0.8809, data_loss: 0.8809\n",
            "step 660 , total_loss: 0.9153, data_loss: 0.9153\n",
            "step 680 , total_loss: 0.8747, data_loss: 0.8747\n",
            "step 700 , total_loss: 0.9487, data_loss: 0.9487\n",
            "step 720 , total_loss: 0.9433, data_loss: 0.9433\n",
            "step 740 , total_loss: 0.9709, data_loss: 0.9709\n",
            "step 760 , total_loss: 0.8583, data_loss: 0.8583\n",
            "eval valid at epoch 7: auc:0.8028,logloss:0.5421,mean_mrr:0.7412,ndcg@2:0.7163,ndcg@4:0.8031,ndcg@6:0.8069,group_auc:0.8237\n",
            "step 20 , total_loss: 0.9826, data_loss: 0.9826\n",
            "step 40 , total_loss: 0.8500, data_loss: 0.8500\n",
            "step 60 , total_loss: 0.7933, data_loss: 0.7933\n",
            "step 80 , total_loss: 0.8930, data_loss: 0.8930\n",
            "step 100 , total_loss: 0.8299, data_loss: 0.8299\n",
            "step 120 , total_loss: 0.8025, data_loss: 0.8025\n",
            "step 140 , total_loss: 0.8679, data_loss: 0.8679\n",
            "step 160 , total_loss: 0.8987, data_loss: 0.8987\n",
            "step 180 , total_loss: 0.9241, data_loss: 0.9241\n",
            "step 200 , total_loss: 0.9446, data_loss: 0.9446\n",
            "step 220 , total_loss: 0.8765, data_loss: 0.8765\n",
            "step 240 , total_loss: 0.8876, data_loss: 0.8876\n",
            "step 260 , total_loss: 0.8210, data_loss: 0.8210\n",
            "step 280 , total_loss: 0.9039, data_loss: 0.9039\n",
            "step 300 , total_loss: 0.8625, data_loss: 0.8625\n",
            "step 320 , total_loss: 0.8249, data_loss: 0.8249\n",
            "step 340 , total_loss: 0.9059, data_loss: 0.9059\n",
            "step 360 , total_loss: 0.8786, data_loss: 0.8786\n",
            "step 380 , total_loss: 0.8504, data_loss: 0.8504\n",
            "step 400 , total_loss: 0.8245, data_loss: 0.8245\n",
            "step 420 , total_loss: 0.8904, data_loss: 0.8904\n",
            "step 440 , total_loss: 0.8925, data_loss: 0.8925\n",
            "step 460 , total_loss: 0.8150, data_loss: 0.8150\n",
            "step 480 , total_loss: 0.8648, data_loss: 0.8648\n",
            "step 500 , total_loss: 0.8113, data_loss: 0.8113\n",
            "step 520 , total_loss: 0.8744, data_loss: 0.8744\n",
            "step 540 , total_loss: 0.9039, data_loss: 0.9039\n",
            "step 560 , total_loss: 0.8709, data_loss: 0.8709\n",
            "step 580 , total_loss: 0.8793, data_loss: 0.8793\n",
            "step 600 , total_loss: 0.8937, data_loss: 0.8937\n",
            "step 620 , total_loss: 0.8756, data_loss: 0.8756\n",
            "step 640 , total_loss: 0.8230, data_loss: 0.8230\n",
            "step 660 , total_loss: 0.9650, data_loss: 0.9650\n",
            "step 680 , total_loss: 0.8780, data_loss: 0.8780\n",
            "step 700 , total_loss: 0.8554, data_loss: 0.8554\n",
            "step 720 , total_loss: 0.9112, data_loss: 0.9112\n",
            "step 740 , total_loss: 0.8565, data_loss: 0.8565\n",
            "step 760 , total_loss: 0.8588, data_loss: 0.8588\n",
            "eval valid at epoch 8: auc:0.81,logloss:0.6974,mean_mrr:0.7407,ndcg@2:0.7169,ndcg@4:0.802,ndcg@6:0.8065,group_auc:0.8237\n",
            "step 20 , total_loss: 0.8196, data_loss: 0.8196\n",
            "step 40 , total_loss: 0.9010, data_loss: 0.9010\n",
            "step 60 , total_loss: 0.8950, data_loss: 0.8950\n",
            "step 80 , total_loss: 0.8244, data_loss: 0.8244\n",
            "step 100 , total_loss: 0.8498, data_loss: 0.8498\n",
            "step 120 , total_loss: 0.8196, data_loss: 0.8196\n",
            "step 140 , total_loss: 0.9049, data_loss: 0.9049\n",
            "step 160 , total_loss: 0.8992, data_loss: 0.8992\n",
            "step 180 , total_loss: 0.8307, data_loss: 0.8307\n",
            "step 200 , total_loss: 0.8709, data_loss: 0.8709\n",
            "step 220 , total_loss: 0.8841, data_loss: 0.8841\n",
            "step 240 , total_loss: 0.8873, data_loss: 0.8873\n",
            "step 260 , total_loss: 0.8434, data_loss: 0.8434\n",
            "step 280 , total_loss: 0.8201, data_loss: 0.8201\n",
            "step 300 , total_loss: 0.8465, data_loss: 0.8465\n",
            "step 320 , total_loss: 0.8127, data_loss: 0.8127\n",
            "step 340 , total_loss: 0.9123, data_loss: 0.9123\n",
            "step 360 , total_loss: 0.9223, data_loss: 0.9223\n",
            "step 380 , total_loss: 0.8192, data_loss: 0.8192\n",
            "step 400 , total_loss: 0.8725, data_loss: 0.8725\n",
            "step 420 , total_loss: 0.9438, data_loss: 0.9438\n",
            "step 440 , total_loss: 0.8414, data_loss: 0.8414\n",
            "step 460 , total_loss: 0.8873, data_loss: 0.8873\n",
            "step 480 , total_loss: 0.8944, data_loss: 0.8944\n",
            "step 500 , total_loss: 0.8280, data_loss: 0.8280\n",
            "step 520 , total_loss: 0.9257, data_loss: 0.9257\n",
            "step 540 , total_loss: 0.8778, data_loss: 0.8778\n",
            "step 560 , total_loss: 0.7783, data_loss: 0.7783\n",
            "step 580 , total_loss: 0.8826, data_loss: 0.8826\n",
            "step 600 , total_loss: 0.8329, data_loss: 0.8329\n",
            "step 620 , total_loss: 0.8401, data_loss: 0.8401\n",
            "step 640 , total_loss: 0.9313, data_loss: 0.9313\n",
            "step 660 , total_loss: 0.9357, data_loss: 0.9357\n",
            "step 680 , total_loss: 0.8629, data_loss: 0.8629\n",
            "step 700 , total_loss: 0.7929, data_loss: 0.7929\n",
            "step 720 , total_loss: 0.7792, data_loss: 0.7792\n",
            "step 740 , total_loss: 0.8986, data_loss: 0.8986\n",
            "step 760 , total_loss: 0.8838, data_loss: 0.8838\n",
            "eval valid at epoch 9: auc:0.8233,logloss:0.645,mean_mrr:0.752,ndcg@2:0.7314,ndcg@4:0.8108,ndcg@6:0.815,group_auc:0.8319\n",
            "step 20 , total_loss: 0.8775, data_loss: 0.8775\n",
            "step 40 , total_loss: 0.8024, data_loss: 0.8024\n",
            "step 60 , total_loss: 0.8669, data_loss: 0.8669\n",
            "step 80 , total_loss: 0.8036, data_loss: 0.8036\n",
            "step 100 , total_loss: 0.8684, data_loss: 0.8684\n",
            "step 120 , total_loss: 0.8555, data_loss: 0.8555\n",
            "step 140 , total_loss: 0.8563, data_loss: 0.8563\n",
            "step 160 , total_loss: 0.8103, data_loss: 0.8103\n",
            "step 180 , total_loss: 0.7840, data_loss: 0.7840\n",
            "step 200 , total_loss: 0.8576, data_loss: 0.8576\n",
            "step 220 , total_loss: 0.8430, data_loss: 0.8430\n",
            "step 240 , total_loss: 0.8511, data_loss: 0.8511\n",
            "step 260 , total_loss: 0.8492, data_loss: 0.8492\n",
            "step 280 , total_loss: 0.8555, data_loss: 0.8555\n",
            "step 300 , total_loss: 0.7832, data_loss: 0.7832\n",
            "step 320 , total_loss: 0.8997, data_loss: 0.8997\n",
            "step 340 , total_loss: 0.8943, data_loss: 0.8943\n",
            "step 360 , total_loss: 0.9486, data_loss: 0.9486\n",
            "step 380 , total_loss: 0.8352, data_loss: 0.8352\n",
            "step 400 , total_loss: 0.8498, data_loss: 0.8498\n",
            "step 420 , total_loss: 0.8383, data_loss: 0.8383\n",
            "step 440 , total_loss: 0.8498, data_loss: 0.8498\n",
            "step 460 , total_loss: 0.8486, data_loss: 0.8486\n",
            "step 480 , total_loss: 0.8615, data_loss: 0.8615\n",
            "step 500 , total_loss: 0.9135, data_loss: 0.9135\n",
            "step 520 , total_loss: 0.8414, data_loss: 0.8414\n",
            "step 540 , total_loss: 0.7989, data_loss: 0.7989\n",
            "step 560 , total_loss: 0.9302, data_loss: 0.9302\n",
            "step 580 , total_loss: 0.8033, data_loss: 0.8033\n",
            "step 600 , total_loss: 0.8412, data_loss: 0.8412\n",
            "step 620 , total_loss: 0.8522, data_loss: 0.8522\n",
            "step 640 , total_loss: 0.8343, data_loss: 0.8343\n",
            "step 660 , total_loss: 0.8630, data_loss: 0.8630\n",
            "step 680 , total_loss: 0.8016, data_loss: 0.8016\n",
            "step 700 , total_loss: 0.7947, data_loss: 0.7947\n",
            "step 720 , total_loss: 0.7723, data_loss: 0.7723\n",
            "step 740 , total_loss: 0.8388, data_loss: 0.8388\n",
            "step 760 , total_loss: 0.9591, data_loss: 0.9591\n",
            "eval valid at epoch 10: auc:0.828,logloss:0.5516,mean_mrr:0.7512,ndcg@2:0.7305,ndcg@4:0.8111,ndcg@6:0.8145,group_auc:0.8326\n",
            "[(1, {'auc': 0.7435, 'logloss': 0.616, 'mean_mrr': 0.6639, 'ndcg@2': 0.6162, 'ndcg@4': 0.7345, 'ndcg@6': 0.7484, 'group_auc': 0.7499}), (2, {'auc': 0.7433, 'logloss': 0.831, 'mean_mrr': 0.6825, 'ndcg@2': 0.6385, 'ndcg@4': 0.7527, 'ndcg@6': 0.7625, 'group_auc': 0.7694}), (3, {'auc': 0.7458, 'logloss': 0.4998, 'mean_mrr': 0.694, 'ndcg@2': 0.6557, 'ndcg@4': 0.7624, 'ndcg@6': 0.7713, 'group_auc': 0.7814}), (4, {'auc': 0.7684, 'logloss': 0.6132, 'mean_mrr': 0.7163, 'ndcg@2': 0.6849, 'ndcg@4': 0.7821, 'ndcg@6': 0.7881, 'group_auc': 0.8023}), (5, {'auc': 0.7907, 'logloss': 0.6108, 'mean_mrr': 0.7228, 'ndcg@2': 0.6941, 'ndcg@4': 0.7877, 'ndcg@6': 0.793, 'group_auc': 0.8076}), (6, {'auc': 0.7955, 'logloss': 0.5273, 'mean_mrr': 0.7298, 'ndcg@2': 0.7024, 'ndcg@4': 0.7933, 'ndcg@6': 0.7983, 'group_auc': 0.8136}), (7, {'auc': 0.8028, 'logloss': 0.5421, 'mean_mrr': 0.7412, 'ndcg@2': 0.7163, 'ndcg@4': 0.8031, 'ndcg@6': 0.8069, 'group_auc': 0.8237}), (8, {'auc': 0.81, 'logloss': 0.6974, 'mean_mrr': 0.7407, 'ndcg@2': 0.7169, 'ndcg@4': 0.802, 'ndcg@6': 0.8065, 'group_auc': 0.8237}), (9, {'auc': 0.8233, 'logloss': 0.645, 'mean_mrr': 0.752, 'ndcg@2': 0.7314, 'ndcg@4': 0.8108, 'ndcg@6': 0.815, 'group_auc': 0.8319}), (10, {'auc': 0.828, 'logloss': 0.5516, 'mean_mrr': 0.7512, 'ndcg@2': 0.7305, 'ndcg@4': 0.8111, 'ndcg@6': 0.8145, 'group_auc': 0.8326})]\n",
            "best epoch: 10\n",
            "Time cost for training is 259.17 mins\n"
          ]
        }
      ],
      "source": [
        "with Timer() as train_time:\n",
        "    model = model.fit(train_file, valid_file, valid_num_ngs=valid_num_ngs) \n",
        "\n",
        "# valid_num_ngs is the number of negative lines after each positive line in your valid_file \n",
        "# we will evaluate the performance of model on valid_file every epoch\n",
        "print('Time cost for training is {0:.2f} mins'.format(train_time.interval/60.0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gmm3DPjbiY9N",
        "outputId": "6dbb1a1d-8b36-49b1-ef5e-f4a807dddfde"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'auc': 0.8255, 'logloss': 0.5659, 'mean_mrr': 0.5999, 'ndcg@2': 0.5289, 'ndcg@4': 0.6456, 'ndcg@6': 0.6863, 'group_auc': 0.831}\n"
          ]
        }
      ],
      "source": [
        "res_syn = model.run_eval(test_file, num_ngs=test_num_ngs)\n",
        "print(res_syn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "VB7MBFNkjl5e",
        "outputId": "12b143c9-1d98-4d99-9136-8c779de2d832"
      },
      "outputs": [
        {
          "data": {
            "application/scrapbook.scrap.json+json": {
              "data": {
                "auc": 0.8255,
                "group_auc": 0.831,
                "logloss": 0.5659,
                "mean_mrr": 0.5999,
                "ndcg@2": 0.5289,
                "ndcg@4": 0.6456,
                "ndcg@6": 0.6863
              },
              "encoder": "json",
              "name": "res_syn",
              "version": 1
            }
          },
          "metadata": {
            "scrapbook": {
              "data": true,
              "display": false,
              "name": "res_syn"
            }
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "sb.glue(\"res_syn\", res_syn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "h2uxYGfFjpL9"
      },
      "outputs": [],
      "source": [
        "model = model.predict(test_file, output_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wGZ26pt-ujeo",
        "outputId": "5cbe4b07-34e3-4891-c64b-40b2a70a6fa7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\조예은\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\recommenders\\models\\deeprec\\models\\sequential\\sum_cells.py:78: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  self._erase_W = self.add_variable(\n",
            "C:\\Users\\조예은\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\recommenders\\models\\deeprec\\models\\sequential\\sum_cells.py:81: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  self._erase_b = self.add_variable(\n",
            "C:\\Users\\조예은\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\recommenders\\models\\deeprec\\models\\sequential\\sum_cells.py:91: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  self._reset_W = self.add_variable(\n",
            "C:\\Users\\조예은\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\recommenders\\models\\deeprec\\models\\sequential\\sum_cells.py:94: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  self._reset_b = self.add_variable(\n",
            "C:\\Users\\조예은\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\recommenders\\models\\deeprec\\models\\sequential\\sum_cells.py:104: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  self._add_W = self.add_variable(\n",
            "C:\\Users\\조예은\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\recommenders\\models\\deeprec\\models\\sequential\\sum_cells.py:107: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  self._add_b = self.add_variable(\n",
            "C:\\Users\\조예은\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\recommenders\\models\\deeprec\\models\\sequential\\sum_cells.py:116: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  self.heads = self.add_variable(\n",
            "C:\\Users\\조예은\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\recommenders\\models\\deeprec\\models\\sequential\\sum_cells.py:120: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  self._beta = self.add_variable(\n",
            "C:\\Users\\조예은\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\recommenders\\models\\deeprec\\models\\sequential\\sum_cells.py:127: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  self._alpha = self.add_variable(\n",
            "C:\\Users\\조예은\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\recommenders\\models\\deeprec\\models\\base_model.py:701: UserWarning: `tf.layers.batch_normalization` is deprecated and will be removed in a future version. Please use `tf.keras.layers.BatchNormalization` instead. In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.BatchNormalization` documentation).\n",
            "  curr_hidden_nn_layer = tf.compat.v1.layers.batch_normalization(\n",
            "C:\\Users\\조예은\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\legacy_tf_layers\\normalization.py:463: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
            "  return layer.apply(inputs, training=training)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loading saved model in resources/model\\sum/best_model\n"
          ]
        }
      ],
      "source": [
        "model_best_trained = SeqModel(hparams, input_creator, seed=RANDOM_SEED)\n",
        "path_best_trained = os.path.join(hparams.MODEL_DIR, \"best_model\")\n",
        "print('loading saved model in {0}'.format(path_best_trained))\n",
        "model_best_trained.load_model(path_best_trained)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fKtBHxlGu5-K",
        "outputId": "d398f3b9-bbba-4707-fae1-7c1c817a7e66"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'auc': 0.8255,\n",
              " 'logloss': 0.5659,\n",
              " 'mean_mrr': 0.5999,\n",
              " 'ndcg@2': 0.5289,\n",
              " 'ndcg@4': 0.6456,\n",
              " 'ndcg@6': 0.6863,\n",
              " 'group_auc': 0.831}"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_best_trained.run_eval(test_file, num_ngs=test_num_ngs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XH7L5VOQvARx",
        "outputId": "46721757-0218-4a41-8557-fcd7806b0852"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<recommenders.models.deeprec.models.sequential.sum.SUMModel at 0x1458576be80>"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_best_trained.predict(test_file, output_file)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "GRU4REC.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
