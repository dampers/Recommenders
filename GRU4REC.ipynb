{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sknnn2xs-AHi",
        "outputId": "a4f15d3b-23ff-42f1-8d49-ded97e42bf52"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "System version: 3.8.9 (tags/v3.8.9:a743f81, Apr  6 2021, 14:02:34) [MSC v.1928 64 bit (AMD64)]\n",
            "Tensorflow version: 2.8.0\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "import logging\n",
        "import papermill as pm\n",
        "import scrapbook as sb\n",
        "from tempfile import TemporaryDirectory\n",
        "import numpy as np\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.get_logger().setLevel('ERROR') # only show error messages\n",
        "\n",
        "from recommenders.utils.timer import Timer\n",
        "from recommenders.utils.constants import SEED\n",
        "from recommenders.models.deeprec.deeprec_utils import (\n",
        "    prepare_hparams\n",
        ")\n",
        "\n",
        "from resources.data_preprocessing import data_preprocessing\n",
        "# from recommenders.datasets.amazon_reviews import download_and_extract, data_preprocessing\n",
        "from recommenders.datasets.download_utils import maybe_download\n",
        "\n",
        "\n",
        "# from recommenders.models.deeprec.models.sequential.sli_rec import SLI_RECModel as SeqModel\n",
        "####  to use the other model, use one of the following lines:\n",
        "#from recommenders.models.deeprec.models.sequential.asvd import A2SVDModel as SeqModel\n",
        "# from recommenders.models.deeprec.models.sequential.caser import CaserModel as SeqModel\n",
        "from recommenders.models.deeprec.models.sequential.gru4rec import GRU4RecModel as SeqModel\n",
        "# from recommenders.models.deeprec.models.sequential.sum import SUMModel as SeqModel\n",
        "\n",
        "#from recommenders.models.deeprec.models.sequential.nextitnet import NextItNetModel\n",
        "\n",
        "from recommenders.models.deeprec.io.sequential_iterator import SequentialIterator\n",
        "#from recommenders.models.deeprec.io.nextitnet_iterator import NextItNetIterator\n",
        "\n",
        "print(\"System version: {}\".format(sys.version))\n",
        "print(\"Tensorflow version: {}\".format(tf.__version__))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "sxtfUZfP-ZWR"
      },
      "outputs": [],
      "source": [
        "##  ATTENTION: change to the corresponding config file, e.g., caser.yaml for CaserModel, sum.yaml for SUMModel\n",
        "# yaml_file = '../../recommenders/models/deeprec/config/sli_rec.yaml'  \n",
        "yaml_file = './gru4rec.yaml'  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "_NnzhN4h_rI5"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 10\n",
        "BATCH_SIZE = 400\n",
        "RANDOM_SEED = SEED  # Set None for non-deterministic result\n",
        "\n",
        "data_path = os.path.join(\"resources/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "0RAMNcI3AClc"
      },
      "outputs": [],
      "source": [
        "# for test\n",
        "train_file = os.path.join(data_path, r'train_data')\n",
        "valid_file = os.path.join(data_path, r'valid_data')\n",
        "test_file = os.path.join(data_path, r'test_data')\n",
        "user_vocab = os.path.join(data_path, r'user_vocab.pkl')\n",
        "item_vocab = os.path.join(data_path, r'item_vocab.pkl')\n",
        "cate_vocab = os.path.join(data_path, r'cate_vocab.pkl')\n",
        "output_file = os.path.join(data_path, r'output_gru4rec.txt')\n",
        "\n",
        "# reviews_name = 'json'\n",
        "# meta_name = 'json'\n",
        "# reviews_file = os.path.join(data_path, reviews_name)\n",
        "# meta_file = os.path.join(data_path, meta_name)\n",
        "train_num_ngs = 4 # number of negative instances with a positive instance for training\n",
        "valid_num_ngs = 4 # number of negative instances with a positive instance for validation\n",
        "test_num_ngs = 9 # number of negative instances with a positive instance for testing\n",
        "sample_rate = 0.01 # sample a small item set for training and testing here for fast example\n",
        "\n",
        "input_files = [data_path, train_file, valid_file, test_file, user_vocab, item_vocab, cate_vocab]\n",
        "\n",
        "if not os.path.exists(train_file):\n",
        "    # download_and_extract(reviews_name, reviews_file)\n",
        "    # download_and_extract(meta_name, meta_file)\n",
        "    data_preprocessing(*input_files, sample_rate=sample_rate, valid_num_ngs=valid_num_ngs, test_num_ngs=test_num_ngs)\n",
        "    #### uncomment this for the NextItNet model, because it does not need to unfold the user history\n",
        "    # data_preprocessing(*input_files, sample_rate=sample_rate, valid_num_ngs=valid_num_ngs, test_num_ngs=test_num_ngs, is_history_expanding=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "rwTMO3WEWaCu"
      },
      "outputs": [],
      "source": [
        "### NOTE:  \n",
        "### remember to use `_create_vocab(train_file, user_vocab, item_vocab, cate_vocab)` to generate the user_vocab, item_vocab and cate_vocab files, if you are using your own dataset rather than using our demo Amazon dataset.\n",
        "hparams = prepare_hparams(yaml_file, \n",
        "                          embed_l2=0., \n",
        "                          layer_l2=0., \n",
        "                          learning_rate=0.001,  # set to 0.01 if batch normalization is disable\n",
        "                          epochs=EPOCHS,\n",
        "                          batch_size=BATCH_SIZE,\n",
        "                          show_step=20,\n",
        "                          MODEL_DIR=os.path.join(data_path, \"model\", \"gru4rec/\"),\n",
        "                          SUMMARIES_DIR=os.path.join(data_path, \"summary\", \"gru4rec/\"),\n",
        "                          user_vocab=user_vocab,\n",
        "                          item_vocab=item_vocab,\n",
        "                          cate_vocab=cate_vocab,\n",
        "                          need_sample=True,\n",
        "                          train_num_ngs=train_num_ngs, # provides the number of negative instances for each positive instance for loss computation.\n",
        "            )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "4Rvq7gFluVPq"
      },
      "outputs": [],
      "source": [
        "input_creator = SequentialIterator\n",
        "#### uncomment this for the NextItNet model, because it needs a special data iterator for training\n",
        "#input_creator = NextItNetIterator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VFNfgcKNuaVv",
        "outputId": "2fa083f0-f73d-452a-efef-12151b40c7c5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\조예은\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\recommenders\\models\\deeprec\\models\\sequential\\gru4rec.py:71: UserWarning: `tf.nn.rnn_cell.GRUCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.GRUCell`, and will be replaced by that in Tensorflow 2.0.\n",
            "  GRUCell(self.hidden_size),\n",
            "C:\\Users\\조예은\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\layers\\legacy_rnn\\rnn_cell_impl.py:570: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  self._gate_kernel = self.add_variable(\n",
            "C:\\Users\\조예은\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\layers\\legacy_rnn\\rnn_cell_impl.py:574: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  self._gate_bias = self.add_variable(\n",
            "C:\\Users\\조예은\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\layers\\legacy_rnn\\rnn_cell_impl.py:580: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  self._candidate_kernel = self.add_variable(\n",
            "C:\\Users\\조예은\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\layers\\legacy_rnn\\rnn_cell_impl.py:584: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  self._candidate_bias = self.add_variable(\n",
            "C:\\Users\\조예은\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\recommenders\\models\\deeprec\\models\\base_model.py:701: UserWarning: `tf.layers.batch_normalization` is deprecated and will be removed in a future version. Please use `tf.keras.layers.BatchNormalization` instead. In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.BatchNormalization` documentation).\n",
            "  curr_hidden_nn_layer = tf.compat.v1.layers.batch_normalization(\n",
            "C:\\Users\\조예은\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\legacy_tf_layers\\normalization.py:463: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
            "  return layer.apply(inputs, training=training)\n"
          ]
        }
      ],
      "source": [
        "model = SeqModel(hparams, input_creator, seed=RANDOM_SEED)\n",
        "\n",
        "## sometimes we don't want to train a model from scratch\n",
        "## then we can load a pre-trained model like this: \n",
        "#model.load_model(r'your_model_path')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8AQxPlk-uheU",
        "outputId": "55a5328a-44bd-4213-b10c-96f26313586a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'auc': 0.5196, 'logloss': 0.6931, 'mean_mrr': 0.2813, 'ndcg@2': 0.147, 'ndcg@4': 0.2448, 'ndcg@6': 0.3221, 'group_auc': 0.5196}\n"
          ]
        }
      ],
      "source": [
        "# test_num_ngs is the number of negative lines after each positive line in your test_file\n",
        "print(model.run_eval(test_file, num_ngs=test_num_ngs)) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QFKQu9rGu0jq",
        "outputId": "c93b6ebf-50cc-43ea-e620-418e4537ad31"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 20 , total_loss: 1.4457, data_loss: 1.4457\n",
            "step 40 , total_loss: 1.3506, data_loss: 1.3506\n",
            "step 60 , total_loss: 1.3131, data_loss: 1.3131\n",
            "step 80 , total_loss: 1.2563, data_loss: 1.2563\n",
            "step 100 , total_loss: 1.3174, data_loss: 1.3174\n",
            "step 120 , total_loss: 1.2491, data_loss: 1.2491\n",
            "step 140 , total_loss: 1.2656, data_loss: 1.2656\n",
            "step 160 , total_loss: 1.2836, data_loss: 1.2836\n",
            "step 180 , total_loss: 1.2230, data_loss: 1.2230\n",
            "step 200 , total_loss: 1.2121, data_loss: 1.2121\n",
            "step 220 , total_loss: 1.1946, data_loss: 1.1946\n",
            "step 240 , total_loss: 1.2286, data_loss: 1.2286\n",
            "step 260 , total_loss: 1.2607, data_loss: 1.2607\n",
            "step 280 , total_loss: 1.2292, data_loss: 1.2292\n",
            "step 300 , total_loss: 1.1750, data_loss: 1.1750\n",
            "step 320 , total_loss: 1.2444, data_loss: 1.2444\n",
            "step 340 , total_loss: 1.1585, data_loss: 1.1585\n",
            "step 360 , total_loss: 1.1805, data_loss: 1.1805\n",
            "step 380 , total_loss: 1.2176, data_loss: 1.2176\n",
            "step 400 , total_loss: 1.2288, data_loss: 1.2288\n",
            "step 420 , total_loss: 1.1378, data_loss: 1.1378\n",
            "step 440 , total_loss: 1.1951, data_loss: 1.1951\n",
            "step 460 , total_loss: 1.2414, data_loss: 1.2414\n",
            "step 480 , total_loss: 1.1543, data_loss: 1.1543\n",
            "step 500 , total_loss: 1.1050, data_loss: 1.1050\n",
            "step 520 , total_loss: 1.1198, data_loss: 1.1198\n",
            "step 540 , total_loss: 1.1409, data_loss: 1.1409\n",
            "step 560 , total_loss: 1.1353, data_loss: 1.1353\n",
            "step 580 , total_loss: 1.1404, data_loss: 1.1404\n",
            "step 600 , total_loss: 1.1745, data_loss: 1.1745\n",
            "step 620 , total_loss: 1.1686, data_loss: 1.1686\n",
            "step 640 , total_loss: 1.1802, data_loss: 1.1802\n",
            "step 660 , total_loss: 1.0953, data_loss: 1.0953\n",
            "step 680 , total_loss: 1.1408, data_loss: 1.1408\n",
            "step 700 , total_loss: 1.0725, data_loss: 1.0725\n",
            "step 720 , total_loss: 1.1391, data_loss: 1.1391\n",
            "step 740 , total_loss: 1.0931, data_loss: 1.0931\n",
            "step 760 , total_loss: 1.0949, data_loss: 1.0949\n",
            "eval valid at epoch 1: auc:0.774,logloss:0.5552,mean_mrr:0.6861,ndcg@2:0.6457,ndcg@4:0.7551,ndcg@6:0.7653,group_auc:0.7731\n",
            "step 20 , total_loss: 1.0720, data_loss: 1.0720\n",
            "step 40 , total_loss: 1.1235, data_loss: 1.1235\n",
            "step 60 , total_loss: 1.1987, data_loss: 1.1987\n",
            "step 80 , total_loss: 1.1305, data_loss: 1.1305\n",
            "step 100 , total_loss: 1.0937, data_loss: 1.0937\n",
            "step 120 , total_loss: 1.1340, data_loss: 1.1340\n",
            "step 140 , total_loss: 1.1055, data_loss: 1.1055\n",
            "step 160 , total_loss: 1.2134, data_loss: 1.2134\n",
            "step 180 , total_loss: 1.0896, data_loss: 1.0896\n",
            "step 200 , total_loss: 1.0726, data_loss: 1.0726\n",
            "step 220 , total_loss: 1.1267, data_loss: 1.1267\n",
            "step 240 , total_loss: 1.0747, data_loss: 1.0747\n",
            "step 260 , total_loss: 1.1203, data_loss: 1.1203\n",
            "step 280 , total_loss: 1.1146, data_loss: 1.1146\n",
            "step 300 , total_loss: 1.1322, data_loss: 1.1322\n",
            "step 320 , total_loss: 1.0488, data_loss: 1.0488\n",
            "step 340 , total_loss: 1.1199, data_loss: 1.1199\n",
            "step 360 , total_loss: 1.0481, data_loss: 1.0481\n",
            "step 380 , total_loss: 1.1687, data_loss: 1.1687\n",
            "step 400 , total_loss: 1.0853, data_loss: 1.0853\n",
            "step 420 , total_loss: 1.1493, data_loss: 1.1493\n",
            "step 440 , total_loss: 1.0803, data_loss: 1.0803\n",
            "step 460 , total_loss: 1.0292, data_loss: 1.0292\n",
            "step 480 , total_loss: 1.0901, data_loss: 1.0901\n",
            "step 500 , total_loss: 1.1096, data_loss: 1.1096\n",
            "step 520 , total_loss: 1.0990, data_loss: 1.0990\n",
            "step 540 , total_loss: 1.0644, data_loss: 1.0644\n",
            "step 560 , total_loss: 1.0524, data_loss: 1.0524\n",
            "step 580 , total_loss: 1.0818, data_loss: 1.0818\n",
            "step 600 , total_loss: 1.1111, data_loss: 1.1111\n",
            "step 620 , total_loss: 1.1568, data_loss: 1.1568\n",
            "step 640 , total_loss: 1.0843, data_loss: 1.0843\n",
            "step 660 , total_loss: 1.0242, data_loss: 1.0242\n",
            "step 680 , total_loss: 1.0859, data_loss: 1.0859\n",
            "step 700 , total_loss: 1.0656, data_loss: 1.0656\n",
            "step 720 , total_loss: 1.0774, data_loss: 1.0774\n",
            "step 740 , total_loss: 1.0800, data_loss: 1.0800\n",
            "step 760 , total_loss: 1.0646, data_loss: 1.0646\n",
            "eval valid at epoch 2: auc:0.7895,logloss:0.5086,mean_mrr:0.7037,ndcg@2:0.668,ndcg@4:0.7702,ndcg@6:0.7786,group_auc:0.7893\n",
            "step 20 , total_loss: 1.0009, data_loss: 1.0009\n",
            "step 40 , total_loss: 1.0353, data_loss: 1.0353\n",
            "step 60 , total_loss: 1.0734, data_loss: 1.0734\n",
            "step 80 , total_loss: 1.0382, data_loss: 1.0382\n",
            "step 100 , total_loss: 1.0361, data_loss: 1.0361\n",
            "step 120 , total_loss: 0.9903, data_loss: 0.9903\n",
            "step 140 , total_loss: 1.0390, data_loss: 1.0390\n",
            "step 160 , total_loss: 1.0426, data_loss: 1.0426\n",
            "step 180 , total_loss: 1.0615, data_loss: 1.0615\n",
            "step 200 , total_loss: 1.0856, data_loss: 1.0856\n",
            "step 220 , total_loss: 1.0958, data_loss: 1.0958\n",
            "step 240 , total_loss: 1.0114, data_loss: 1.0114\n",
            "step 260 , total_loss: 0.9933, data_loss: 0.9933\n",
            "step 280 , total_loss: 1.0886, data_loss: 1.0886\n",
            "step 300 , total_loss: 1.0101, data_loss: 1.0101\n",
            "step 320 , total_loss: 1.0269, data_loss: 1.0269\n",
            "step 340 , total_loss: 1.0766, data_loss: 1.0766\n",
            "step 360 , total_loss: 0.9830, data_loss: 0.9830\n",
            "step 380 , total_loss: 1.0641, data_loss: 1.0641\n",
            "step 400 , total_loss: 1.0701, data_loss: 1.0701\n",
            "step 420 , total_loss: 1.0708, data_loss: 1.0708\n",
            "step 440 , total_loss: 1.1199, data_loss: 1.1199\n",
            "step 460 , total_loss: 1.0749, data_loss: 1.0749\n",
            "step 480 , total_loss: 1.0621, data_loss: 1.0621\n",
            "step 500 , total_loss: 1.0099, data_loss: 1.0099\n",
            "step 520 , total_loss: 1.0764, data_loss: 1.0764\n",
            "step 540 , total_loss: 1.0738, data_loss: 1.0738\n",
            "step 560 , total_loss: 1.0321, data_loss: 1.0321\n",
            "step 580 , total_loss: 1.0625, data_loss: 1.0625\n",
            "step 600 , total_loss: 1.0710, data_loss: 1.0710\n",
            "step 620 , total_loss: 1.0215, data_loss: 1.0215\n",
            "step 640 , total_loss: 1.0157, data_loss: 1.0157\n",
            "step 660 , total_loss: 1.0775, data_loss: 1.0775\n",
            "step 680 , total_loss: 1.1142, data_loss: 1.1142\n",
            "step 700 , total_loss: 1.0249, data_loss: 1.0249\n",
            "step 720 , total_loss: 1.0242, data_loss: 1.0242\n",
            "step 740 , total_loss: 1.0058, data_loss: 1.0058\n",
            "step 760 , total_loss: 1.0537, data_loss: 1.0537\n",
            "eval valid at epoch 3: auc:0.7958,logloss:0.5408,mean_mrr:0.7119,ndcg@2:0.6788,ndcg@4:0.7778,ndcg@6:0.7848,group_auc:0.7976\n",
            "step 20 , total_loss: 0.9884, data_loss: 0.9884\n",
            "step 40 , total_loss: 1.0319, data_loss: 1.0319\n",
            "step 60 , total_loss: 1.0422, data_loss: 1.0422\n",
            "step 80 , total_loss: 1.0102, data_loss: 1.0102\n",
            "step 100 , total_loss: 1.0246, data_loss: 1.0246\n",
            "step 120 , total_loss: 1.0572, data_loss: 1.0572\n",
            "step 140 , total_loss: 1.0334, data_loss: 1.0334\n",
            "step 160 , total_loss: 1.1465, data_loss: 1.1465\n",
            "step 180 , total_loss: 1.0621, data_loss: 1.0621\n",
            "step 200 , total_loss: 0.9653, data_loss: 0.9653\n",
            "step 220 , total_loss: 1.0698, data_loss: 1.0698\n",
            "step 240 , total_loss: 0.9647, data_loss: 0.9647\n",
            "step 260 , total_loss: 0.9862, data_loss: 0.9862\n",
            "step 280 , total_loss: 1.0104, data_loss: 1.0104\n",
            "step 300 , total_loss: 0.9741, data_loss: 0.9741\n",
            "step 320 , total_loss: 1.0184, data_loss: 1.0184\n",
            "step 340 , total_loss: 1.0596, data_loss: 1.0596\n",
            "step 360 , total_loss: 1.0899, data_loss: 1.0899\n",
            "step 380 , total_loss: 0.9868, data_loss: 0.9868\n",
            "step 400 , total_loss: 1.0749, data_loss: 1.0749\n",
            "step 420 , total_loss: 1.0041, data_loss: 1.0041\n",
            "step 440 , total_loss: 0.9695, data_loss: 0.9695\n",
            "step 460 , total_loss: 1.0554, data_loss: 1.0554\n",
            "step 480 , total_loss: 0.9191, data_loss: 0.9191\n",
            "step 500 , total_loss: 0.9994, data_loss: 0.9994\n",
            "step 520 , total_loss: 0.9605, data_loss: 0.9605\n",
            "step 540 , total_loss: 1.0070, data_loss: 1.0070\n",
            "step 560 , total_loss: 1.0439, data_loss: 1.0439\n",
            "step 580 , total_loss: 0.9665, data_loss: 0.9665\n",
            "step 600 , total_loss: 1.0136, data_loss: 1.0136\n",
            "step 620 , total_loss: 1.0242, data_loss: 1.0242\n",
            "step 640 , total_loss: 1.0030, data_loss: 1.0030\n",
            "step 660 , total_loss: 0.9664, data_loss: 0.9664\n",
            "step 680 , total_loss: 0.9929, data_loss: 0.9929\n",
            "step 700 , total_loss: 1.0272, data_loss: 1.0272\n",
            "step 720 , total_loss: 0.9574, data_loss: 0.9574\n",
            "step 740 , total_loss: 1.0202, data_loss: 1.0202\n",
            "step 760 , total_loss: 1.0271, data_loss: 1.0271\n",
            "eval valid at epoch 4: auc:0.8059,logloss:0.5199,mean_mrr:0.7254,ndcg@2:0.697,ndcg@4:0.7892,ndcg@6:0.795,group_auc:0.8097\n",
            "step 20 , total_loss: 0.9914, data_loss: 0.9914\n",
            "step 40 , total_loss: 0.9960, data_loss: 0.9960\n",
            "step 60 , total_loss: 0.9538, data_loss: 0.9538\n",
            "step 80 , total_loss: 1.0803, data_loss: 1.0803\n",
            "step 100 , total_loss: 0.9501, data_loss: 0.9501\n",
            "step 120 , total_loss: 1.0007, data_loss: 1.0007\n",
            "step 140 , total_loss: 1.0101, data_loss: 1.0101\n",
            "step 160 , total_loss: 0.9810, data_loss: 0.9810\n",
            "step 180 , total_loss: 1.0262, data_loss: 1.0262\n",
            "step 200 , total_loss: 0.9972, data_loss: 0.9972\n",
            "step 220 , total_loss: 1.0166, data_loss: 1.0166\n",
            "step 240 , total_loss: 1.0185, data_loss: 1.0185\n",
            "step 260 , total_loss: 1.1153, data_loss: 1.1153\n",
            "step 280 , total_loss: 1.0039, data_loss: 1.0039\n",
            "step 300 , total_loss: 0.9202, data_loss: 0.9202\n",
            "step 320 , total_loss: 0.9931, data_loss: 0.9931\n",
            "step 340 , total_loss: 1.0947, data_loss: 1.0947\n",
            "step 360 , total_loss: 0.9903, data_loss: 0.9903\n",
            "step 380 , total_loss: 0.9693, data_loss: 0.9693\n",
            "step 400 , total_loss: 0.9325, data_loss: 0.9325\n",
            "step 420 , total_loss: 0.9966, data_loss: 0.9966\n",
            "step 440 , total_loss: 0.9896, data_loss: 0.9896\n",
            "step 460 , total_loss: 0.9978, data_loss: 0.9978\n",
            "step 480 , total_loss: 0.9686, data_loss: 0.9686\n",
            "step 500 , total_loss: 0.9652, data_loss: 0.9652\n",
            "step 520 , total_loss: 1.0849, data_loss: 1.0849\n",
            "step 540 , total_loss: 1.0645, data_loss: 1.0645\n",
            "step 560 , total_loss: 0.8995, data_loss: 0.8995\n",
            "step 580 , total_loss: 1.0054, data_loss: 1.0054\n",
            "step 600 , total_loss: 1.0769, data_loss: 1.0769\n",
            "step 620 , total_loss: 0.9802, data_loss: 0.9802\n",
            "step 640 , total_loss: 0.9933, data_loss: 0.9933\n",
            "step 660 , total_loss: 1.1011, data_loss: 1.1011\n",
            "step 680 , total_loss: 1.0185, data_loss: 1.0185\n",
            "step 700 , total_loss: 1.0535, data_loss: 1.0535\n",
            "step 720 , total_loss: 0.9666, data_loss: 0.9666\n",
            "step 740 , total_loss: 1.0588, data_loss: 1.0588\n",
            "step 760 , total_loss: 1.0456, data_loss: 1.0456\n",
            "eval valid at epoch 5: auc:0.8108,logloss:0.5181,mean_mrr:0.7281,ndcg@2:0.7001,ndcg@4:0.7912,ndcg@6:0.7971,group_auc:0.8119\n",
            "step 20 , total_loss: 0.9960, data_loss: 0.9960\n",
            "step 40 , total_loss: 1.0190, data_loss: 1.0190\n",
            "step 60 , total_loss: 1.0253, data_loss: 1.0253\n",
            "step 80 , total_loss: 0.9811, data_loss: 0.9811\n",
            "step 100 , total_loss: 0.9775, data_loss: 0.9775\n",
            "step 120 , total_loss: 1.0510, data_loss: 1.0510\n",
            "step 140 , total_loss: 0.9938, data_loss: 0.9938\n",
            "step 160 , total_loss: 0.9696, data_loss: 0.9696\n",
            "step 180 , total_loss: 0.8926, data_loss: 0.8926\n",
            "step 200 , total_loss: 0.9924, data_loss: 0.9924\n",
            "step 220 , total_loss: 0.9830, data_loss: 0.9830\n",
            "step 240 , total_loss: 1.0324, data_loss: 1.0324\n",
            "step 260 , total_loss: 0.9490, data_loss: 0.9490\n",
            "step 280 , total_loss: 1.0617, data_loss: 1.0617\n",
            "step 300 , total_loss: 1.0294, data_loss: 1.0294\n",
            "step 320 , total_loss: 0.9252, data_loss: 0.9252\n",
            "step 340 , total_loss: 1.0155, data_loss: 1.0155\n",
            "step 360 , total_loss: 0.9992, data_loss: 0.9992\n",
            "step 380 , total_loss: 0.9641, data_loss: 0.9641\n",
            "step 400 , total_loss: 1.0274, data_loss: 1.0274\n",
            "step 420 , total_loss: 0.9667, data_loss: 0.9667\n",
            "step 440 , total_loss: 0.9824, data_loss: 0.9824\n",
            "step 460 , total_loss: 0.9580, data_loss: 0.9580\n",
            "step 480 , total_loss: 1.0080, data_loss: 1.0080\n",
            "step 500 , total_loss: 0.9576, data_loss: 0.9576\n",
            "step 520 , total_loss: 0.9457, data_loss: 0.9457\n",
            "step 540 , total_loss: 1.0242, data_loss: 1.0242\n",
            "step 560 , total_loss: 0.9323, data_loss: 0.9323\n",
            "step 580 , total_loss: 0.8907, data_loss: 0.8907\n",
            "step 600 , total_loss: 0.9248, data_loss: 0.9248\n",
            "step 620 , total_loss: 0.9956, data_loss: 0.9956\n",
            "step 640 , total_loss: 0.9874, data_loss: 0.9874\n",
            "step 660 , total_loss: 0.9906, data_loss: 0.9906\n",
            "step 680 , total_loss: 0.9517, data_loss: 0.9517\n",
            "step 700 , total_loss: 0.9519, data_loss: 0.9519\n",
            "step 720 , total_loss: 0.9399, data_loss: 0.9399\n",
            "step 740 , total_loss: 1.0175, data_loss: 1.0175\n",
            "step 760 , total_loss: 0.9621, data_loss: 0.9621\n",
            "eval valid at epoch 6: auc:0.813,logloss:0.5403,mean_mrr:0.7312,ndcg@2:0.7051,ndcg@4:0.7938,ndcg@6:0.7993,group_auc:0.8148\n",
            "step 20 , total_loss: 0.9682, data_loss: 0.9682\n",
            "step 40 , total_loss: 1.0569, data_loss: 1.0569\n",
            "step 60 , total_loss: 0.9816, data_loss: 0.9816\n",
            "step 80 , total_loss: 0.9823, data_loss: 0.9823\n",
            "step 100 , total_loss: 0.9345, data_loss: 0.9345\n",
            "step 120 , total_loss: 0.9395, data_loss: 0.9395\n",
            "step 140 , total_loss: 0.9729, data_loss: 0.9729\n",
            "step 160 , total_loss: 0.9283, data_loss: 0.9283\n",
            "step 180 , total_loss: 0.9340, data_loss: 0.9340\n",
            "step 200 , total_loss: 0.9521, data_loss: 0.9521\n",
            "step 220 , total_loss: 1.0345, data_loss: 1.0345\n",
            "step 240 , total_loss: 1.0174, data_loss: 1.0174\n",
            "step 260 , total_loss: 0.8966, data_loss: 0.8966\n",
            "step 280 , total_loss: 0.9596, data_loss: 0.9596\n",
            "step 300 , total_loss: 1.0204, data_loss: 1.0204\n",
            "step 320 , total_loss: 0.9098, data_loss: 0.9098\n",
            "step 340 , total_loss: 0.9385, data_loss: 0.9385\n",
            "step 360 , total_loss: 0.8930, data_loss: 0.8930\n",
            "step 380 , total_loss: 1.0357, data_loss: 1.0357\n",
            "step 400 , total_loss: 1.0963, data_loss: 1.0963\n",
            "step 420 , total_loss: 0.9821, data_loss: 0.9821\n",
            "step 440 , total_loss: 0.9023, data_loss: 0.9023\n",
            "step 460 , total_loss: 1.0418, data_loss: 1.0418\n",
            "step 480 , total_loss: 0.9899, data_loss: 0.9899\n",
            "step 500 , total_loss: 0.9656, data_loss: 0.9656\n",
            "step 520 , total_loss: 1.0282, data_loss: 1.0282\n",
            "step 540 , total_loss: 0.9376, data_loss: 0.9376\n",
            "step 560 , total_loss: 0.9941, data_loss: 0.9941\n",
            "step 580 , total_loss: 1.0175, data_loss: 1.0175\n",
            "step 600 , total_loss: 0.9706, data_loss: 0.9706\n",
            "step 620 , total_loss: 0.9224, data_loss: 0.9224\n",
            "step 640 , total_loss: 0.9365, data_loss: 0.9365\n",
            "step 660 , total_loss: 0.9911, data_loss: 0.9911\n",
            "step 680 , total_loss: 0.9514, data_loss: 0.9514\n",
            "step 700 , total_loss: 1.0628, data_loss: 1.0628\n",
            "step 720 , total_loss: 1.0160, data_loss: 1.0160\n",
            "step 740 , total_loss: 1.0185, data_loss: 1.0185\n",
            "step 760 , total_loss: 0.9088, data_loss: 0.9088\n",
            "eval valid at epoch 7: auc:0.8161,logloss:0.5376,mean_mrr:0.7343,ndcg@2:0.708,ndcg@4:0.7966,ndcg@6:0.8017,group_auc:0.8174\n",
            "step 20 , total_loss: 1.0876, data_loss: 1.0876\n",
            "step 40 , total_loss: 0.9560, data_loss: 0.9560\n",
            "step 60 , total_loss: 0.9461, data_loss: 0.9461\n",
            "step 80 , total_loss: 0.9907, data_loss: 0.9907\n",
            "step 100 , total_loss: 0.9331, data_loss: 0.9331\n",
            "step 120 , total_loss: 0.8887, data_loss: 0.8887\n",
            "step 140 , total_loss: 0.9683, data_loss: 0.9683\n",
            "step 160 , total_loss: 0.9389, data_loss: 0.9389\n",
            "step 180 , total_loss: 1.0340, data_loss: 1.0340\n",
            "step 200 , total_loss: 1.0501, data_loss: 1.0501\n",
            "step 220 , total_loss: 0.9335, data_loss: 0.9335\n",
            "step 240 , total_loss: 0.9562, data_loss: 0.9562\n",
            "step 260 , total_loss: 0.9390, data_loss: 0.9390\n",
            "step 280 , total_loss: 1.0006, data_loss: 1.0006\n",
            "step 300 , total_loss: 0.9694, data_loss: 0.9694\n",
            "step 320 , total_loss: 0.9417, data_loss: 0.9417\n",
            "step 340 , total_loss: 1.0243, data_loss: 1.0243\n",
            "step 360 , total_loss: 0.9338, data_loss: 0.9338\n",
            "step 380 , total_loss: 0.9711, data_loss: 0.9711\n",
            "step 400 , total_loss: 0.8970, data_loss: 0.8970\n",
            "step 420 , total_loss: 0.9909, data_loss: 0.9909\n",
            "step 440 , total_loss: 1.0286, data_loss: 1.0286\n",
            "step 460 , total_loss: 0.9135, data_loss: 0.9135\n",
            "step 480 , total_loss: 0.9866, data_loss: 0.9866\n",
            "step 500 , total_loss: 0.8846, data_loss: 0.8846\n",
            "step 520 , total_loss: 0.9333, data_loss: 0.9333\n",
            "step 540 , total_loss: 1.0317, data_loss: 1.0317\n",
            "step 560 , total_loss: 0.9662, data_loss: 0.9662\n",
            "step 580 , total_loss: 0.9804, data_loss: 0.9804\n",
            "step 600 , total_loss: 1.0206, data_loss: 1.0206\n",
            "step 620 , total_loss: 0.9582, data_loss: 0.9582\n",
            "step 640 , total_loss: 0.9743, data_loss: 0.9743\n",
            "step 660 , total_loss: 1.0154, data_loss: 1.0154\n",
            "step 680 , total_loss: 0.9720, data_loss: 0.9720\n",
            "step 700 , total_loss: 0.9395, data_loss: 0.9395\n",
            "step 720 , total_loss: 1.0652, data_loss: 1.0652\n",
            "step 740 , total_loss: 0.9507, data_loss: 0.9507\n",
            "step 760 , total_loss: 0.9277, data_loss: 0.9277\n",
            "eval valid at epoch 8: auc:0.8195,logloss:0.5595,mean_mrr:0.7356,ndcg@2:0.7121,ndcg@4:0.7979,ndcg@6:0.8027,group_auc:0.8194\n",
            "step 20 , total_loss: 0.9621, data_loss: 0.9621\n",
            "step 40 , total_loss: 0.9676, data_loss: 0.9676\n",
            "step 60 , total_loss: 0.9513, data_loss: 0.9513\n",
            "step 80 , total_loss: 0.9494, data_loss: 0.9494\n",
            "step 100 , total_loss: 0.9701, data_loss: 0.9701\n",
            "step 120 , total_loss: 0.9578, data_loss: 0.9578\n",
            "step 140 , total_loss: 1.0269, data_loss: 1.0269\n",
            "step 160 , total_loss: 0.9672, data_loss: 0.9672\n",
            "step 180 , total_loss: 0.9260, data_loss: 0.9260\n",
            "step 200 , total_loss: 0.9298, data_loss: 0.9298\n",
            "step 220 , total_loss: 0.9609, data_loss: 0.9609\n",
            "step 240 , total_loss: 0.9686, data_loss: 0.9686\n",
            "step 260 , total_loss: 0.9391, data_loss: 0.9391\n",
            "step 280 , total_loss: 1.0063, data_loss: 1.0063\n",
            "step 300 , total_loss: 0.9422, data_loss: 0.9422\n",
            "step 320 , total_loss: 0.9268, data_loss: 0.9268\n",
            "step 340 , total_loss: 1.0196, data_loss: 1.0196\n",
            "step 360 , total_loss: 0.9919, data_loss: 0.9919\n",
            "step 380 , total_loss: 0.8998, data_loss: 0.8998\n",
            "step 400 , total_loss: 0.9376, data_loss: 0.9376\n",
            "step 420 , total_loss: 1.0524, data_loss: 1.0524\n",
            "step 440 , total_loss: 0.9161, data_loss: 0.9161\n",
            "step 460 , total_loss: 0.9565, data_loss: 0.9565\n",
            "step 480 , total_loss: 0.9819, data_loss: 0.9819\n",
            "step 500 , total_loss: 0.9309, data_loss: 0.9309\n",
            "step 520 , total_loss: 0.9982, data_loss: 0.9982\n",
            "step 540 , total_loss: 0.9624, data_loss: 0.9624\n",
            "step 560 , total_loss: 0.8985, data_loss: 0.8985\n",
            "step 580 , total_loss: 0.9596, data_loss: 0.9596\n",
            "step 600 , total_loss: 0.9413, data_loss: 0.9413\n",
            "step 620 , total_loss: 0.9258, data_loss: 0.9258\n",
            "step 640 , total_loss: 1.0180, data_loss: 1.0180\n",
            "step 660 , total_loss: 1.0153, data_loss: 1.0153\n",
            "step 680 , total_loss: 0.9749, data_loss: 0.9749\n",
            "step 700 , total_loss: 0.9281, data_loss: 0.9281\n",
            "step 720 , total_loss: 0.8782, data_loss: 0.8782\n",
            "step 740 , total_loss: 0.9806, data_loss: 0.9806\n",
            "step 760 , total_loss: 0.9606, data_loss: 0.9606\n",
            "eval valid at epoch 9: auc:0.825,logloss:0.5626,mean_mrr:0.7409,ndcg@2:0.7178,ndcg@4:0.802,ndcg@6:0.8067,group_auc:0.8239\n",
            "step 20 , total_loss: 0.9591, data_loss: 0.9591\n",
            "step 40 , total_loss: 0.9201, data_loss: 0.9201\n",
            "step 60 , total_loss: 0.9460, data_loss: 0.9460\n",
            "step 80 , total_loss: 0.9342, data_loss: 0.9342\n",
            "step 100 , total_loss: 0.9485, data_loss: 0.9485\n",
            "step 120 , total_loss: 0.9415, data_loss: 0.9415\n",
            "step 140 , total_loss: 1.0144, data_loss: 1.0144\n",
            "step 160 , total_loss: 0.9534, data_loss: 0.9534\n",
            "step 180 , total_loss: 0.9075, data_loss: 0.9075\n",
            "step 200 , total_loss: 0.9670, data_loss: 0.9670\n",
            "step 220 , total_loss: 0.9358, data_loss: 0.9358\n",
            "step 240 , total_loss: 0.9452, data_loss: 0.9452\n",
            "step 260 , total_loss: 0.9518, data_loss: 0.9518\n",
            "step 280 , total_loss: 0.9395, data_loss: 0.9395\n",
            "step 300 , total_loss: 0.9114, data_loss: 0.9114\n",
            "step 320 , total_loss: 0.9834, data_loss: 0.9834\n",
            "step 340 , total_loss: 1.0048, data_loss: 1.0048\n",
            "step 360 , total_loss: 1.0139, data_loss: 1.0139\n",
            "step 380 , total_loss: 0.8622, data_loss: 0.8622\n",
            "step 400 , total_loss: 0.9474, data_loss: 0.9474\n",
            "step 420 , total_loss: 0.9563, data_loss: 0.9563\n",
            "step 440 , total_loss: 0.9753, data_loss: 0.9753\n",
            "step 460 , total_loss: 0.9708, data_loss: 0.9708\n",
            "step 480 , total_loss: 0.9515, data_loss: 0.9515\n",
            "step 500 , total_loss: 1.0061, data_loss: 1.0061\n",
            "step 520 , total_loss: 0.9133, data_loss: 0.9133\n",
            "step 540 , total_loss: 0.9126, data_loss: 0.9126\n",
            "step 560 , total_loss: 1.0449, data_loss: 1.0449\n",
            "step 580 , total_loss: 0.9274, data_loss: 0.9274\n",
            "step 600 , total_loss: 0.9551, data_loss: 0.9551\n",
            "step 620 , total_loss: 0.9471, data_loss: 0.9471\n",
            "step 640 , total_loss: 0.9382, data_loss: 0.9382\n",
            "step 660 , total_loss: 1.0034, data_loss: 1.0034\n",
            "step 680 , total_loss: 0.9226, data_loss: 0.9226\n",
            "step 700 , total_loss: 0.9190, data_loss: 0.9190\n",
            "step 720 , total_loss: 0.8803, data_loss: 0.8803\n",
            "step 740 , total_loss: 0.9438, data_loss: 0.9438\n",
            "step 760 , total_loss: 1.0781, data_loss: 1.0781\n",
            "eval valid at epoch 10: auc:0.823,logloss:0.5742,mean_mrr:0.7392,ndcg@2:0.7165,ndcg@4:0.8004,ndcg@6:0.8054,group_auc:0.8224\n",
            "[(1, {'auc': 0.774, 'logloss': 0.5552, 'mean_mrr': 0.6861, 'ndcg@2': 0.6457, 'ndcg@4': 0.7551, 'ndcg@6': 0.7653, 'group_auc': 0.7731}), (2, {'auc': 0.7895, 'logloss': 0.5086, 'mean_mrr': 0.7037, 'ndcg@2': 0.668, 'ndcg@4': 0.7702, 'ndcg@6': 0.7786, 'group_auc': 0.7893}), (3, {'auc': 0.7958, 'logloss': 0.5408, 'mean_mrr': 0.7119, 'ndcg@2': 0.6788, 'ndcg@4': 0.7778, 'ndcg@6': 0.7848, 'group_auc': 0.7976}), (4, {'auc': 0.8059, 'logloss': 0.5199, 'mean_mrr': 0.7254, 'ndcg@2': 0.697, 'ndcg@4': 0.7892, 'ndcg@6': 0.795, 'group_auc': 0.8097}), (5, {'auc': 0.8108, 'logloss': 0.5181, 'mean_mrr': 0.7281, 'ndcg@2': 0.7001, 'ndcg@4': 0.7912, 'ndcg@6': 0.7971, 'group_auc': 0.8119}), (6, {'auc': 0.813, 'logloss': 0.5403, 'mean_mrr': 0.7312, 'ndcg@2': 0.7051, 'ndcg@4': 0.7938, 'ndcg@6': 0.7993, 'group_auc': 0.8148}), (7, {'auc': 0.8161, 'logloss': 0.5376, 'mean_mrr': 0.7343, 'ndcg@2': 0.708, 'ndcg@4': 0.7966, 'ndcg@6': 0.8017, 'group_auc': 0.8174}), (8, {'auc': 0.8195, 'logloss': 0.5595, 'mean_mrr': 0.7356, 'ndcg@2': 0.7121, 'ndcg@4': 0.7979, 'ndcg@6': 0.8027, 'group_auc': 0.8194}), (9, {'auc': 0.825, 'logloss': 0.5626, 'mean_mrr': 0.7409, 'ndcg@2': 0.7178, 'ndcg@4': 0.802, 'ndcg@6': 0.8067, 'group_auc': 0.8239}), (10, {'auc': 0.823, 'logloss': 0.5742, 'mean_mrr': 0.7392, 'ndcg@2': 0.7165, 'ndcg@4': 0.8004, 'ndcg@6': 0.8054, 'group_auc': 0.8224})]\n",
            "best epoch: 9\n",
            "Time cost for training is 72.70 mins\n"
          ]
        }
      ],
      "source": [
        "with Timer() as train_time:\n",
        "    model = model.fit(train_file, valid_file, valid_num_ngs=valid_num_ngs) \n",
        "\n",
        "# valid_num_ngs is the number of negative lines after each positive line in your valid_file \n",
        "# we will evaluate the performance of model on valid_file every epoch\n",
        "print('Time cost for training is {0:.2f} mins'.format(train_time.interval/60.0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gmm3DPjbiY9N",
        "outputId": "6dbb1a1d-8b36-49b1-ef5e-f4a807dddfde"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'auc': 0.819, 'logloss': 0.599, 'mean_mrr': 0.5825, 'ndcg@2': 0.5068, 'ndcg@4': 0.6249, 'ndcg@6': 0.6697, 'group_auc': 0.8178}\n"
          ]
        }
      ],
      "source": [
        "res_syn = model.run_eval(test_file, num_ngs=test_num_ngs)\n",
        "print(res_syn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "VB7MBFNkjl5e",
        "outputId": "12b143c9-1d98-4d99-9136-8c779de2d832"
      },
      "outputs": [
        {
          "data": {
            "application/scrapbook.scrap.json+json": {
              "data": {
                "auc": 0.819,
                "group_auc": 0.8178,
                "logloss": 0.599,
                "mean_mrr": 0.5825,
                "ndcg@2": 0.5068,
                "ndcg@4": 0.6249,
                "ndcg@6": 0.6697
              },
              "encoder": "json",
              "name": "res_syn",
              "version": 1
            }
          },
          "metadata": {
            "scrapbook": {
              "data": true,
              "display": false,
              "name": "res_syn"
            }
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "sb.glue(\"res_syn\", res_syn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "h2uxYGfFjpL9"
      },
      "outputs": [],
      "source": [
        "model = model.predict(test_file, output_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wGZ26pt-ujeo",
        "outputId": "5cbe4b07-34e3-4891-c64b-40b2a70a6fa7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\조예은\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\recommenders\\models\\deeprec\\models\\sequential\\gru4rec.py:71: UserWarning: `tf.nn.rnn_cell.GRUCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.GRUCell`, and will be replaced by that in Tensorflow 2.0.\n",
            "  GRUCell(self.hidden_size),\n",
            "C:\\Users\\조예은\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\layers\\legacy_rnn\\rnn_cell_impl.py:570: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  self._gate_kernel = self.add_variable(\n",
            "C:\\Users\\조예은\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\layers\\legacy_rnn\\rnn_cell_impl.py:574: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  self._gate_bias = self.add_variable(\n",
            "C:\\Users\\조예은\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\layers\\legacy_rnn\\rnn_cell_impl.py:580: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  self._candidate_kernel = self.add_variable(\n",
            "C:\\Users\\조예은\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\layers\\legacy_rnn\\rnn_cell_impl.py:584: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  self._candidate_bias = self.add_variable(\n",
            "C:\\Users\\조예은\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\recommenders\\models\\deeprec\\models\\base_model.py:701: UserWarning: `tf.layers.batch_normalization` is deprecated and will be removed in a future version. Please use `tf.keras.layers.BatchNormalization` instead. In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.BatchNormalization` documentation).\n",
            "  curr_hidden_nn_layer = tf.compat.v1.layers.batch_normalization(\n",
            "C:\\Users\\조예은\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\legacy_tf_layers\\normalization.py:463: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
            "  return layer.apply(inputs, training=training)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loading saved model in resources/model\\gru4rec/best_model\n"
          ]
        }
      ],
      "source": [
        "model_best_trained = SeqModel(hparams, input_creator, seed=RANDOM_SEED)\n",
        "path_best_trained = os.path.join(hparams.MODEL_DIR, \"best_model\")\n",
        "print('loading saved model in {0}'.format(path_best_trained))\n",
        "model_best_trained.load_model(path_best_trained)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fKtBHxlGu5-K",
        "outputId": "d398f3b9-bbba-4707-fae1-7c1c817a7e66"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'auc': 0.8208,\n",
              " 'logloss': 0.5858,\n",
              " 'mean_mrr': 0.5862,\n",
              " 'ndcg@2': 0.5135,\n",
              " 'ndcg@4': 0.6284,\n",
              " 'ndcg@6': 0.6733,\n",
              " 'group_auc': 0.8202}"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_best_trained.run_eval(test_file, num_ngs=test_num_ngs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XH7L5VOQvARx",
        "outputId": "46721757-0218-4a41-8557-fcd7806b0852"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<recommenders.models.deeprec.models.sequential.gru4rec.GRU4RecModel at 0x1fb0483b430>"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_best_trained.predict(test_file, output_file)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "GRU4REC.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
