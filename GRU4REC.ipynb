{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sknnn2xs-AHi",
        "outputId": "a4f15d3b-23ff-42f1-8d49-ded97e42bf52"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "System version: 3.8.9 (tags/v3.8.9:a743f81, Apr  6 2021, 14:02:34) [MSC v.1928 64 bit (AMD64)]\n",
            "Tensorflow version: 2.8.0\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "import logging\n",
        "import papermill as pm\n",
        "import scrapbook as sb\n",
        "from tempfile import TemporaryDirectory\n",
        "import numpy as np\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.get_logger().setLevel('ERROR') # only show error messages\n",
        "\n",
        "from recommenders.utils.timer import Timer\n",
        "from recommenders.utils.constants import SEED\n",
        "from recommenders.models.deeprec.deeprec_utils import (\n",
        "    prepare_hparams\n",
        ")\n",
        "\n",
        "from resources.data_preprocessing import data_preprocessing\n",
        "# from recommenders.datasets.amazon_reviews import download_and_extract, data_preprocessing\n",
        "from recommenders.datasets.download_utils import maybe_download\n",
        "\n",
        "\n",
        "# from recommenders.models.deeprec.models.sequential.sli_rec import SLI_RECModel as SeqModel\n",
        "####  to use the other model, use one of the following lines:\n",
        "#from recommenders.models.deeprec.models.sequential.asvd import A2SVDModel as SeqModel\n",
        "# from recommenders.models.deeprec.models.sequential.caser import CaserModel as SeqModel\n",
        "from recommenders.models.deeprec.models.sequential.gru4rec import GRU4RecModel as SeqModel\n",
        "# from recommenders.models.deeprec.models.sequential.sum import SUMModel as SeqModel\n",
        "\n",
        "#from recommenders.models.deeprec.models.sequential.nextitnet import NextItNetModel\n",
        "\n",
        "from recommenders.models.deeprec.io.sequential_iterator import SequentialIterator\n",
        "#from recommenders.models.deeprec.io.nextitnet_iterator import NextItNetIterator\n",
        "\n",
        "print(\"System version: {}\".format(sys.version))\n",
        "print(\"Tensorflow version: {}\".format(tf.__version__))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "sxtfUZfP-ZWR"
      },
      "outputs": [],
      "source": [
        "##  ATTENTION: change to the corresponding config file, e.g., caser.yaml for CaserModel, sum.yaml for SUMModel\n",
        "# yaml_file = '../../recommenders/models/deeprec/config/sli_rec.yaml'  \n",
        "yaml_file = './gru4rec.yaml'  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "_NnzhN4h_rI5"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 10\n",
        "BATCH_SIZE = 400\n",
        "RANDOM_SEED = SEED  # Set None for non-deterministic result\n",
        "\n",
        "data_path = os.path.join(\"resources/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "0RAMNcI3AClc"
      },
      "outputs": [],
      "source": [
        "# for test\n",
        "train_file = os.path.join(data_path, r'train_data')\n",
        "valid_file = os.path.join(data_path, r'valid_data')\n",
        "test_file = os.path.join(data_path, r'test_data')\n",
        "user_vocab = os.path.join(data_path, r'user_vocab.pkl')\n",
        "item_vocab = os.path.join(data_path, r'item_vocab.pkl')\n",
        "cate_vocab = os.path.join(data_path, r'cate_vocab.pkl')\n",
        "output_file = os.path.join(data_path, r'output_gru4rec.txt')\n",
        "\n",
        "# reviews_name = 'json'\n",
        "# meta_name = 'json'\n",
        "# reviews_file = os.path.join(data_path, reviews_name)\n",
        "# meta_file = os.path.join(data_path, meta_name)\n",
        "train_num_ngs = 4 # number of negative instances with a positive instance for training\n",
        "valid_num_ngs = 4 # number of negative instances with a positive instance for validation\n",
        "test_num_ngs = 9 # number of negative instances with a positive instance for testing\n",
        "sample_rate = 0.01 # sample a small item set for training and testing here for fast example\n",
        "\n",
        "input_files = [data_path, train_file, valid_file, test_file, user_vocab, item_vocab, cate_vocab]\n",
        "\n",
        "if not os.path.exists(train_file):\n",
        "    # download_and_extract(reviews_name, reviews_file)\n",
        "    # download_and_extract(meta_name, meta_file)\n",
        "    data_preprocessing(*input_files, sample_rate=sample_rate, valid_num_ngs=valid_num_ngs, test_num_ngs=test_num_ngs)\n",
        "    #### uncomment this for the NextItNet model, because it does not need to unfold the user history\n",
        "    # data_preprocessing(*input_files, sample_rate=sample_rate, valid_num_ngs=valid_num_ngs, test_num_ngs=test_num_ngs, is_history_expanding=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "rwTMO3WEWaCu"
      },
      "outputs": [],
      "source": [
        "### NOTE:  \n",
        "### remember to use `_create_vocab(train_file, user_vocab, item_vocab, cate_vocab)` to generate the user_vocab, item_vocab and cate_vocab files, if you are using your own dataset rather than using our demo Amazon dataset.\n",
        "hparams = prepare_hparams(yaml_file, \n",
        "                          embed_l2=0., \n",
        "                          layer_l2=0., \n",
        "                          learning_rate=0.001,  # set to 0.01 if batch normalization is disable\n",
        "                          epochs=EPOCHS,\n",
        "                          batch_size=BATCH_SIZE,\n",
        "                          show_step=20,\n",
        "                          MODEL_DIR=os.path.join(data_path, \"model\", \"gru4rec/\"),\n",
        "                          SUMMARIES_DIR=os.path.join(data_path, \"summary\", \"gru4rec/\"),\n",
        "                          user_vocab=user_vocab,\n",
        "                          item_vocab=item_vocab,\n",
        "                          cate_vocab=cate_vocab,\n",
        "                          need_sample=True,\n",
        "                          train_num_ngs=train_num_ngs, # provides the number of negative instances for each positive instance for loss computation.\n",
        "            )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "4Rvq7gFluVPq"
      },
      "outputs": [],
      "source": [
        "input_creator = SequentialIterator\n",
        "#### uncomment this for the NextItNet model, because it needs a special data iterator for training\n",
        "#input_creator = NextItNetIterator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VFNfgcKNuaVv",
        "outputId": "2fa083f0-f73d-452a-efef-12151b40c7c5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\조예은\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\recommenders\\models\\deeprec\\models\\sequential\\gru4rec.py:71: UserWarning: `tf.nn.rnn_cell.GRUCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.GRUCell`, and will be replaced by that in Tensorflow 2.0.\n",
            "  GRUCell(self.hidden_size),\n",
            "C:\\Users\\조예은\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\layers\\legacy_rnn\\rnn_cell_impl.py:570: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  self._gate_kernel = self.add_variable(\n",
            "C:\\Users\\조예은\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\layers\\legacy_rnn\\rnn_cell_impl.py:574: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  self._gate_bias = self.add_variable(\n",
            "C:\\Users\\조예은\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\layers\\legacy_rnn\\rnn_cell_impl.py:580: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  self._candidate_kernel = self.add_variable(\n",
            "C:\\Users\\조예은\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\layers\\legacy_rnn\\rnn_cell_impl.py:584: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  self._candidate_bias = self.add_variable(\n",
            "C:\\Users\\조예은\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\recommenders\\models\\deeprec\\models\\base_model.py:701: UserWarning: `tf.layers.batch_normalization` is deprecated and will be removed in a future version. Please use `tf.keras.layers.BatchNormalization` instead. In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.BatchNormalization` documentation).\n",
            "  curr_hidden_nn_layer = tf.compat.v1.layers.batch_normalization(\n",
            "C:\\Users\\조예은\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\legacy_tf_layers\\normalization.py:463: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
            "  return layer.apply(inputs, training=training)\n"
          ]
        }
      ],
      "source": [
        "model = SeqModel(hparams, input_creator, seed=RANDOM_SEED)\n",
        "\n",
        "## sometimes we don't want to train a model from scratch\n",
        "## then we can load a pre-trained model like this: \n",
        "#model.load_model(r'your_model_path')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8AQxPlk-uheU",
        "outputId": "55a5328a-44bd-4213-b10c-96f26313586a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'auc': 0.5276, 'logloss': 0.6931, 'mean_mrr': 0.2854, 'ndcg@2': 0.1525, 'ndcg@4': 0.2504, 'ndcg@6': 0.3251, 'group_auc': 0.5281}\n"
          ]
        }
      ],
      "source": [
        "# test_num_ngs is the number of negative lines after each positive line in your test_file\n",
        "print(model.run_eval(test_file, num_ngs=test_num_ngs)) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QFKQu9rGu0jq",
        "outputId": "c93b6ebf-50cc-43ea-e620-418e4537ad31"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 20 , total_loss: 1.4803, data_loss: 1.4803\n",
            "step 40 , total_loss: 1.3771, data_loss: 1.3771\n",
            "step 60 , total_loss: 1.3180, data_loss: 1.3180\n",
            "step 80 , total_loss: 1.3611, data_loss: 1.3611\n",
            "step 100 , total_loss: 1.3035, data_loss: 1.3035\n",
            "step 120 , total_loss: 1.2060, data_loss: 1.2060\n",
            "step 140 , total_loss: 1.2920, data_loss: 1.2920\n",
            "step 160 , total_loss: 1.3323, data_loss: 1.3323\n",
            "step 180 , total_loss: 1.2187, data_loss: 1.2187\n",
            "step 200 , total_loss: 1.2995, data_loss: 1.2995\n",
            "step 220 , total_loss: 1.2219, data_loss: 1.2219\n",
            "step 240 , total_loss: 1.2943, data_loss: 1.2943\n",
            "step 260 , total_loss: 1.1977, data_loss: 1.1977\n",
            "step 280 , total_loss: 1.2058, data_loss: 1.2058\n",
            "step 300 , total_loss: 1.2679, data_loss: 1.2679\n",
            "step 320 , total_loss: 1.2253, data_loss: 1.2253\n",
            "step 340 , total_loss: 1.2836, data_loss: 1.2836\n",
            "step 360 , total_loss: 1.2942, data_loss: 1.2942\n",
            "step 380 , total_loss: 1.2115, data_loss: 1.2115\n",
            "step 400 , total_loss: 1.2019, data_loss: 1.2019\n",
            "step 420 , total_loss: 1.2636, data_loss: 1.2636\n",
            "step 440 , total_loss: 1.1702, data_loss: 1.1702\n",
            "step 460 , total_loss: 1.1487, data_loss: 1.1487\n",
            "step 480 , total_loss: 1.2513, data_loss: 1.2513\n",
            "step 500 , total_loss: 1.1688, data_loss: 1.1688\n",
            "step 520 , total_loss: 1.1301, data_loss: 1.1301\n",
            "step 540 , total_loss: 1.1871, data_loss: 1.1871\n",
            "step 560 , total_loss: 1.2504, data_loss: 1.2504\n",
            "step 580 , total_loss: 1.1532, data_loss: 1.1532\n",
            "step 600 , total_loss: 1.1236, data_loss: 1.1236\n",
            "step 620 , total_loss: 1.2187, data_loss: 1.2187\n",
            "step 640 , total_loss: 1.2275, data_loss: 1.2275\n",
            "step 660 , total_loss: 1.2138, data_loss: 1.2138\n",
            "step 680 , total_loss: 1.1471, data_loss: 1.1471\n",
            "step 700 , total_loss: 1.1908, data_loss: 1.1908\n",
            "step 720 , total_loss: 1.1830, data_loss: 1.1830\n",
            "step 740 , total_loss: 1.1821, data_loss: 1.1821\n",
            "step 760 , total_loss: 1.1819, data_loss: 1.1819\n",
            "step 780 , total_loss: 1.2177, data_loss: 1.2177\n",
            "step 800 , total_loss: 1.1502, data_loss: 1.1502\n",
            "step 820 , total_loss: 1.1043, data_loss: 1.1043\n",
            "step 840 , total_loss: 1.1072, data_loss: 1.1072\n",
            "step 860 , total_loss: 1.2035, data_loss: 1.2035\n",
            "step 880 , total_loss: 1.0935, data_loss: 1.0935\n",
            "step 900 , total_loss: 1.1848, data_loss: 1.1848\n",
            "step 920 , total_loss: 1.0821, data_loss: 1.0821\n",
            "eval valid at epoch 1: auc:0.7504,logloss:0.6517,mean_mrr:0.6887,ndcg@2:0.6399,ndcg@4:0.7494,ndcg@6:0.7666,group_auc:0.7596\n",
            "step 20 , total_loss: 1.1885, data_loss: 1.1885\n",
            "step 40 , total_loss: 1.0702, data_loss: 1.0702\n",
            "step 60 , total_loss: 1.1939, data_loss: 1.1939\n",
            "step 80 , total_loss: 1.1342, data_loss: 1.1342\n",
            "step 100 , total_loss: 1.1487, data_loss: 1.1487\n",
            "step 120 , total_loss: 1.1163, data_loss: 1.1163\n",
            "step 140 , total_loss: 1.1342, data_loss: 1.1342\n",
            "step 160 , total_loss: 1.1507, data_loss: 1.1507\n",
            "step 180 , total_loss: 1.0570, data_loss: 1.0570\n",
            "step 200 , total_loss: 1.1317, data_loss: 1.1317\n",
            "step 220 , total_loss: 1.0763, data_loss: 1.0763\n",
            "step 240 , total_loss: 1.1378, data_loss: 1.1378\n",
            "step 260 , total_loss: 1.1065, data_loss: 1.1065\n",
            "step 280 , total_loss: 1.1801, data_loss: 1.1801\n",
            "step 300 , total_loss: 1.1729, data_loss: 1.1729\n",
            "step 320 , total_loss: 1.1626, data_loss: 1.1626\n",
            "step 340 , total_loss: 1.1344, data_loss: 1.1344\n",
            "step 360 , total_loss: 1.1540, data_loss: 1.1540\n",
            "step 380 , total_loss: 1.0964, data_loss: 1.0964\n",
            "step 400 , total_loss: 1.0784, data_loss: 1.0784\n",
            "step 420 , total_loss: 1.1321, data_loss: 1.1321\n",
            "step 440 , total_loss: 1.1843, data_loss: 1.1843\n",
            "step 460 , total_loss: 1.1204, data_loss: 1.1204\n",
            "step 480 , total_loss: 1.0533, data_loss: 1.0533\n",
            "step 500 , total_loss: 1.1343, data_loss: 1.1343\n",
            "step 520 , total_loss: 1.1073, data_loss: 1.1073\n",
            "step 540 , total_loss: 1.1689, data_loss: 1.1689\n",
            "step 560 , total_loss: 1.1115, data_loss: 1.1115\n",
            "step 580 , total_loss: 1.1540, data_loss: 1.1540\n",
            "step 600 , total_loss: 1.1348, data_loss: 1.1348\n",
            "step 620 , total_loss: 1.1202, data_loss: 1.1202\n",
            "step 640 , total_loss: 1.1324, data_loss: 1.1324\n",
            "step 660 , total_loss: 1.0987, data_loss: 1.0987\n",
            "step 680 , total_loss: 1.1219, data_loss: 1.1219\n",
            "step 700 , total_loss: 1.0778, data_loss: 1.0778\n",
            "step 720 , total_loss: 1.0988, data_loss: 1.0988\n",
            "step 740 , total_loss: 1.1497, data_loss: 1.1497\n",
            "step 760 , total_loss: 1.1399, data_loss: 1.1399\n",
            "step 780 , total_loss: 1.0850, data_loss: 1.0850\n",
            "step 800 , total_loss: 1.1442, data_loss: 1.1442\n",
            "step 820 , total_loss: 1.1220, data_loss: 1.1220\n",
            "step 840 , total_loss: 1.0936, data_loss: 1.0936\n",
            "step 860 , total_loss: 1.1207, data_loss: 1.1207\n",
            "step 880 , total_loss: 1.0511, data_loss: 1.0511\n",
            "step 900 , total_loss: 1.0517, data_loss: 1.0517\n",
            "step 920 , total_loss: 1.1487, data_loss: 1.1487\n",
            "eval valid at epoch 2: auc:0.7715,logloss:0.6102,mean_mrr:0.7037,ndcg@2:0.6606,ndcg@4:0.7622,ndcg@6:0.778,group_auc:0.7743\n",
            "step 20 , total_loss: 1.1344, data_loss: 1.1344\n",
            "step 40 , total_loss: 1.1772, data_loss: 1.1772\n",
            "step 60 , total_loss: 1.0619, data_loss: 1.0619\n",
            "step 80 , total_loss: 1.1049, data_loss: 1.1049\n",
            "step 100 , total_loss: 1.0971, data_loss: 1.0971\n",
            "step 120 , total_loss: 1.0906, data_loss: 1.0906\n",
            "step 140 , total_loss: 1.1210, data_loss: 1.1210\n",
            "step 160 , total_loss: 1.0666, data_loss: 1.0666\n",
            "step 180 , total_loss: 1.1009, data_loss: 1.1009\n",
            "step 200 , total_loss: 1.0009, data_loss: 1.0009\n",
            "step 220 , total_loss: 1.1606, data_loss: 1.1606\n",
            "step 240 , total_loss: 1.1913, data_loss: 1.1913\n",
            "step 260 , total_loss: 1.0836, data_loss: 1.0836\n",
            "step 280 , total_loss: 1.0541, data_loss: 1.0541\n",
            "step 300 , total_loss: 1.1228, data_loss: 1.1228\n",
            "step 320 , total_loss: 1.0436, data_loss: 1.0436\n",
            "step 340 , total_loss: 1.0720, data_loss: 1.0720\n",
            "step 360 , total_loss: 1.2072, data_loss: 1.2072\n",
            "step 380 , total_loss: 1.0931, data_loss: 1.0931\n",
            "step 400 , total_loss: 1.1399, data_loss: 1.1399\n",
            "step 420 , total_loss: 1.1082, data_loss: 1.1082\n",
            "step 440 , total_loss: 1.0867, data_loss: 1.0867\n",
            "step 460 , total_loss: 1.0512, data_loss: 1.0512\n",
            "step 480 , total_loss: 0.9780, data_loss: 0.9780\n",
            "step 500 , total_loss: 1.0619, data_loss: 1.0619\n",
            "step 520 , total_loss: 1.1486, data_loss: 1.1486\n",
            "step 540 , total_loss: 1.0434, data_loss: 1.0434\n",
            "step 560 , total_loss: 1.1447, data_loss: 1.1447\n",
            "step 580 , total_loss: 1.1212, data_loss: 1.1212\n",
            "step 600 , total_loss: 1.1343, data_loss: 1.1343\n",
            "step 620 , total_loss: 1.0598, data_loss: 1.0598\n",
            "step 640 , total_loss: 1.1157, data_loss: 1.1157\n",
            "step 660 , total_loss: 0.9976, data_loss: 0.9976\n",
            "step 680 , total_loss: 1.0115, data_loss: 1.0115\n",
            "step 700 , total_loss: 1.0316, data_loss: 1.0316\n",
            "step 720 , total_loss: 1.1205, data_loss: 1.1205\n",
            "step 740 , total_loss: 1.1159, data_loss: 1.1159\n",
            "step 760 , total_loss: 1.0988, data_loss: 1.0988\n",
            "step 780 , total_loss: 1.1065, data_loss: 1.1065\n",
            "step 800 , total_loss: 1.0849, data_loss: 1.0849\n",
            "step 820 , total_loss: 1.0503, data_loss: 1.0503\n",
            "step 840 , total_loss: 1.1097, data_loss: 1.1097\n",
            "step 860 , total_loss: 1.1040, data_loss: 1.1040\n",
            "step 880 , total_loss: 1.1174, data_loss: 1.1174\n",
            "step 900 , total_loss: 1.1017, data_loss: 1.1017\n",
            "step 920 , total_loss: 1.1006, data_loss: 1.1006\n",
            "eval valid at epoch 3: auc:0.7832,logloss:0.5962,mean_mrr:0.7114,ndcg@2:0.6725,ndcg@4:0.77,ndcg@6:0.784,group_auc:0.7845\n",
            "step 20 , total_loss: 1.1375, data_loss: 1.1375\n",
            "step 40 , total_loss: 1.0558, data_loss: 1.0558\n",
            "step 60 , total_loss: 1.1037, data_loss: 1.1037\n",
            "step 80 , total_loss: 1.1011, data_loss: 1.1011\n",
            "step 100 , total_loss: 1.0510, data_loss: 1.0510\n",
            "step 120 , total_loss: 1.1133, data_loss: 1.1133\n",
            "step 140 , total_loss: 1.0586, data_loss: 1.0586\n",
            "step 160 , total_loss: 1.1072, data_loss: 1.1072\n",
            "step 180 , total_loss: 1.1030, data_loss: 1.1030\n",
            "step 200 , total_loss: 1.0806, data_loss: 1.0806\n",
            "step 220 , total_loss: 1.0389, data_loss: 1.0389\n",
            "step 240 , total_loss: 1.0917, data_loss: 1.0917\n",
            "step 260 , total_loss: 1.0195, data_loss: 1.0195\n",
            "step 280 , total_loss: 1.0536, data_loss: 1.0536\n",
            "step 300 , total_loss: 1.1026, data_loss: 1.1026\n",
            "step 320 , total_loss: 1.1261, data_loss: 1.1261\n",
            "step 340 , total_loss: 1.0873, data_loss: 1.0873\n",
            "step 360 , total_loss: 1.0659, data_loss: 1.0659\n",
            "step 380 , total_loss: 1.0711, data_loss: 1.0711\n",
            "step 400 , total_loss: 1.0888, data_loss: 1.0888\n",
            "step 420 , total_loss: 1.1149, data_loss: 1.1149\n",
            "step 440 , total_loss: 1.0310, data_loss: 1.0310\n",
            "step 460 , total_loss: 1.1026, data_loss: 1.1026\n",
            "step 480 , total_loss: 1.1227, data_loss: 1.1227\n",
            "step 500 , total_loss: 1.0331, data_loss: 1.0331\n",
            "step 520 , total_loss: 1.0910, data_loss: 1.0910\n",
            "step 540 , total_loss: 1.0351, data_loss: 1.0351\n",
            "step 560 , total_loss: 1.0974, data_loss: 1.0974\n",
            "step 580 , total_loss: 1.0650, data_loss: 1.0650\n",
            "step 600 , total_loss: 0.9735, data_loss: 0.9735\n",
            "step 620 , total_loss: 1.0023, data_loss: 1.0023\n",
            "step 640 , total_loss: 1.0985, data_loss: 1.0985\n",
            "step 660 , total_loss: 1.1243, data_loss: 1.1243\n",
            "step 680 , total_loss: 1.0846, data_loss: 1.0846\n",
            "step 700 , total_loss: 1.0263, data_loss: 1.0263\n",
            "step 720 , total_loss: 1.0751, data_loss: 1.0751\n",
            "step 740 , total_loss: 1.0929, data_loss: 1.0929\n",
            "step 760 , total_loss: 1.0569, data_loss: 1.0569\n",
            "step 780 , total_loss: 1.0055, data_loss: 1.0055\n",
            "step 800 , total_loss: 1.0735, data_loss: 1.0735\n",
            "step 820 , total_loss: 1.0082, data_loss: 1.0082\n",
            "step 840 , total_loss: 1.1150, data_loss: 1.1150\n",
            "step 860 , total_loss: 1.0103, data_loss: 1.0103\n",
            "step 880 , total_loss: 1.0507, data_loss: 1.0507\n",
            "step 900 , total_loss: 1.1078, data_loss: 1.1078\n",
            "step 920 , total_loss: 1.0241, data_loss: 1.0241\n",
            "eval valid at epoch 4: auc:0.7927,logloss:0.5851,mean_mrr:0.7202,ndcg@2:0.6855,ndcg@4:0.7787,ndcg@6:0.7907,group_auc:0.7944\n",
            "step 20 , total_loss: 1.0278, data_loss: 1.0278\n",
            "step 40 , total_loss: 1.0432, data_loss: 1.0432\n",
            "step 60 , total_loss: 1.0545, data_loss: 1.0545\n",
            "step 80 , total_loss: 1.0178, data_loss: 1.0178\n",
            "step 100 , total_loss: 1.0487, data_loss: 1.0487\n",
            "step 120 , total_loss: 1.1358, data_loss: 1.1358\n",
            "step 140 , total_loss: 0.9835, data_loss: 0.9835\n",
            "step 160 , total_loss: 1.0219, data_loss: 1.0219\n",
            "step 180 , total_loss: 1.0216, data_loss: 1.0216\n",
            "step 200 , total_loss: 1.0077, data_loss: 1.0077\n",
            "step 220 , total_loss: 1.0814, data_loss: 1.0814\n",
            "step 240 , total_loss: 1.0993, data_loss: 1.0993\n",
            "step 260 , total_loss: 1.0931, data_loss: 1.0931\n",
            "step 280 , total_loss: 1.0444, data_loss: 1.0444\n",
            "step 300 , total_loss: 1.0568, data_loss: 1.0568\n",
            "step 320 , total_loss: 1.1088, data_loss: 1.1088\n",
            "step 340 , total_loss: 1.0159, data_loss: 1.0159\n",
            "step 360 , total_loss: 1.0715, data_loss: 1.0715\n",
            "step 380 , total_loss: 1.0597, data_loss: 1.0597\n",
            "step 400 , total_loss: 1.0452, data_loss: 1.0452\n",
            "step 420 , total_loss: 1.0497, data_loss: 1.0497\n",
            "step 440 , total_loss: 1.0621, data_loss: 1.0621\n",
            "step 460 , total_loss: 1.0977, data_loss: 1.0977\n",
            "step 480 , total_loss: 1.0667, data_loss: 1.0667\n",
            "step 500 , total_loss: 1.0160, data_loss: 1.0160\n",
            "step 520 , total_loss: 0.9386, data_loss: 0.9386\n",
            "step 540 , total_loss: 0.9996, data_loss: 0.9996\n",
            "step 560 , total_loss: 0.9403, data_loss: 0.9403\n",
            "step 580 , total_loss: 1.0626, data_loss: 1.0626\n",
            "step 600 , total_loss: 1.0615, data_loss: 1.0615\n",
            "step 620 , total_loss: 1.0228, data_loss: 1.0228\n",
            "step 640 , total_loss: 1.0945, data_loss: 1.0945\n",
            "step 660 , total_loss: 1.0364, data_loss: 1.0364\n",
            "step 680 , total_loss: 0.9845, data_loss: 0.9845\n",
            "step 700 , total_loss: 1.1159, data_loss: 1.1159\n",
            "step 720 , total_loss: 1.0109, data_loss: 1.0109\n",
            "step 740 , total_loss: 1.1033, data_loss: 1.1033\n",
            "step 760 , total_loss: 1.1078, data_loss: 1.1078\n",
            "step 780 , total_loss: 1.0168, data_loss: 1.0168\n",
            "step 800 , total_loss: 1.0340, data_loss: 1.0340\n",
            "step 820 , total_loss: 1.1015, data_loss: 1.1015\n",
            "step 840 , total_loss: 1.0743, data_loss: 1.0743\n",
            "step 860 , total_loss: 1.0388, data_loss: 1.0388\n",
            "step 880 , total_loss: 1.0959, data_loss: 1.0959\n",
            "step 900 , total_loss: 0.9795, data_loss: 0.9795\n",
            "step 920 , total_loss: 0.9951, data_loss: 0.9951\n",
            "eval valid at epoch 5: auc:0.7985,logloss:0.6385,mean_mrr:0.7232,ndcg@2:0.6873,ndcg@4:0.782,ndcg@6:0.7929,group_auc:0.7965\n",
            "step 20 , total_loss: 0.9593, data_loss: 0.9593\n",
            "step 40 , total_loss: 1.0705, data_loss: 1.0705\n",
            "step 60 , total_loss: 1.0586, data_loss: 1.0586\n",
            "step 80 , total_loss: 1.0781, data_loss: 1.0781\n",
            "step 100 , total_loss: 1.0617, data_loss: 1.0617\n",
            "step 120 , total_loss: 1.0814, data_loss: 1.0814\n",
            "step 140 , total_loss: 1.0569, data_loss: 1.0569\n",
            "step 160 , total_loss: 1.0513, data_loss: 1.0513\n",
            "step 180 , total_loss: 1.0006, data_loss: 1.0006\n",
            "step 200 , total_loss: 1.0307, data_loss: 1.0307\n",
            "step 220 , total_loss: 1.0218, data_loss: 1.0218\n",
            "step 240 , total_loss: 1.0707, data_loss: 1.0707\n",
            "step 260 , total_loss: 1.0278, data_loss: 1.0278\n",
            "step 280 , total_loss: 1.0162, data_loss: 1.0162\n",
            "step 300 , total_loss: 1.0308, data_loss: 1.0308\n",
            "step 320 , total_loss: 0.9642, data_loss: 0.9642\n",
            "step 340 , total_loss: 1.0498, data_loss: 1.0498\n",
            "step 360 , total_loss: 1.1044, data_loss: 1.1044\n",
            "step 380 , total_loss: 1.0756, data_loss: 1.0756\n",
            "step 400 , total_loss: 1.0203, data_loss: 1.0203\n",
            "step 420 , total_loss: 0.9899, data_loss: 0.9899\n",
            "step 440 , total_loss: 1.0202, data_loss: 1.0202\n",
            "step 460 , total_loss: 1.0032, data_loss: 1.0032\n",
            "step 480 , total_loss: 1.0847, data_loss: 1.0847\n",
            "step 500 , total_loss: 1.0324, data_loss: 1.0324\n",
            "step 520 , total_loss: 1.0234, data_loss: 1.0234\n",
            "step 540 , total_loss: 1.0145, data_loss: 1.0145\n",
            "step 560 , total_loss: 0.9922, data_loss: 0.9922\n",
            "step 580 , total_loss: 1.0719, data_loss: 1.0719\n",
            "step 600 , total_loss: 1.0452, data_loss: 1.0452\n",
            "step 620 , total_loss: 1.0052, data_loss: 1.0052\n",
            "step 640 , total_loss: 0.9817, data_loss: 0.9817\n",
            "step 660 , total_loss: 1.0358, data_loss: 1.0358\n",
            "step 680 , total_loss: 0.9992, data_loss: 0.9992\n",
            "step 700 , total_loss: 1.0212, data_loss: 1.0212\n",
            "step 720 , total_loss: 0.9803, data_loss: 0.9803\n",
            "step 740 , total_loss: 1.0068, data_loss: 1.0068\n",
            "step 760 , total_loss: 1.0257, data_loss: 1.0257\n",
            "step 780 , total_loss: 0.9728, data_loss: 0.9728\n",
            "step 800 , total_loss: 1.0654, data_loss: 1.0654\n",
            "step 820 , total_loss: 0.9816, data_loss: 0.9816\n",
            "step 840 , total_loss: 1.0058, data_loss: 1.0058\n",
            "step 860 , total_loss: 1.0670, data_loss: 1.0670\n",
            "step 880 , total_loss: 1.0909, data_loss: 1.0909\n",
            "step 900 , total_loss: 1.0726, data_loss: 1.0726\n",
            "step 920 , total_loss: 1.0630, data_loss: 1.0630\n",
            "eval valid at epoch 6: auc:0.8062,logloss:0.6108,mean_mrr:0.7287,ndcg@2:0.6979,ndcg@4:0.7865,ndcg@6:0.7972,group_auc:0.803\n",
            "step 20 , total_loss: 1.0286, data_loss: 1.0286\n",
            "step 40 , total_loss: 0.9680, data_loss: 0.9680\n",
            "step 60 , total_loss: 1.0222, data_loss: 1.0222\n",
            "step 80 , total_loss: 0.9705, data_loss: 0.9705\n",
            "step 100 , total_loss: 1.0490, data_loss: 1.0490\n",
            "step 120 , total_loss: 1.0355, data_loss: 1.0355\n",
            "step 140 , total_loss: 1.0112, data_loss: 1.0112\n",
            "step 160 , total_loss: 0.9697, data_loss: 0.9697\n",
            "step 180 , total_loss: 0.9896, data_loss: 0.9896\n",
            "step 200 , total_loss: 1.0415, data_loss: 1.0415\n",
            "step 220 , total_loss: 1.0767, data_loss: 1.0767\n",
            "step 240 , total_loss: 0.9608, data_loss: 0.9608\n",
            "step 260 , total_loss: 1.0242, data_loss: 1.0242\n",
            "step 280 , total_loss: 1.0419, data_loss: 1.0419\n",
            "step 300 , total_loss: 1.0145, data_loss: 1.0145\n",
            "step 320 , total_loss: 1.0238, data_loss: 1.0238\n",
            "step 340 , total_loss: 1.0131, data_loss: 1.0131\n",
            "step 360 , total_loss: 0.9843, data_loss: 0.9843\n",
            "step 380 , total_loss: 0.9911, data_loss: 0.9911\n",
            "step 400 , total_loss: 1.0942, data_loss: 1.0942\n",
            "step 420 , total_loss: 0.9703, data_loss: 0.9703\n",
            "step 440 , total_loss: 0.9612, data_loss: 0.9612\n",
            "step 460 , total_loss: 1.0194, data_loss: 1.0194\n",
            "step 480 , total_loss: 1.0554, data_loss: 1.0554\n",
            "step 500 , total_loss: 1.0806, data_loss: 1.0806\n",
            "step 520 , total_loss: 0.9843, data_loss: 0.9843\n",
            "step 540 , total_loss: 0.9923, data_loss: 0.9923\n",
            "step 560 , total_loss: 0.9521, data_loss: 0.9521\n",
            "step 580 , total_loss: 0.9711, data_loss: 0.9711\n",
            "step 600 , total_loss: 1.0391, data_loss: 1.0391\n",
            "step 620 , total_loss: 1.0188, data_loss: 1.0188\n",
            "step 640 , total_loss: 1.0628, data_loss: 1.0628\n",
            "step 660 , total_loss: 1.0067, data_loss: 1.0067\n",
            "step 680 , total_loss: 0.9637, data_loss: 0.9637\n",
            "step 700 , total_loss: 0.9658, data_loss: 0.9658\n",
            "step 720 , total_loss: 0.9939, data_loss: 0.9939\n",
            "step 740 , total_loss: 1.0010, data_loss: 1.0010\n",
            "step 760 , total_loss: 1.0037, data_loss: 1.0037\n",
            "step 780 , total_loss: 0.9856, data_loss: 0.9856\n",
            "step 800 , total_loss: 0.9967, data_loss: 0.9967\n",
            "step 820 , total_loss: 0.9152, data_loss: 0.9152\n",
            "step 840 , total_loss: 1.0197, data_loss: 1.0197\n",
            "step 860 , total_loss: 1.0320, data_loss: 1.0320\n",
            "step 880 , total_loss: 1.0566, data_loss: 1.0566\n",
            "step 900 , total_loss: 1.0572, data_loss: 1.0572\n",
            "step 920 , total_loss: 1.0800, data_loss: 1.0800\n",
            "eval valid at epoch 7: auc:0.8035,logloss:0.6611,mean_mrr:0.728,ndcg@2:0.6945,ndcg@4:0.7866,ndcg@6:0.7966,group_auc:0.8023\n",
            "step 20 , total_loss: 1.0540, data_loss: 1.0540\n",
            "step 40 , total_loss: 1.0962, data_loss: 1.0962\n",
            "step 60 , total_loss: 1.0324, data_loss: 1.0324\n",
            "step 80 , total_loss: 1.0524, data_loss: 1.0524\n",
            "step 100 , total_loss: 0.9945, data_loss: 0.9945\n",
            "step 120 , total_loss: 0.9891, data_loss: 0.9891\n",
            "step 140 , total_loss: 0.9630, data_loss: 0.9630\n",
            "step 160 , total_loss: 0.9683, data_loss: 0.9683\n",
            "step 180 , total_loss: 1.0741, data_loss: 1.0741\n",
            "step 200 , total_loss: 1.0190, data_loss: 1.0190\n",
            "step 220 , total_loss: 1.0582, data_loss: 1.0582\n",
            "step 240 , total_loss: 0.9942, data_loss: 0.9942\n",
            "step 260 , total_loss: 1.0071, data_loss: 1.0071\n",
            "step 280 , total_loss: 0.9099, data_loss: 0.9099\n",
            "step 300 , total_loss: 1.0207, data_loss: 1.0207\n",
            "step 320 , total_loss: 1.0709, data_loss: 1.0709\n",
            "step 340 , total_loss: 1.0564, data_loss: 1.0564\n",
            "step 360 , total_loss: 1.1131, data_loss: 1.1131\n",
            "step 380 , total_loss: 1.0192, data_loss: 1.0192\n",
            "step 400 , total_loss: 1.0290, data_loss: 1.0290\n",
            "step 420 , total_loss: 0.8912, data_loss: 0.8912\n",
            "step 440 , total_loss: 1.0699, data_loss: 1.0699\n",
            "step 460 , total_loss: 0.9511, data_loss: 0.9511\n",
            "step 480 , total_loss: 0.9418, data_loss: 0.9418\n",
            "step 500 , total_loss: 0.9950, data_loss: 0.9950\n",
            "step 520 , total_loss: 0.9685, data_loss: 0.9685\n",
            "step 540 , total_loss: 1.0199, data_loss: 1.0199\n",
            "step 560 , total_loss: 0.9733, data_loss: 0.9733\n",
            "step 580 , total_loss: 0.9602, data_loss: 0.9602\n",
            "step 600 , total_loss: 1.0893, data_loss: 1.0893\n",
            "step 620 , total_loss: 1.0863, data_loss: 1.0863\n",
            "step 640 , total_loss: 1.0198, data_loss: 1.0198\n",
            "step 660 , total_loss: 1.0588, data_loss: 1.0588\n",
            "step 680 , total_loss: 0.8937, data_loss: 0.8937\n",
            "step 700 , total_loss: 0.9858, data_loss: 0.9858\n",
            "step 720 , total_loss: 0.9861, data_loss: 0.9861\n",
            "step 740 , total_loss: 0.9863, data_loss: 0.9863\n",
            "step 760 , total_loss: 0.9742, data_loss: 0.9742\n",
            "step 780 , total_loss: 1.0151, data_loss: 1.0151\n",
            "step 800 , total_loss: 1.0249, data_loss: 1.0249\n",
            "step 820 , total_loss: 0.9962, data_loss: 0.9962\n",
            "step 840 , total_loss: 1.0169, data_loss: 1.0169\n",
            "step 860 , total_loss: 0.9513, data_loss: 0.9513\n",
            "step 880 , total_loss: 1.0653, data_loss: 1.0653\n",
            "step 900 , total_loss: 0.9562, data_loss: 0.9562\n",
            "step 920 , total_loss: 1.0296, data_loss: 1.0296\n",
            "eval valid at epoch 8: auc:0.8018,logloss:0.6121,mean_mrr:0.7317,ndcg@2:0.6994,ndcg@4:0.7894,ndcg@6:0.7993,group_auc:0.8047\n",
            "step 20 , total_loss: 0.9353, data_loss: 0.9353\n",
            "step 40 , total_loss: 0.9879, data_loss: 0.9879\n",
            "step 60 , total_loss: 1.0585, data_loss: 1.0585\n",
            "step 80 , total_loss: 0.9617, data_loss: 0.9617\n",
            "step 100 , total_loss: 1.0082, data_loss: 1.0082\n",
            "step 120 , total_loss: 0.8703, data_loss: 0.8703\n",
            "step 140 , total_loss: 1.0354, data_loss: 1.0354\n",
            "step 160 , total_loss: 0.9676, data_loss: 0.9676\n",
            "step 180 , total_loss: 1.0127, data_loss: 1.0127\n",
            "step 200 , total_loss: 1.0101, data_loss: 1.0101\n",
            "step 220 , total_loss: 0.9911, data_loss: 0.9911\n",
            "step 240 , total_loss: 0.9501, data_loss: 0.9501\n",
            "step 260 , total_loss: 0.9577, data_loss: 0.9577\n",
            "step 280 , total_loss: 0.9884, data_loss: 0.9884\n",
            "step 300 , total_loss: 0.9362, data_loss: 0.9362\n",
            "step 320 , total_loss: 1.0059, data_loss: 1.0059\n",
            "step 340 , total_loss: 1.0410, data_loss: 1.0410\n",
            "step 360 , total_loss: 1.0057, data_loss: 1.0057\n",
            "step 380 , total_loss: 1.0186, data_loss: 1.0186\n",
            "step 400 , total_loss: 1.0733, data_loss: 1.0733\n",
            "step 420 , total_loss: 1.0102, data_loss: 1.0102\n",
            "step 440 , total_loss: 1.0275, data_loss: 1.0275\n",
            "step 460 , total_loss: 1.0055, data_loss: 1.0055\n",
            "step 480 , total_loss: 1.0517, data_loss: 1.0517\n",
            "step 500 , total_loss: 0.9146, data_loss: 0.9146\n",
            "step 520 , total_loss: 0.9779, data_loss: 0.9779\n",
            "step 540 , total_loss: 1.0027, data_loss: 1.0027\n",
            "step 560 , total_loss: 0.9841, data_loss: 0.9841\n",
            "step 580 , total_loss: 1.0055, data_loss: 1.0055\n",
            "step 600 , total_loss: 0.9588, data_loss: 0.9588\n",
            "step 620 , total_loss: 1.0005, data_loss: 1.0005\n",
            "step 640 , total_loss: 1.0500, data_loss: 1.0500\n",
            "step 660 , total_loss: 0.9666, data_loss: 0.9666\n",
            "step 680 , total_loss: 0.9550, data_loss: 0.9550\n",
            "step 700 , total_loss: 0.9856, data_loss: 0.9856\n",
            "step 720 , total_loss: 0.9979, data_loss: 0.9979\n",
            "step 740 , total_loss: 1.0428, data_loss: 1.0428\n",
            "step 760 , total_loss: 1.0680, data_loss: 1.0680\n",
            "step 780 , total_loss: 1.0478, data_loss: 1.0478\n",
            "step 800 , total_loss: 0.9158, data_loss: 0.9158\n",
            "step 820 , total_loss: 1.0797, data_loss: 1.0797\n",
            "step 840 , total_loss: 0.9629, data_loss: 0.9629\n",
            "step 860 , total_loss: 0.9773, data_loss: 0.9773\n",
            "step 880 , total_loss: 0.9333, data_loss: 0.9333\n",
            "step 900 , total_loss: 1.0176, data_loss: 1.0176\n",
            "step 920 , total_loss: 1.0863, data_loss: 1.0863\n",
            "eval valid at epoch 9: auc:0.8152,logloss:0.7002,mean_mrr:0.74,ndcg@2:0.7115,ndcg@4:0.7975,ndcg@6:0.8057,group_auc:0.8151\n",
            "step 20 , total_loss: 0.9797, data_loss: 0.9797\n",
            "step 40 , total_loss: 0.9315, data_loss: 0.9315\n",
            "step 60 , total_loss: 0.9418, data_loss: 0.9418\n",
            "step 80 , total_loss: 0.9443, data_loss: 0.9443\n",
            "step 100 , total_loss: 1.0226, data_loss: 1.0226\n",
            "step 120 , total_loss: 1.0065, data_loss: 1.0065\n",
            "step 140 , total_loss: 1.0240, data_loss: 1.0240\n",
            "step 160 , total_loss: 0.9974, data_loss: 0.9974\n",
            "step 180 , total_loss: 1.0156, data_loss: 1.0156\n",
            "step 200 , total_loss: 0.9487, data_loss: 0.9487\n",
            "step 220 , total_loss: 1.0137, data_loss: 1.0137\n",
            "step 240 , total_loss: 0.9832, data_loss: 0.9832\n",
            "step 260 , total_loss: 0.9536, data_loss: 0.9536\n",
            "step 280 , total_loss: 0.9702, data_loss: 0.9702\n",
            "step 300 , total_loss: 1.0543, data_loss: 1.0543\n",
            "step 320 , total_loss: 1.0117, data_loss: 1.0117\n",
            "step 340 , total_loss: 1.0309, data_loss: 1.0309\n",
            "step 360 , total_loss: 1.0417, data_loss: 1.0417\n",
            "step 380 , total_loss: 0.9302, data_loss: 0.9302\n",
            "step 400 , total_loss: 0.9809, data_loss: 0.9809\n",
            "step 420 , total_loss: 0.9928, data_loss: 0.9928\n",
            "step 440 , total_loss: 0.8973, data_loss: 0.8973\n",
            "step 460 , total_loss: 0.9525, data_loss: 0.9525\n",
            "step 480 , total_loss: 1.0897, data_loss: 1.0897\n",
            "step 500 , total_loss: 0.9951, data_loss: 0.9951\n",
            "step 520 , total_loss: 0.9775, data_loss: 0.9775\n",
            "step 540 , total_loss: 0.9959, data_loss: 0.9959\n",
            "step 560 , total_loss: 0.9811, data_loss: 0.9811\n",
            "step 580 , total_loss: 0.9325, data_loss: 0.9325\n",
            "step 600 , total_loss: 0.9686, data_loss: 0.9686\n",
            "step 620 , total_loss: 0.9136, data_loss: 0.9136\n",
            "step 640 , total_loss: 0.8874, data_loss: 0.8874\n",
            "step 660 , total_loss: 0.9815, data_loss: 0.9815\n",
            "step 680 , total_loss: 1.0573, data_loss: 1.0573\n",
            "step 700 , total_loss: 0.9754, data_loss: 0.9754\n",
            "step 720 , total_loss: 1.0684, data_loss: 1.0684\n",
            "step 740 , total_loss: 0.9664, data_loss: 0.9664\n",
            "step 760 , total_loss: 1.0421, data_loss: 1.0421\n",
            "step 780 , total_loss: 0.9963, data_loss: 0.9963\n",
            "step 800 , total_loss: 1.0210, data_loss: 1.0210\n",
            "step 820 , total_loss: 0.9381, data_loss: 0.9381\n",
            "step 840 , total_loss: 0.9979, data_loss: 0.9979\n",
            "step 860 , total_loss: 1.0565, data_loss: 1.0565\n",
            "step 880 , total_loss: 0.9990, data_loss: 0.9990\n",
            "step 900 , total_loss: 1.0356, data_loss: 1.0356\n",
            "step 920 , total_loss: 1.0808, data_loss: 1.0808\n",
            "eval valid at epoch 10: auc:0.8192,logloss:0.6857,mean_mrr:0.7417,ndcg@2:0.7141,ndcg@4:0.7994,ndcg@6:0.807,group_auc:0.8169\n",
            "[(1, {'auc': 0.7504, 'logloss': 0.6517, 'mean_mrr': 0.6887, 'ndcg@2': 0.6399, 'ndcg@4': 0.7494, 'ndcg@6': 0.7666, 'group_auc': 0.7596}), (2, {'auc': 0.7715, 'logloss': 0.6102, 'mean_mrr': 0.7037, 'ndcg@2': 0.6606, 'ndcg@4': 0.7622, 'ndcg@6': 0.778, 'group_auc': 0.7743}), (3, {'auc': 0.7832, 'logloss': 0.5962, 'mean_mrr': 0.7114, 'ndcg@2': 0.6725, 'ndcg@4': 0.77, 'ndcg@6': 0.784, 'group_auc': 0.7845}), (4, {'auc': 0.7927, 'logloss': 0.5851, 'mean_mrr': 0.7202, 'ndcg@2': 0.6855, 'ndcg@4': 0.7787, 'ndcg@6': 0.7907, 'group_auc': 0.7944}), (5, {'auc': 0.7985, 'logloss': 0.6385, 'mean_mrr': 0.7232, 'ndcg@2': 0.6873, 'ndcg@4': 0.782, 'ndcg@6': 0.7929, 'group_auc': 0.7965}), (6, {'auc': 0.8062, 'logloss': 0.6108, 'mean_mrr': 0.7287, 'ndcg@2': 0.6979, 'ndcg@4': 0.7865, 'ndcg@6': 0.7972, 'group_auc': 0.803}), (7, {'auc': 0.8035, 'logloss': 0.6611, 'mean_mrr': 0.728, 'ndcg@2': 0.6945, 'ndcg@4': 0.7866, 'ndcg@6': 0.7966, 'group_auc': 0.8023}), (8, {'auc': 0.8018, 'logloss': 0.6121, 'mean_mrr': 0.7317, 'ndcg@2': 0.6994, 'ndcg@4': 0.7894, 'ndcg@6': 0.7993, 'group_auc': 0.8047}), (9, {'auc': 0.8152, 'logloss': 0.7002, 'mean_mrr': 0.74, 'ndcg@2': 0.7115, 'ndcg@4': 0.7975, 'ndcg@6': 0.8057, 'group_auc': 0.8151}), (10, {'auc': 0.8192, 'logloss': 0.6857, 'mean_mrr': 0.7417, 'ndcg@2': 0.7141, 'ndcg@4': 0.7994, 'ndcg@6': 0.807, 'group_auc': 0.8169})]\n",
            "best epoch: 10\n",
            "Time cost for training is 87.98 mins\n"
          ]
        }
      ],
      "source": [
        "with Timer() as train_time:\n",
        "    model = model.fit(train_file, valid_file, valid_num_ngs=valid_num_ngs) \n",
        "\n",
        "# valid_num_ngs is the number of negative lines after each positive line in your valid_file \n",
        "# we will evaluate the performance of model on valid_file every epoch\n",
        "print('Time cost for training is {0:.2f} mins'.format(train_time.interval/60.0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gmm3DPjbiY9N",
        "outputId": "6dbb1a1d-8b36-49b1-ef5e-f4a807dddfde"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'auc': 0.8174, 'logloss': 0.7359, 'mean_mrr': 0.5938, 'ndcg@2': 0.5232, 'ndcg@4': 0.6322, 'ndcg@6': 0.6735, 'group_auc': 0.8171}\n"
          ]
        }
      ],
      "source": [
        "res_syn = model.run_eval(test_file, num_ngs=test_num_ngs)\n",
        "print(res_syn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "VB7MBFNkjl5e",
        "outputId": "12b143c9-1d98-4d99-9136-8c779de2d832"
      },
      "outputs": [
        {
          "data": {
            "application/scrapbook.scrap.json+json": {
              "data": {
                "auc": 0.8174,
                "group_auc": 0.8171,
                "logloss": 0.7359,
                "mean_mrr": 0.5938,
                "ndcg@2": 0.5232,
                "ndcg@4": 0.6322,
                "ndcg@6": 0.6735
              },
              "encoder": "json",
              "name": "res_syn",
              "version": 1
            }
          },
          "metadata": {
            "scrapbook": {
              "data": true,
              "display": false,
              "name": "res_syn"
            }
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "sb.glue(\"res_syn\", res_syn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "h2uxYGfFjpL9"
      },
      "outputs": [],
      "source": [
        "model = model.predict(test_file, output_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wGZ26pt-ujeo",
        "outputId": "5cbe4b07-34e3-4891-c64b-40b2a70a6fa7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\조예은\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\recommenders\\models\\deeprec\\models\\sequential\\gru4rec.py:71: UserWarning: `tf.nn.rnn_cell.GRUCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.GRUCell`, and will be replaced by that in Tensorflow 2.0.\n",
            "  GRUCell(self.hidden_size),\n",
            "C:\\Users\\조예은\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\layers\\legacy_rnn\\rnn_cell_impl.py:570: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  self._gate_kernel = self.add_variable(\n",
            "C:\\Users\\조예은\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\layers\\legacy_rnn\\rnn_cell_impl.py:574: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  self._gate_bias = self.add_variable(\n",
            "C:\\Users\\조예은\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\layers\\legacy_rnn\\rnn_cell_impl.py:580: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  self._candidate_kernel = self.add_variable(\n",
            "C:\\Users\\조예은\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\layers\\legacy_rnn\\rnn_cell_impl.py:584: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  self._candidate_bias = self.add_variable(\n",
            "C:\\Users\\조예은\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\recommenders\\models\\deeprec\\models\\base_model.py:701: UserWarning: `tf.layers.batch_normalization` is deprecated and will be removed in a future version. Please use `tf.keras.layers.BatchNormalization` instead. In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.BatchNormalization` documentation).\n",
            "  curr_hidden_nn_layer = tf.compat.v1.layers.batch_normalization(\n",
            "C:\\Users\\조예은\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\legacy_tf_layers\\normalization.py:463: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
            "  return layer.apply(inputs, training=training)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loading saved model in resources/model\\gru4rec/best_model\n"
          ]
        }
      ],
      "source": [
        "model_best_trained = SeqModel(hparams, input_creator, seed=RANDOM_SEED)\n",
        "path_best_trained = os.path.join(hparams.MODEL_DIR, \"best_model\")\n",
        "print('loading saved model in {0}'.format(path_best_trained))\n",
        "model_best_trained.load_model(path_best_trained)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fKtBHxlGu5-K",
        "outputId": "d398f3b9-bbba-4707-fae1-7c1c817a7e66"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'auc': 0.8174,\n",
              " 'logloss': 0.7359,\n",
              " 'mean_mrr': 0.5938,\n",
              " 'ndcg@2': 0.5232,\n",
              " 'ndcg@4': 0.6322,\n",
              " 'ndcg@6': 0.6735,\n",
              " 'group_auc': 0.8171}"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_best_trained.run_eval(test_file, num_ngs=test_num_ngs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XH7L5VOQvARx",
        "outputId": "46721757-0218-4a41-8557-fcd7806b0852"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<recommenders.models.deeprec.models.sequential.gru4rec.GRU4RecModel at 0x20ec79ffa90>"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_best_trained.predict(test_file, output_file)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "GRU4REC.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
